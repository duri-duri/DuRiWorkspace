#!/usr/bin/env python3
"""
Day38 Enhanced: PoU ê³ ë„í™” ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ (Day34 ê¸°ë°˜ í™•ì¥)
- ê¸°ì¡´ integrated_pou_monitoring_system.py í™•ì¥
- ì‹¤ì‹œê°„ ì‹œê³„ì—´ ìˆ˜ì§‘ (15ë¶„ bin)
- ëª©ì í•¨ìˆ˜ J ê³„ì‚° í†µí•©
- ë¯¼ê°ë„ ë¶„ì„ (âˆ‚J/âˆ‚metric)
- ìš´ì˜ ì•ŒëŒ ì‹œìŠ¤í…œ
"""

import argparse
import csv
import glob
import json
import logging
import subprocess
import tempfile
import time
from collections import defaultdict
from datetime import datetime, timedelta, timezone
from pathlib import Path
from statistics import mean
from typing import Any, Dict, List, Tuple


class EnhancedPoUMonitoringSystem:
    def __init__(self, config_path: str = "configs/monitoring.yaml"):
        self.config = self._load_config(config_path)
        self.logger = self._setup_logging()

        # ê¸°ì¡´ Day34 êµ¬ì¡° ìœ ì§€í•˜ë©´ì„œ í™•ì¥
        self.pilots = {
            "medical": {"status": "active", "last_update": None, "metrics": {}},
            "rehab": {"status": "active", "last_update": None, "metrics": {}},
            "coding": {"status": "active", "last_update": None, "metrics": {}},
        }

        # Day38 í™•ì¥ ê¸°ëŠ¥
        self.time_series_data = defaultdict(list)
        self.alert_history = []
        self.sensitivity_cache = {}

        self.logger.info("Day38 Enhanced PoU ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            import yaml

            with open(config_path, encoding="utf-8") as f:
                return yaml.safe_load(f)
        except Exception as e:
            logging.error(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì„¤ì • ë°˜í™˜
            return {
                "bin_minutes": 15,
                "domains": ["medical", "rehab", "coding"],
                "log_globs": {
                    "medical": "samples/logs/medical_*.jsonl",
                    "rehab": "samples/logs/rehab_*.jsonl",
                    "coding": "samples/logs/coding_*.jsonl",
                },
                "objective": {
                    "config": "configs/objective_params.yaml",
                    "weight_preset": "safety_first",
                    "evaluator": "tools/evaluate_objective.py",
                },
                "alerts": {
                    "fail_rate_p95_threshold": 0.008,
                    "fail_rate_persist_bins": 2,
                    "latency_p95_ms_threshold": 1800,
                    "latency_persist_bins": 2,
                },
                "outdir": {
                    "bins": "artifacts/day38/bins",
                    "series": "artifacts/day38/series",
                    "alerts": "artifacts/day38/alerts",
                    "daily": "artifacts/day38/daily",
                },
            }

    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì • (ê¸°ì¡´ Day34 ë°©ì‹ ìœ ì§€)"""
        logger = logging.getLogger(self.__class__.__name__)
        logger.setLevel(logging.INFO)
        formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")

        ch = logging.StreamHandler()
        ch.setFormatter(formatter)
        logger.addHandler(ch)

        return logger

    def parse_jsonl(self, path: str) -> List[Dict[str, Any]]:
        """JSONL íŒŒì‹± (ê¸°ì¡´ pou_metrics_ingest.py ë°©ì‹)"""
        items = []
        try:
            with open(path, "r", encoding="utf-8") as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        items.append(json.loads(line))
                    except json.JSONDecodeError:
                        # ë¦¬ë”© ì œë¡œ ë“± ë¹„ì •í˜• ë°©ì–´
                        line = line.replace(": .", ": 0.")
                        try:
                            items.append(json.loads(line))
                        except json.JSONDecodeError as e:
                            self.logger.warning(f"JSONL íŒŒì‹± ì‹¤íŒ¨ {path}:{line_num} - {e}")
        except Exception as e:
            self.logger.error(f"JSONL íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {path}: {e}")

        return items

    def normalize_metrics(self, rec: Dict[str, Any]) -> Dict[str, float]:
        """ë©”íŠ¸ë¦­ ì •ê·œí™” (ê¸°ì¡´ ë°©ì‹ í™•ì¥)"""
        lat = rec.get("p95_latency_ms") or rec.get("latency_ms") or rec.get("latency") or 0
        acc = rec.get("accuracy", 0)
        exp = rec.get("explainability", rec.get("explain", 0))
        status = rec.get("status", "ok")
        fail = 1.0 if status not in {"ok", "success"} else 0.0
        ts = rec.get("timestamp")

        return {
            "ts": ts,
            "latency_ms": float(lat),
            "accuracy": float(acc),
            "explainability": float(exp),
            "failure": float(fail),
        }

    def bin_key(self, ts_iso: str, bin_minutes: int) -> datetime:
        """ì‹œê°„ bin í‚¤ ìƒì„±"""
        dt = datetime.fromisoformat(ts_iso.replace("Z", "+00:00")).astimezone(timezone.utc)
        minute = (dt.minute // bin_minutes) * bin_minutes
        dt2 = dt.replace(minute=0, second=0, microsecond=0) + timedelta(minutes=minute)
        return dt2.replace(tzinfo=timezone.utc)

    def p95(self, xs: List[float]) -> float:
        """P95 ê³„ì‚°"""
        if not xs:
            return 0.0
        xs = sorted(xs)
        k = max(0, min(len(xs) - 1, int(round(0.95 * (len(xs) - 1)))))
        return float(xs[k])

    def eval_objective_function(self, metrics_dict: Dict[str, float]) -> float:
        """ëª©ì í•¨ìˆ˜ J ê³„ì‚° (ê¸°ì¡´ evaluate_objective.py í™œìš©)"""
        try:
            with tempfile.NamedTemporaryFile(
                "w", delete=False, suffix=".json", encoding="utf-8"
            ) as tf:
                json.dump(metrics_dict, tf)
                tmp = tf.name

            cmd = [
                "python",
                self.config["objective"]["evaluator"],
                "--metrics",
                tmp,
                "--config",
                self.config["objective"]["config"],
                "--weight_preset",
                self.config["objective"]["weight_preset"],
            ]

            out = subprocess.check_output(cmd, text=True)
            result = json.loads(out)
            return result["J"]
        except Exception as e:
            self.logger.error(f"ëª©ì í•¨ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
        finally:
            try:
                os.remove(tmp)
            except:
                pass

    def calculate_sensitivity(
        self, base_metrics: Dict[str, float], eps: float = 0.001
    ) -> Tuple[float, float, float]:
        """ë¯¼ê°ë„ ê³„ì‚° (âˆ‚J/âˆ‚metric)"""
        J0 = self.eval_objective_function(base_metrics)

        # ì‹¤íŒ¨ìœ¨ ë¯¼ê°ë„
        m2 = dict(base_metrics)
        m2["failure_rate"] = max(0.0, min(1.0, base_metrics["failure_rate"] + eps))
        J1 = self.eval_objective_function(m2)
        dJ_dfail = (J1 - J0) / eps

        # ì§€ì—°ì‹œê°„ ë¯¼ê°ë„
        m3 = dict(base_metrics)
        m3["latency_ms"] = base_metrics["latency_ms"] + 50.0
        J2 = self.eval_objective_function(m3)
        dJ_dlat = (J2 - J0) / 50.0

        return J0, dJ_dfail, dJ_dlat

    def collect_domain_metrics(self, domain: str) -> Dict[str, Any]:
        """ë„ë©”ì¸ë³„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (ê¸°ì¡´ Day34 ë°©ì‹ í™•ì¥)"""
        files = []
        for pat in self.config["log_globs"][domain].split(","):
            files.extend(glob.glob(pat.strip()))

        by_bin = defaultdict(list)
        for fp in files:
            for rec in self.parse_jsonl(fp):
                if not rec.get("timestamp"):
                    continue
                r = self.normalize_metrics(rec)
                by_bin[self.bin_key(r["ts"], self.config["bin_minutes"])].append(r)

        if not by_bin:
            return {"status": "no_data", "metrics": {}}

        # ìµœì‹  bin ì²˜ë¦¬
        latest_complete = max(by_bin.keys())
        rows = by_bin[latest_complete]

        lat_list = [r["latency_ms"] for r in rows]
        acc_list = [r["accuracy"] for r in rows]
        exp_list = [r["explainability"] for r in rows]
        fail_list = [r["failure"] for r in rows]

        metrics = {
            "latency_ms_p95": self.p95(lat_list),
            "accuracy_mean": mean(acc_list) if acc_list else 0.0,
            "explain_mean": mean(exp_list) if exp_list else 0.0,
            "failure_rate": mean(fail_list) if fail_list else 0.0,
            "n": len(rows),
            "bin_start_utc": latest_complete.isoformat(),
        }

        # ëª©ì í•¨ìˆ˜ ê³„ì‚°
        J_input = {
            "latency_ms": metrics["latency_ms_p95"],
            "accuracy": metrics["accuracy_mean"],
            "explainability": metrics["explain_mean"],
            "failure_rate": metrics["failure_rate"],
        }

        J, dJ_dfail, dJ_dlat = self.calculate_sensitivity(J_input)
        metrics["J"] = J
        metrics["dJ_dfail"] = dJ_dfail
        metrics["dJ_dlat"] = dJ_dlat

        return {"status": "active", "metrics": metrics}

    def check_alerts(self, domain: str, metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì•ŒëŒ ì²´í¬ (ê¸°ì¡´ Day34 ë°©ì‹ í™•ì¥)"""
        alerts = []
        al_cfg = self.config["alerts"]

        # ì‹¤íŒ¨ìœ¨ ì•ŒëŒ
        if metrics["failure_rate"] > al_cfg["fail_rate_p95_threshold"]:
            alerts.append(
                {
                    "type": "FAIL_RATE_HIGH",
                    "domain": domain,
                    "value": metrics["failure_rate"],
                    "threshold": al_cfg["fail_rate_p95_threshold"],
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                }
            )

        # ì§€ì—°ì‹œê°„ ì•ŒëŒ
        if metrics["latency_ms_p95"] > al_cfg["latency_p95_ms_threshold"]:
            alerts.append(
                {
                    "type": "LATENCY_HIGH",
                    "domain": domain,
                    "value": metrics["latency_ms_p95"],
                    "threshold": al_cfg["latency_p95_ms_threshold"],
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                }
            )

        return alerts

    def save_time_series(self, domain: str, metrics: Dict[str, Any]):
        """ì‹œê³„ì—´ ë°ì´í„° ì €ì¥"""
        series_dir = Path(self.config["outdir"]["series"])
        series_dir.mkdir(parents=True, exist_ok=True)

        csv_file = series_dir / f"{domain}.csv"
        row = {
            "ts_utc": metrics["bin_start_utc"],
            "n": metrics["n"],
            "latency_ms_p95": round(metrics["latency_ms_p95"], 3),
            "accuracy_mean": round(metrics["accuracy_mean"], 6),
            "explain_mean": round(metrics["explain_mean"], 6),
            "failure_rate": round(metrics["failure_rate"], 6),
            "J": round(metrics["J"], 9),
            "dJ_dfail": round(metrics["dJ_dfail"], 6),
            "dJ_dlat": round(metrics["dJ_dlat"], 9),
        }

        header = [
            "ts_utc",
            "n",
            "latency_ms_p95",
            "accuracy_mean",
            "explain_mean",
            "failure_rate",
            "J",
            "dJ_dfail",
            "dJ_dlat",
        ]

        new_file = not csv_file.exists()
        with open(csv_file, "a", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=header)
            if new_file:
                writer.writeheader()
            writer.writerow(row)

    def generate_enhanced_dashboard(self) -> Dict[str, Any]:
        """ê³ ë„í™”ëœ ëŒ€ì‹œë³´ë“œ ìƒì„± (ê¸°ì¡´ Day34 í™•ì¥)"""
        dashboard_data = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "overall_status": "healthy",
            "pilots": {},
            "summary_metrics": {},
            "alerts": [],
            "recommendations": [],
            "day38_features": {
                "objective_function": True,
                "sensitivity_analysis": True,
                "time_series": True,
                "enhanced_alerts": True,
            },
        }

        total_quality = 0
        total_safety = 0
        total_performance = 0
        total_error_rate = 0
        total_J = 0
        active_pilots = 0

        for pilot_name in self.pilots.keys():
            result = self.collect_domain_metrics(pilot_name)
            self.pilots[pilot_name]["metrics"] = result["metrics"]
            self.pilots[pilot_name]["last_update"] = datetime.now()

            if result["status"] == "active":
                metrics = result["metrics"]

                # ì‹œê³„ì—´ ì €ì¥
                self.save_time_series(pilot_name, metrics)

                # ì•ŒëŒ ì²´í¬
                alerts = self.check_alerts(pilot_name, metrics)
                dashboard_data["alerts"].extend(alerts)

                # ëŒ€ì‹œë³´ë“œ ë°ì´í„° êµ¬ì„±
                dashboard_data["pilots"][pilot_name] = {
                    "status": "healthy",
                    "last_updated": metrics["bin_start_utc"],
                    "quality_score": round(metrics["accuracy_mean"] * 100, 1),
                    "safety_score": round((1 - metrics["failure_rate"]) * 100, 1),
                    "performance_ms": round(metrics["latency_ms_p95"], 0),
                    "error_rate_percent": round(metrics["failure_rate"] * 100, 2),
                    "explainability_score": round(metrics["explain_mean"] * 100, 1),
                    "objective_function_J": round(metrics["J"], 6),
                    "sensitivity_failure": round(metrics["dJ_dfail"], 3),
                    "sensitivity_latency": round(metrics["dJ_dlat"], 6),
                }

                total_quality += metrics["accuracy_mean"] * 100
                total_safety += (1 - metrics["failure_rate"]) * 100
                total_performance += metrics["latency_ms_p95"]
                total_error_rate += metrics["failure_rate"] * 100
                total_J += metrics["J"]
                active_pilots += 1

        # ìš”ì•½ ë©”íŠ¸ë¦­ ê³„ì‚°
        if active_pilots > 0:
            dashboard_data["summary_metrics"] = {
                "avg_quality_score": round(total_quality / active_pilots, 1),
                "avg_safety_score": round(total_safety / active_pilots, 1),
                "avg_performance_ms": round(total_performance / active_pilots, 0),
                "avg_error_rate_percent": round(total_error_rate / active_pilots, 2),
                "avg_objective_function_J": round(total_J / active_pilots, 6),
                "active_pilots": active_pilots,
                "total_alerts": len(dashboard_data["alerts"]),
            }

        return dashboard_data

    def run_monitoring_cycle(self):
        """ëª¨ë‹ˆí„°ë§ ì‚¬ì´í´ ì‹¤í–‰ (ê¸°ì¡´ Day34 ë°©ì‹ í™•ì¥)"""
        self.logger.info("ğŸš€ Day38 Enhanced PoU ëª¨ë‹ˆí„°ë§ ì‹œì‘")

        dashboard_data = self.generate_enhanced_dashboard()

        # ê²°ê³¼ ì €ì¥
        self._save_dashboard(dashboard_data)

        # ì½˜ì†” ì¶œë ¥
        self._print_dashboard_summary(dashboard_data)

        self.logger.info("âœ… Day38 Enhanced PoU ëª¨ë‹ˆí„°ë§ ì™„ë£Œ")

    def _save_dashboard(self, dashboard_data: Dict[str, Any]):
        """ëŒ€ì‹œë³´ë“œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        dashboard_filename = f"artifacts/day38/day38_enhanced_dashboard_{timestamp}.json"

        Path(dashboard_filename).parent.mkdir(parents=True, exist_ok=True)
        with open(dashboard_filename, "w", encoding="utf-8") as f:
            json.dump(dashboard_data, f, ensure_ascii=False, indent=2)

        self.logger.info(f"ğŸ“Š ëŒ€ì‹œë³´ë“œ ì €ì¥ ì™„ë£Œ: {dashboard_filename}")

    def _print_dashboard_summary(self, dashboard_data: Dict[str, Any]):
        """ëŒ€ì‹œë³´ë“œ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ“Š Day38 Enhanced PoU ëª¨ë‹ˆí„°ë§ ê²°ê³¼")
        print("=" * 80)
        print(f"ì „ì²´ ìƒíƒœ: {dashboard_data['overall_status'].upper()}")
        print(f"í™œì„± íŒŒì¼ëŸ¿: {dashboard_data['summary_metrics'].get('active_pilots', 0)}ê°œ")
        print(f"í‰ê·  í’ˆì§ˆ ì ìˆ˜: {dashboard_data['summary_metrics'].get('avg_quality_score', 0)}")
        print(f"í‰ê·  ì•ˆì „ ì ìˆ˜: {dashboard_data['summary_metrics'].get('avg_safety_score', 0)}")
        print(f"í‰ê·  ì„±ëŠ¥: {dashboard_data['summary_metrics'].get('avg_performance_ms', 0)}ms")
        print(f"í‰ê·  ì˜¤ë¥˜ìœ¨: {dashboard_data['summary_metrics'].get('avg_error_rate_percent', 0)}%")
        print(
            f"í‰ê·  ëª©ì í•¨ìˆ˜ J: {dashboard_data['summary_metrics'].get('avg_objective_function_J', 0)}"
        )
        print(f"ì´ ì•Œë¦¼ ìˆ˜: {dashboard_data['summary_metrics'].get('total_alerts', 0)}")

        if dashboard_data["alerts"]:
            print("\nâš ï¸ ì•Œë¦¼:")
            for alert in dashboard_data["alerts"]:
                print(f"  - {alert['domain']}: {alert['type']} - {alert['value']}")

        print("=" * 80)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Day38 Enhanced PoU ëª¨ë‹ˆí„°ë§")
    parser.add_argument("--config", default="configs/monitoring.yaml", help="ì„¤ì • íŒŒì¼")
    parser.add_argument("--loop", action="store_true", help="ì§€ì† ì‹¤í–‰ (60ì´ˆ ì£¼ê¸°)")
    parser.add_argument("--verbose", "-v", action="store_true", help="ìƒì„¸ ë¡œê·¸")

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    monitoring_system = EnhancedPoUMonitoringSystem(args.config)

    if args.loop:
        while True:
            try:
                monitoring_system.run_monitoring_cycle()
            except Exception as e:
                print(f"[monitoring] error: {e}")
            time.sleep(60)
    else:
        monitoring_system.run_monitoring_cycle()


if __name__ == "__main__":
    main()
