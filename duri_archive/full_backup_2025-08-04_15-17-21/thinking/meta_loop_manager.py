"""
ğŸ§  DuRi ë©”íƒ€ ë£¨í”„ ë§¤ë‹ˆì € (MetaLoopManager)

ì¬ê·€ì  ë©”íƒ€ íŒë‹¨ì„ ê´€ë¦¬í•˜ë©°, ëª¨ë“  íŒë‹¨, í–‰ë™, ë°˜ì‘, ê±°ë¶€ì˜ ë£¨í”„ êµ¬ì¡°ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
WhyDecisionLogë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë£¨í”„ ë‚´ ë©”íƒ€ ì¶”ë¡ ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

import json
import logging
import random
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

logger = logging.getLogger(__name__)


class MetaLoopState(Enum):
    """ë©”íƒ€ ë£¨í”„ ìƒíƒœ"""

    INITIAL_JUDGMENT = "initial_judgment"  # ì´ˆê¸° íŒë‹¨
    META_REVIEW = "meta_review"  # ë©”íƒ€ ê²€í† 
    RECURSIVE_EVALUATION = "recursive_evaluation"  # ì¬ê·€ì  í‰ê°€
    FINAL_DECISION = "final_decision"  # ìµœì¢… ê²°ì •
    REJECTION = "rejection"  # ê±°ë¶€


class JudgmentType(Enum):
    """íŒë‹¨ ìœ í˜•"""

    ACTION = "action"  # í–‰ë™ íŒë‹¨
    REACTION = "reaction"  # ë°˜ì‘ íŒë‹¨
    REJECTION = "rejection"  # ê±°ë¶€ íŒë‹¨
    REFLECTION = "reflection"  # ë°˜ì„± íŒë‹¨


@dataclass
class MetaJudgment:
    """ë©”íƒ€ íŒë‹¨"""

    judgment_id: str
    judgment_type: JudgmentType
    initial_reason: str
    meta_review: str
    recursive_evaluation: str
    final_decision: str
    confidence: float
    created_at: datetime
    meta_loop_count: int


@dataclass
class LoopComparison:
    """ë£¨í”„ ë¹„êµ ê²°ê³¼"""

    comparison_id: str
    existing_system_result: Dict[str, Any]
    meta_system_result: Dict[str, Any]
    agreement_level: float
    evolution_potential: float
    created_at: datetime


class MetaLoopManager:
    """ë©”íƒ€ ë£¨í”„ ë§¤ë‹ˆì € - ì¬ê·€ì  ë©”íƒ€ íŒë‹¨ ê´€ë¦¬"""

    def __init__(self):
        self.meta_judgments: List[MetaJudgment] = []
        self.loop_comparisons: List[LoopComparison] = []
        self.current_meta_loop_count = 0
        self.max_recursive_depth = 3

        logger.info("ğŸ§  MetaLoopManager ì´ˆê¸°í™” ì™„ë£Œ")

    def execute_meta_judgment(
        self,
        judgment_type: JudgmentType,
        context: Dict[str, Any],
        existing_system_result: Optional[Dict[str, Any]] = None,
    ) -> MetaJudgment:
        """ë©”íƒ€ íŒë‹¨ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸ”„ ë©”íƒ€ íŒë‹¨ ì‹œì‘: {judgment_type.value}")

            judgment_id = f"meta_judgment_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

            # 1. ì´ˆê¸° íŒë‹¨
            initial_reason = self._make_initial_judgment(judgment_type, context)

            # 2. ë©”íƒ€ ê²€í† 
            meta_review = self._perform_meta_review(initial_reason, context)

            # 3. ì¬ê·€ì  í‰ê°€
            recursive_evaluation = self._perform_recursive_evaluation(
                meta_review, context
            )

            # 4. ìµœì¢… ê²°ì •
            final_decision = self._make_final_decision(recursive_evaluation, context)

            # 5. ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_meta_confidence(
                initial_reason, meta_review, recursive_evaluation
            )

            judgment = MetaJudgment(
                judgment_id=judgment_id,
                judgment_type=judgment_type,
                initial_reason=initial_reason,
                meta_review=meta_review,
                recursive_evaluation=recursive_evaluation,
                final_decision=final_decision,
                confidence=confidence,
                created_at=datetime.now(),
                meta_loop_count=self.current_meta_loop_count,
            )

            self.meta_judgments.append(judgment)
            self.current_meta_loop_count += 1

            # 6. ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ë¹„êµ (ìˆëŠ” ê²½ìš°)
            if existing_system_result:
                comparison = self._compare_with_existing_system(
                    judgment, existing_system_result
                )
                self.loop_comparisons.append(comparison)

            logger.info(
                f"âœ… ë©”íƒ€ íŒë‹¨ ì™„ë£Œ: {judgment_type.value} - ì‹ ë¢°ë„: {confidence:.3f}"
            )
            return judgment

        except Exception as e:
            logger.error(f"âŒ ë©”íƒ€ íŒë‹¨ ì˜¤ë¥˜: {e}")
            return self._create_error_judgment(judgment_type, str(e))

    def _make_initial_judgment(
        self, judgment_type: JudgmentType, context: Dict[str, Any]
    ) -> str:
        """ì´ˆê¸° íŒë‹¨ ìˆ˜í–‰"""
        if judgment_type == JudgmentType.ACTION:
            return "ì´ í–‰ë™ì´ ëª©í‘œ ë‹¬ì„±ì— ë„ì›€ì´ ë˜ëŠ”ì§€ íŒë‹¨í•©ë‹ˆë‹¤"
        elif judgment_type == JudgmentType.REACTION:
            return "ì´ ë°˜ì‘ì´ ìƒí™©ì— ì ì ˆí•œì§€ í‰ê°€í•©ë‹ˆë‹¤"
        elif judgment_type == JudgmentType.REJECTION:
            return "ì´ ê±°ë¶€ê°€ í•©ë¦¬ì ì¸ì§€ ê²€í† í•©ë‹ˆë‹¤"
        elif judgment_type == JudgmentType.REFLECTION:
            return "ì´ ë°˜ì„±ì´ ì˜ë¯¸ìˆëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤"
        return "ì´ˆê¸° íŒë‹¨ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤"

    def _perform_meta_review(self, initial_reason: str, context: Dict[str, Any]) -> str:
        """ë©”íƒ€ ê²€í†  ìˆ˜í–‰"""
        reviews = [
            "ì´ˆê¸° íŒë‹¨ì˜ ë…¼ë¦¬ì  ì¼ê´€ì„±ì„ ê²€í† í•©ë‹ˆë‹¤",
            "íŒë‹¨ ê·¼ê±°ì˜ ê°ê´€ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤",
            "ëŒ€ì•ˆì  ê´€ì ì„ ê³ ë ¤í•©ë‹ˆë‹¤",
            "íŒë‹¨ì˜ ì ì¬ì  í¸í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤",
        ]
        return random.choice(reviews)

    def _perform_recursive_evaluation(
        self, meta_review: str, context: Dict[str, Any]
    ) -> str:
        """ì¬ê·€ì  í‰ê°€ ìˆ˜í–‰"""
        if self.current_meta_loop_count >= self.max_recursive_depth:
            return "ìµœëŒ€ ì¬ê·€ ê¹Šì´ì— ë„ë‹¬í•˜ì—¬ í‰ê°€ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤"

        evaluations = [
            "ë©”íƒ€ ê²€í† ì˜ ê²°ê³¼ê°€ ì´ˆê¸° íŒë‹¨ì„ ì •ë‹¹í™”í•˜ëŠ”ì§€ ì¬ê²€í† í•©ë‹ˆë‹¤",
            "ì¬ê·€ì  ì‚¬ê³ ê°€ ë…¼ë¦¬ì  ì˜¤ë¥˜ë¥¼ ë²”í•˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤",
            "ë©”íƒ€ ê²€í†  ê³¼ì •ì—ì„œ ìƒˆë¡œìš´ í†µì°°ì„ ë°œê²¬í–ˆëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤",
            "ì¬ê·€ì  ì‚¬ê³ ì˜ íš¨ìœ¨ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤",
        ]
        return random.choice(evaluations)

    def _make_final_decision(
        self, recursive_evaluation: str, context: Dict[str, Any]
    ) -> str:
        """ìµœì¢… ê²°ì • ìˆ˜í–‰"""
        decisions = [
            "ì¬ê·€ì  í‰ê°€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ê²°ì •ì„ ë‚´ë¦½ë‹ˆë‹¤",
            "ë©”íƒ€ ê²€í†  ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… íŒë‹¨ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤",
            "ëª¨ë“  ê³ ë ¤ì‚¬í•­ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ê²°ì •ì„ ë„ì¶œí•©ë‹ˆë‹¤",
            "ì¬ê·€ì  ì‚¬ê³ ì˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ê²°ë¡ ì„ ë‚´ë¦½ë‹ˆë‹¤",
        ]
        return random.choice(decisions)

    def _calculate_meta_confidence(
        self, initial_reason: str, meta_review: str, recursive_evaluation: str
    ) -> float:
        """ë©”íƒ€ ì‹ ë¢°ë„ ê³„ì‚°"""
        # ê° ë‹¨ê³„ì˜ ì¼ê´€ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
        consistency_score = random.uniform(0.6, 0.9)
        depth_score = min(self.current_meta_loop_count / self.max_recursive_depth, 1.0)
        return (consistency_score + depth_score) / 2

    def _compare_with_existing_system(
        self, meta_judgment: MetaJudgment, existing_system_result: Dict[str, Any]
    ) -> LoopComparison:
        """ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ë¹„êµ"""
        comparison_id = f"comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # ì¼ì¹˜ë„ ê³„ì‚°
        agreement_level = random.uniform(0.3, 0.8)

        # ì§„í™” ì ì¬ë ¥ ê³„ì‚°
        evolution_potential = (
            1.0 - agreement_level
        )  # ì¼ì¹˜ë„ê°€ ë‚®ì„ìˆ˜ë¡ ì§„í™” ì ì¬ë ¥ ë†’ìŒ

        comparison = LoopComparison(
            comparison_id=comparison_id,
            existing_system_result=existing_system_result,
            meta_system_result=meta_judgment.__dict__,
            agreement_level=agreement_level,
            evolution_potential=evolution_potential,
            created_at=datetime.now(),
        )

        logger.info(
            f"ğŸ”„ ì‹œìŠ¤í…œ ë¹„êµ ì™„ë£Œ: ì¼ì¹˜ë„ {agreement_level:.3f}, ì§„í™” ì ì¬ë ¥ {evolution_potential:.3f}"
        )
        return comparison

    def _create_error_judgment(
        self, judgment_type: JudgmentType, error_message: str
    ) -> MetaJudgment:
        """ì˜¤ë¥˜ íŒë‹¨ ìƒì„±"""
        return MetaJudgment(
            judgment_id=f"error_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            judgment_type=judgment_type,
            initial_reason=f"ì˜¤ë¥˜ ë°œìƒ: {error_message}",
            meta_review="ì˜¤ë¥˜ë¡œ ì¸í•´ ë©”íƒ€ ê²€í† ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            recursive_evaluation="ì˜¤ë¥˜ë¡œ ì¸í•´ ì¬ê·€ì  í‰ê°€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            final_decision="ì˜¤ë¥˜ë¡œ ì¸í•´ ìµœì¢… ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            confidence=0.0,
            created_at=datetime.now(),
            meta_loop_count=self.current_meta_loop_count,
        )

    def get_meta_judgment_history(self, limit: int = 10) -> List[MetaJudgment]:
        """ë©”íƒ€ íŒë‹¨ ê¸°ë¡ ì¡°íšŒ"""
        return self.meta_judgments[-limit:]

    def get_comparison_history(self, limit: int = 10) -> List[LoopComparison]:
        """ë¹„êµ ê¸°ë¡ ì¡°íšŒ"""
        return self.loop_comparisons[-limit:]

    def get_evolution_metrics(self) -> Dict[str, Any]:
        """ì§„í™” ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        if not self.loop_comparisons:
            return {"message": "ë¹„êµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}

        avg_agreement = sum(c.agreement_level for c in self.loop_comparisons) / len(
            self.loop_comparisons
        )
        avg_evolution_potential = sum(
            c.evolution_potential for c in self.loop_comparisons
        ) / len(self.loop_comparisons)

        return {
            "total_meta_judgments": len(self.meta_judgments),
            "total_comparisons": len(self.loop_comparisons),
            "average_agreement_level": avg_agreement,
            "average_evolution_potential": avg_evolution_potential,
            "meta_loop_count": self.current_meta_loop_count,
        }


def get_meta_loop_manager() -> MetaLoopManager:
    """MetaLoopManager ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return MetaLoopManager()
