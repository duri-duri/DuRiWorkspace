"""
Phase 25: ìœ¤ë¦¬ì  íŒë‹¨ ì‹œìŠ¤í…œ (Ethical Judgment System)
ì±…ì„ ìˆëŠ” AI ì˜ì‚¬ê²°ì •ê³¼ ì‚¬íšŒì  ì˜í–¥ ê³ ë ¤
"""

import json
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

class EthicalPrinciple(Enum):
    BENEFICENCE = "beneficence"           # ì„ í–‰ ì›ì¹™
    NON_MALEFICENCE = "non_maleficence"   # ë¬´í•´ ì›ì¹™
    AUTONOMY = "autonomy"                 # ììœ¨ì„± ì›ì¹™
    JUSTICE = "justice"                   # ì •ì˜ ì›ì¹™
    TRANSPARENCY = "transparency"         # íˆ¬ëª…ì„± ì›ì¹™
    ACCOUNTABILITY = "accountability"     # ì±…ì„ì„± ì›ì¹™

class ImpactLevel(Enum):
    LOW = "low"           # ë‚®ì€ ì˜í–¥
    MEDIUM = "medium"     # ì¤‘ê°„ ì˜í–¥
    HIGH = "high"         # ë†’ì€ ì˜í–¥
    CRITICAL = "critical" # ì¤‘ìš” ì˜í–¥

@dataclass
class EthicalAnalysis:
    """ìœ¤ë¦¬ì  ë¶„ì„ ê²°ê³¼"""
    principles_applied: List[EthicalPrinciple]
    impact_assessment: Dict[str, ImpactLevel]
    risk_factors: List[str]
    mitigation_strategies: List[str]
    stakeholder_considerations: List[str]
    ethical_score: float

@dataclass
class SocialImpactAssessment:
    """ì‚¬íšŒì  ì˜í–¥ í‰ê°€"""
    direct_impact: Dict[str, str]
    indirect_impact: Dict[str, str]
    long_term_effects: List[str]
    vulnerable_groups: List[str]
    benefit_distribution: Dict[str, str]

class EthicalJudgmentSystem:
    """Phase 25: ìœ¤ë¦¬ì  íŒë‹¨ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.ethical_guidelines = self._load_ethical_guidelines()
        self.decision_history = []
        self.impact_assessments = {}
        self.ethical_frameworks = self._load_ethical_frameworks()
        
    def _load_ethical_guidelines(self) -> Dict[str, Any]:
        """ìœ¤ë¦¬ì  ê°€ì´ë“œë¼ì¸ ë¡œë“œ"""
        return {
            "human_centric": {
                "principle": "ì¸ê°„ ì¤‘ì‹¬",
                "description": "ëª¨ë“  ê²°ì •ì—ì„œ ì¸ê°„ì˜ ë³µì§€ì™€ ê¶Œìµì„ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤",
                "weight": 0.25
            },
            "transparency": {
                "principle": "íˆ¬ëª…ì„±",
                "description": "ì˜ì‚¬ê²°ì • ê³¼ì •ê³¼ ê·¼ê±°ë¥¼ ëª…í™•í•˜ê²Œ ê³µê°œ",
                "weight": 0.20
            },
            "accountability": {
                "principle": "ì±…ì„ì„±",
                "description": "AIì˜ í–‰ë™ê³¼ ê²°ê³¼ì— ëŒ€í•œ ì±…ì„ì„ ëª…í™•íˆ í•¨",
                "weight": 0.20
            },
            "fairness": {
                "principle": "ê³µì •ì„±",
                "description": "ëª¨ë“  ì´í•´ê´€ê³„ìì—ê²Œ ê³µì •í•œ ê¸°íšŒì™€ ê²°ê³¼ ì œê³µ",
                "weight": 0.15
            },
            "privacy": {
                "principle": "í”„ë¼ì´ë²„ì‹œ",
                "description": "ê°œì¸ì •ë³´ì™€ í”„ë¼ì´ë²„ì‹œë¥¼ ì ê·¹ì ìœ¼ë¡œ ë³´í˜¸",
                "weight": 0.10
            },
            "safety": {
                "principle": "ì•ˆì „ì„±",
                "description": "ì•ˆì „í•˜ê³  í•´ë¡œìš´ ê²°ê³¼ë¥¼ ë°©ì§€í•˜ëŠ” ì¡°ì¹˜",
                "weight": 0.10
            }
        }
    
    def _load_ethical_frameworks(self) -> Dict[str, Any]:
        """ìœ¤ë¦¬ì  í”„ë ˆì„ì›Œí¬ ë¡œë“œ"""
        return {
            "utilitarianism": {
                "name": "ê³µë¦¬ì£¼ì˜",
                "focus": "ìµœëŒ€ ë‹¤ìˆ˜ì˜ ìµœëŒ€ í–‰ë³µ",
                "evaluation_method": "ê²°ê³¼ì˜ íš¨ìš©ì„± í‰ê°€"
            },
            "deontology": {
                "name": "ì˜ë¬´ë¡ ",
                "focus": "í–‰ë™ì˜ ë„ë•ì  ì˜ë¬´",
                "evaluation_method": "ì˜ë¬´ì™€ ê¶Œë¦¬ì˜ ì¤€ìˆ˜"
            },
            "virtue_ethics": {
                "name": "ë•ìœ¤ë¦¬",
                "focus": "ë•ìŠ¤ëŸ¬ìš´ ì„±ê²©ê³¼ í–‰ë™",
                "evaluation_method": "ë•ì˜ ì‹¤í˜„ê³¼ ì„±ì¥"
            },
            "care_ethics": {
                "name": "ëŒë´„ ìœ¤ë¦¬",
                "focus": "ê´€ê³„ì™€ ëŒë´„ì˜ ê°€ì¹˜",
                "evaluation_method": "ê´€ê³„ì˜ ì§ˆê³¼ ëŒë´„ì˜ ì‹¤í˜„"
            }
        }
    
    def analyze_ethical_implications(self, decision_context: Dict[str, Any], proposed_action: str) -> EthicalAnalysis:
        """ìœ¤ë¦¬ì  í•¨ì˜ ë¶„ì„"""
        print(f"âš–ï¸ ìœ¤ë¦¬ì  í•¨ì˜ ë¶„ì„ ì‹œì‘: {proposed_action[:50]}...")
        
        # ì ìš©í•  ìœ¤ë¦¬ì  ì›ì¹™ ì‹ë³„
        principles_applied = self._identify_applicable_principles(decision_context, proposed_action)
        
        # ì˜í–¥ í‰ê°€
        impact_assessment = self._assess_impact_levels(decision_context, proposed_action)
        
        # ìœ„í—˜ ìš”ì†Œ ì‹ë³„
        risk_factors = self._identify_risk_factors(decision_context, proposed_action)
        
        # ì™„í™” ì „ëµ ê°œë°œ
        mitigation_strategies = self._develop_mitigation_strategies(risk_factors)
        
        # ì´í•´ê´€ê³„ì ê³ ë ¤ì‚¬í•­
        stakeholder_considerations = self._identify_stakeholder_considerations(decision_context)
        
        # ìœ¤ë¦¬ì  ì ìˆ˜ ê³„ì‚°
        ethical_score = self._calculate_ethical_score(principles_applied, impact_assessment, risk_factors)
        
        analysis = EthicalAnalysis(
            principles_applied=principles_applied,
            impact_assessment=impact_assessment,
            risk_factors=risk_factors,
            mitigation_strategies=mitigation_strategies,
            stakeholder_considerations=stakeholder_considerations,
            ethical_score=ethical_score
        )
        
        print(f"âœ… ìœ¤ë¦¬ì  í•¨ì˜ ë¶„ì„ ì™„ë£Œ: ì ìˆ˜ {ethical_score:.2f}")
        
        return analysis
    
    def _identify_applicable_principles(self, context: Dict[str, Any], action: str) -> List[EthicalPrinciple]:
        """ì ìš© ê°€ëŠ¥í•œ ìœ¤ë¦¬ì  ì›ì¹™ ì‹ë³„"""
        applicable_principles = []
        
        # ì¸ê°„ ì¤‘ì‹¬ì„± ê²€ì‚¬
        if self._involves_human_welfare(context, action):
            applicable_principles.append(EthicalPrinciple.BENEFICENCE)
            applicable_principles.append(EthicalPrinciple.NON_MALEFICENCE)
        
        # ììœ¨ì„± ê²€ì‚¬
        if self._involves_autonomy(context, action):
            applicable_principles.append(EthicalPrinciple.AUTONOMY)
        
        # ê³µì •ì„± ê²€ì‚¬
        if self._involves_fairness(context, action):
            applicable_principles.append(EthicalPrinciple.JUSTICE)
        
        # íˆ¬ëª…ì„± ê²€ì‚¬
        if self._requires_transparency(context, action):
            applicable_principles.append(EthicalPrinciple.TRANSPARENCY)
        
        # ì±…ì„ì„± ê²€ì‚¬
        if self._requires_accountability(context, action):
            applicable_principles.append(EthicalPrinciple.ACCOUNTABILITY)
        
        return applicable_principles
    
    def _involves_human_welfare(self, context: Dict[str, Any], action: str) -> bool:
        """ì¸ê°„ ë³µì§€ ê´€ë ¨ ì—¬ë¶€ í™•ì¸"""
        welfare_keywords = ["ì‚¬ìš©ì", "ì‚¬ëŒ", "ì‚¬ìš©ì", "ë³µì§€", "ì•ˆì „", "ê±´ê°•", "ê¶Œìµ"]
        return any(keyword in action for keyword in welfare_keywords)
    
    def _involves_autonomy(self, context: Dict[str, Any], action: str) -> bool:
        """ììœ¨ì„± ê´€ë ¨ ì—¬ë¶€ í™•ì¸"""
        autonomy_keywords = ["ì„ íƒ", "ê²°ì •", "ììœ¨", "ê¶Œë¦¬", "ì˜ì‚¬"]
        return any(keyword in action for keyword in autonomy_keywords)
    
    def _involves_fairness(self, context: Dict[str, Any], action: str) -> bool:
        """ê³µì •ì„± ê´€ë ¨ ì—¬ë¶€ í™•ì¸"""
        fairness_keywords = ["ì°¨ë³„", "ê³µì •", "í‰ë“±", "ê¸°íšŒ", "ì ‘ê·¼"]
        return any(keyword in action for keyword in fairness_keywords)
    
    def _requires_transparency(self, context: Dict[str, Any], action: str) -> bool:
        """íˆ¬ëª…ì„± ìš”êµ¬ ì—¬ë¶€ í™•ì¸"""
        transparency_keywords = ["ì„¤ëª…", "ê³µê°œ", "íˆ¬ëª…", "ì´í•´", "ëª…í™•"]
        return any(keyword in action for keyword in transparency_keywords)
    
    def _requires_accountability(self, context: Dict[str, Any], action: str) -> bool:
        """ì±…ì„ì„± ìš”êµ¬ ì—¬ë¶€ í™•ì¸"""
        accountability_keywords = ["ì±…ì„", "ê²°ê³¼", "ì˜í–¥", "í‰ê°€", "ê²€ì¦"]
        return any(keyword in action for keyword in accountability_keywords)
    
    def _assess_impact_levels(self, context: Dict[str, Any], action: str) -> Dict[str, ImpactLevel]:
        """ì˜í–¥ ìˆ˜ì¤€ í‰ê°€"""
        impact_assessment = {}
        
        # ê°œì¸ì  ì˜í–¥
        if self._has_personal_impact(context, action):
            impact_assessment["personal"] = ImpactLevel.MEDIUM
        
        # ì‚¬íšŒì  ì˜í–¥
        if self._has_social_impact(context, action):
            impact_assessment["social"] = ImpactLevel.HIGH
        
        # ê²½ì œì  ì˜í–¥
        if self._has_economic_impact(context, action):
            impact_assessment["economic"] = ImpactLevel.MEDIUM
        
        # í™˜ê²½ì  ì˜í–¥
        if self._has_environmental_impact(context, action):
            impact_assessment["environmental"] = ImpactLevel.LOW
        
        # ê¸°ìˆ ì  ì˜í–¥
        if self._has_technological_impact(context, action):
            impact_assessment["technological"] = ImpactLevel.HIGH
        
        return impact_assessment
    
    def _has_personal_impact(self, context: Dict[str, Any], action: str) -> bool:
        """ê°œì¸ì  ì˜í–¥ ì—¬ë¶€"""
        personal_keywords = ["ê°œì¸", "ì‚¬ìš©ì", "í”„ë¼ì´ë²„ì‹œ", "ë°ì´í„°", "ì •ë³´"]
        return any(keyword in action for keyword in personal_keywords)
    
    def _has_social_impact(self, context: Dict[str, Any], action: str) -> bool:
        """ì‚¬íšŒì  ì˜í–¥ ì—¬ë¶€"""
        social_keywords = ["ì‚¬íšŒ", "ì»¤ë®¤ë‹ˆí‹°", "ì§‘ë‹¨", "ë¬¸í™”", "ê´€ìŠµ"]
        return any(keyword in action for keyword in social_keywords)
    
    def _has_economic_impact(self, context: Dict[str, Any], action: str) -> bool:
        """ê²½ì œì  ì˜í–¥ ì—¬ë¶€"""
        economic_keywords = ["ê²½ì œ", "ë¹„ìš©", "ìˆ˜ìµ", "ì‹œì¥", "ê¸ˆìœµ"]
        return any(keyword in action for keyword in economic_keywords)
    
    def _has_environmental_impact(self, context: Dict[str, Any], action: str) -> bool:
        """í™˜ê²½ì  ì˜í–¥ ì—¬ë¶€"""
        environmental_keywords = ["í™˜ê²½", "ìì›", "ì—ë„ˆì§€", "íê¸°ë¬¼", "ì§€ì†ê°€ëŠ¥"]
        return any(keyword in action for keyword in environmental_keywords)
    
    def _has_technological_impact(self, context: Dict[str, Any], action: str) -> bool:
        """ê¸°ìˆ ì  ì˜í–¥ ì—¬ë¶€"""
        technological_keywords = ["ê¸°ìˆ ", "ì‹œìŠ¤í…œ", "ì•Œê³ ë¦¬ì¦˜", "ìë™í™”", "ë””ì§€í„¸"]
        return any(keyword in action for keyword in technological_keywords)
    
    def _identify_risk_factors(self, context: Dict[str, Any], action: str) -> List[str]:
        """ìœ„í—˜ ìš”ì†Œ ì‹ë³„"""
        risk_factors = []
        
        # í”„ë¼ì´ë²„ì‹œ ìœ„í—˜
        if "ê°œì¸ì •ë³´" in action or "ë°ì´í„°" in action:
            risk_factors.append("í”„ë¼ì´ë²„ì‹œ ì¹¨í•´ ìœ„í—˜")
        
        # ì°¨ë³„ ìœ„í—˜
        if "ë¶„ë¥˜" in action or "ì„ íƒ" in action:
            risk_factors.append("ì°¨ë³„ì  ê²°ê³¼ ìœ„í—˜")
        
        # ì•ˆì „ ìœ„í—˜
        if "ìë™í™”" in action or "ì œì–´" in action:
            risk_factors.append("ì•ˆì „ì„± ìœ„í—˜")
        
        # íˆ¬ëª…ì„± ìœ„í—˜
        if "ë³µì¡" in action or "ë¸”ë™ë°•ìŠ¤" in action:
            risk_factors.append("íˆ¬ëª…ì„± ë¶€ì¡± ìœ„í—˜")
        
        return risk_factors
    
    def _develop_mitigation_strategies(self, risk_factors: List[str]) -> List[str]:
        """ì™„í™” ì „ëµ ê°œë°œ"""
        mitigation_strategies = []
        
        for risk in risk_factors:
            if "í”„ë¼ì´ë²„ì‹œ" in risk:
                mitigation_strategies.append("ë°ì´í„° ìµœì†Œí™” ë° ì•”í˜¸í™” ì ìš©")
            elif "ì°¨ë³„" in risk:
                mitigation_strategies.append("í¸í–¥ ê²€ì‚¬ ë° ê³µì •ì„± ëª¨ë‹ˆí„°ë§")
            elif "ì•ˆì „ì„±" in risk:
                mitigation_strategies.append("ì•ˆì „ì¥ì¹˜ ë° ì¸ê°„ ê°ë… ì‹œìŠ¤í…œ")
            elif "íˆ¬ëª…ì„±" in risk:
                mitigation_strategies.append("ì„¤ëª… ê°€ëŠ¥í•œ AI ë° ë¡œê·¸ ê¸°ë¡")
        
        return mitigation_strategies
    
    def _identify_stakeholder_considerations(self, context: Dict[str, Any]) -> List[str]:
        """ì´í•´ê´€ê³„ì ê³ ë ¤ì‚¬í•­ ì‹ë³„"""
        considerations = []
        
        # ì£¼ìš” ì´í•´ê´€ê³„ìë“¤
        stakeholders = ["ìµœì¢… ì‚¬ìš©ì", "ê°œë°œì", "ì¡°ì§", "ì‚¬íšŒ", "í™˜ê²½"]
        
        for stakeholder in stakeholders:
            considerations.append(f"{stakeholder}ì˜ ê¶Œìµ ë³´í˜¸")
        
        return considerations
    
    def _calculate_ethical_score(self, principles: List[EthicalPrinciple], 
                               impact: Dict[str, ImpactLevel], 
                               risks: List[str]) -> float:
        """ìœ¤ë¦¬ì  ì ìˆ˜ ê³„ì‚°"""
        base_score = 0.7
        
        # ì›ì¹™ ì ìš© ë³´ë„ˆìŠ¤
        principle_bonus = len(principles) * 0.05
        base_score += principle_bonus
        
        # ì˜í–¥ ìˆ˜ì¤€ ì¡°ì •
        high_impact_count = sum(1 for level in impact.values() if level == ImpactLevel.HIGH)
        if high_impact_count > 0:
            base_score += 0.1
        
        # ìœ„í—˜ ìš”ì†Œ í˜ë„í‹°
        risk_penalty = len(risks) * 0.05
        base_score -= risk_penalty
        
        return max(0.0, min(1.0, base_score))
    
    def assess_social_impact(self, decision_context: Dict[str, Any], proposed_action: str) -> SocialImpactAssessment:
        """ì‚¬íšŒì  ì˜í–¥ í‰ê°€"""
        print("ğŸŒ ì‚¬íšŒì  ì˜í–¥ í‰ê°€ ì¤‘...")
        
        # ì§ì ‘ì  ì˜í–¥ ë¶„ì„
        direct_impact = self._analyze_direct_impact(decision_context, proposed_action)
        
        # ê°„ì ‘ì  ì˜í–¥ ë¶„ì„
        indirect_impact = self._analyze_indirect_impact(decision_context, proposed_action)
        
        # ì¥ê¸°ì  íš¨ê³¼ ë¶„ì„
        long_term_effects = self._analyze_long_term_effects(decision_context, proposed_action)
        
        # ì·¨ì•½ ê³„ì¸µ ì‹ë³„
        vulnerable_groups = self._identify_vulnerable_groups(decision_context, proposed_action)
        
        # í˜œíƒ ë¶„ë°° ë¶„ì„
        benefit_distribution = self._analyze_benefit_distribution(decision_context, proposed_action)
        
        assessment = SocialImpactAssessment(
            direct_impact=direct_impact,
            indirect_impact=indirect_impact,
            long_term_effects=long_term_effects,
            vulnerable_groups=vulnerable_groups,
            benefit_distribution=benefit_distribution
        )
        
        print("âœ… ì‚¬íšŒì  ì˜í–¥ í‰ê°€ ì™„ë£Œ")
        
        return assessment
    
    def _analyze_direct_impact(self, context: Dict[str, Any], action: str) -> Dict[str, str]:
        """ì§ì ‘ì  ì˜í–¥ ë¶„ì„"""
        direct_impact = {}
        
        if "ì‚¬ìš©ì" in action:
            direct_impact["ì‚¬ìš©ì ê²½í—˜"] = "ê°œì„  ë˜ëŠ” ì €í•˜"
        if "ì‹œìŠ¤í…œ" in action:
            direct_impact["ì‹œìŠ¤í…œ ì„±ëŠ¥"] = "í–¥ìƒ ë˜ëŠ” ì €í•˜"
        if "ë°ì´í„°" in action:
            direct_impact["ë°ì´í„° ì²˜ë¦¬"] = "íš¨ìœ¨ì„± ë³€í™”"
        
        return direct_impact
    
    def _analyze_indirect_impact(self, context: Dict[str, Any], action: str) -> Dict[str, str]:
        """ê°„ì ‘ì  ì˜í–¥ ë¶„ì„"""
        indirect_impact = {}
        
        if "ìë™í™”" in action:
            indirect_impact["ê³ ìš©"] = "ì§ë¬´ ë³€í™” ê°€ëŠ¥ì„±"
        if "ê°œì¸ì •ë³´" in action:
            indirect_impact["í”„ë¼ì´ë²„ì‹œ"] = "ê°œì¸ì •ë³´ ë³´í˜¸ ìˆ˜ì¤€"
        if "ì•Œê³ ë¦¬ì¦˜" in action:
            indirect_impact["ê³µì •ì„±"] = "ê²°ê³¼ì˜ ê³µì •ì„±"
        
        return indirect_impact
    
    def _analyze_long_term_effects(self, context: Dict[str, Any], action: str) -> List[str]:
        """ì¥ê¸°ì  íš¨ê³¼ ë¶„ì„"""
        long_term_effects = []
        
        if "í•™ìŠµ" in action:
            long_term_effects.append("ì§€ì†ì  ì„±ëŠ¥ ê°œì„ ")
        if "ë°ì´í„°" in action:
            long_term_effects.append("ë°ì´í„° ì¶•ì  ë° í™œìš©")
        if "ìë™í™”" in action:
            long_term_effects.append("ì—…ë¬´ ë°©ì‹ ë³€í™”")
        
        return long_term_effects
    
    def _identify_vulnerable_groups(self, context: Dict[str, Any], action: str) -> List[str]:
        """ì·¨ì•½ ê³„ì¸µ ì‹ë³„"""
        vulnerable_groups = []
        
        if "ë””ì§€í„¸" in action:
            vulnerable_groups.append("ë””ì§€í„¸ ê²©ì°¨ ê³„ì¸µ")
        if "ì–¸ì–´" in action:
            vulnerable_groups.append("ì–¸ì–´ ì†Œìˆ˜ì")
        if "ê²½ì œ" in action:
            vulnerable_groups.append("ê²½ì œì  ì·¨ì•½ ê³„ì¸µ")
        
        return vulnerable_groups
    
    def _analyze_benefit_distribution(self, context: Dict[str, Any], action: str) -> Dict[str, str]:
        """í˜œíƒ ë¶„ë°° ë¶„ì„"""
        benefit_distribution = {}
        
        if "ê°œë°œì" in action:
            benefit_distribution["ê°œë°œì"] = "ê°œë°œ íš¨ìœ¨ì„± í–¥ìƒ"
        if "ì‚¬ìš©ì" in action:
            benefit_distribution["ì‚¬ìš©ì"] = "ì‚¬ìš© í¸ì˜ì„± ê°œì„ "
        if "ì¡°ì§" in action:
            benefit_distribution["ì¡°ì§"] = "ìš´ì˜ ë¹„ìš© ì ˆê°"
        
        return benefit_distribution
    
    def make_ethical_decision(self, alternatives: List[Dict[str, Any]], context: Dict[str, Any]) -> Dict[str, Any]:
        """ìœ¤ë¦¬ì  ì˜ì‚¬ê²°ì •"""
        print("âš–ï¸ ìœ¤ë¦¬ì  ì˜ì‚¬ê²°ì • ì‹œì‘...")
        
        # ê° ëŒ€ì•ˆì˜ ìœ¤ë¦¬ì  ë¶„ì„
        ethical_analyses = []
        for alternative in alternatives:
            analysis = self.analyze_ethical_implications(context, alternative["action"])
            social_impact = self.assess_social_impact(context, alternative["action"])
            
            ethical_analyses.append({
                "alternative": alternative,
                "ethical_analysis": analysis,
                "social_impact": social_impact
            })
        
        # ìµœì  ëŒ€ì•ˆ ì„ íƒ
        best_alternative = self._select_best_alternative(ethical_analyses)
        
        # ì˜ì‚¬ê²°ì • ê¸°ë¡
        decision_record = {
            "timestamp": time.time(),
            "context": context,
            "alternatives": alternatives,
            "selected_alternative": best_alternative,
            "reasoning": self._generate_ethical_reasoning(best_alternative)
        }
        
        self.decision_history.append(decision_record)
        
        print("âœ… ìœ¤ë¦¬ì  ì˜ì‚¬ê²°ì • ì™„ë£Œ")
        
        return best_alternative
    
    def _select_best_alternative(self, ethical_analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ìµœì  ëŒ€ì•ˆ ì„ íƒ"""
        best_score = 0.0
        best_alternative = None
        
        for analysis in ethical_analyses:
            ethical_score = analysis["ethical_analysis"].ethical_score
            
            # ì‚¬íšŒì  ì˜í–¥ ê°€ì¤‘ì¹˜ ì ìš©
            social_weight = self._calculate_social_weight(analysis["social_impact"])
            weighted_score = ethical_score * social_weight
            
            if weighted_score > best_score:
                best_score = weighted_score
                best_alternative = analysis
        
        return best_alternative
    
    def _calculate_social_weight(self, social_impact: SocialImpactAssessment) -> float:
        """ì‚¬íšŒì  ì˜í–¥ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        base_weight = 1.0
        
        # ì·¨ì•½ ê³„ì¸µ ì˜í–¥ ê³ ë ¤
        if social_impact.vulnerable_groups:
            base_weight += 0.2
        
        # ì¥ê¸°ì  íš¨ê³¼ ê³ ë ¤
        if social_impact.long_term_effects:
            base_weight += 0.1
        
        return base_weight
    
    def _generate_ethical_reasoning(self, selected_analysis: Dict[str, Any]) -> str:
        """ìœ¤ë¦¬ì  ì¶”ë¡  ìƒì„±"""
        reasoning = "ì„ íƒëœ ëŒ€ì•ˆì˜ ìœ¤ë¦¬ì  ê·¼ê±°:\n"
        
        # ì ìš©ëœ ì›ì¹™ë“¤
        principles = selected_analysis["ethical_analysis"].principles_applied
        reasoning += f"- ì ìš©ëœ ìœ¤ë¦¬ì  ì›ì¹™: {', '.join([p.value for p in principles])}\n"
        
        # ìœ¤ë¦¬ì  ì ìˆ˜
        score = selected_analysis["ethical_analysis"].ethical_score
        reasoning += f"- ìœ¤ë¦¬ì  ì ìˆ˜: {score:.2f}\n"
        
        # ì‚¬íšŒì  ì˜í–¥
        social_impact = selected_analysis["social_impact"]
        reasoning += f"- ì·¨ì•½ ê³„ì¸µ ê³ ë ¤: {len(social_impact.vulnerable_groups)}ê°œ ê·¸ë£¹\n"
        
        return reasoning
    
    def get_ethical_insights(self) -> Dict[str, Any]:
        """ìœ¤ë¦¬ì  ì¸ì‚¬ì´íŠ¸ ì œê³µ"""
        if not self.decision_history:
            return {"message": "ì•„ì§ ìœ¤ë¦¬ì  ì˜ì‚¬ê²°ì • ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        recent_decisions = self.decision_history[-5:]
        
        insights = {
            "total_decisions": len(self.decision_history),
            "average_ethical_score": sum(d["selected_alternative"]["ethical_analysis"].ethical_score 
                                       for d in recent_decisions) / len(recent_decisions),
            "principles_frequency": self._analyze_principles_frequency(),
            "impact_distribution": self._analyze_impact_distribution()
        }
        
        return insights
    
    def _analyze_principles_frequency(self) -> Dict[str, int]:
        """ì›ì¹™ ì ìš© ë¹ˆë„ ë¶„ì„"""
        principle_counts = {}
        for decision in self.decision_history:
            principles = decision["selected_alternative"]["ethical_analysis"].principles_applied
            for principle in principles:
                principle_counts[principle.value] = principle_counts.get(principle.value, 0) + 1
        
        return principle_counts
    
    def _analyze_impact_distribution(self) -> Dict[str, int]:
        """ì˜í–¥ ë¶„í¬ ë¶„ì„"""
        impact_counts = {}
        for decision in self.decision_history:
            impacts = decision["selected_alternative"]["ethical_analysis"].impact_assessment
            for impact_type, level in impacts.items():
                impact_counts[level.value] = impact_counts.get(level.value, 0) + 1
        
        return impact_counts

# Phase 25 ìœ¤ë¦¬ì  íŒë‹¨ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
ethical_judgment_system = EthicalJudgmentSystem()

def phase_25_ethical_judgment(alternatives: List[Dict[str, Any]], context: Dict[str, Any] = None) -> Dict[str, Any]:
    """Phase 25 ìœ¤ë¦¬ì  íŒë‹¨ ì‹œìŠ¤í…œ ë©”ì¸ í•¨ìˆ˜"""
    if context is None:
        context = {}
    
    # ìœ¤ë¦¬ì  ì˜ì‚¬ê²°ì •
    decision = ethical_judgment_system.make_ethical_decision(alternatives, context)
    
    return {
        "phase": 25,
        "system": "ethical_judgment",
        "decision": decision,
        "insights": ethical_judgment_system.get_ethical_insights()
    } 