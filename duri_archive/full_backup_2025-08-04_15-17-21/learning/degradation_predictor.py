"""
DuRi ì„±ëŠ¥ ì €í•˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ

ì„±ëŠ¥ ê²½í–¥ì„ ë¶„ì„í•˜ì—¬ ë¯¸ë˜ì˜ ì„±ëŠ¥ ì €í•˜ë¥¼ ì˜ˆì¸¡í•˜ê³  ë¦¬íŒ©í„°ë§ í•„ìš”ì„±ì„ íŒë‹¨í•©ë‹ˆë‹¤.
"""

import logging
import time
import threading
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
import json

logger = logging.getLogger(__name__)

class DegradationLevel(Enum):
    """ì„±ëŠ¥ ì €í•˜ ìˆ˜ì¤€"""
    NONE = "none"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class RefactorPriority(Enum):
    """ë¦¬íŒ©í„°ë§ ìš°ì„ ìˆœìœ„"""
    NONE = "none"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    URGENT = "urgent"

@dataclass
class DegradationPrediction:
    """ì„±ëŠ¥ ì €í•˜ ì˜ˆì¸¡"""
    metric_name: str
    current_value: float
    predicted_value: float
    time_horizon_hours: int
    confidence: float
    degradation_level: DegradationLevel
    refactor_priority: RefactorPriority
    prediction_reason: str
    timestamp: datetime

@dataclass
class RefactorRecommendation:
    """ë¦¬íŒ©í„°ë§ ê¶Œì¥ì‚¬í•­"""
    recommendation_id: str
    target_module: str
    refactor_type: str
    priority: RefactorPriority
    expected_impact: float
    implementation_cost: str  # "low", "medium", "high"
    risk_level: str  # "low", "medium", "high"
    description: str
    reasoning: str
    timestamp: datetime

class DegradationPredictor:
    """DuRi ì„±ëŠ¥ ì €í•˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """DegradationPredictor ì´ˆê¸°í™”"""
        self.predictions: List[DegradationPrediction] = []
        self.recommendations: List[RefactorRecommendation] = []
        self.is_predicting = False
        self.prediction_thread: Optional[threading.Thread] = None
        
        # ì˜ˆì¸¡ ì„¤ì •
        self.prediction_horizon_hours = 24  # 24ì‹œê°„ í›„ ì˜ˆì¸¡
        self.prediction_interval_minutes = 60  # 1ì‹œê°„ë§ˆë‹¤ ì˜ˆì¸¡
        self.min_confidence_threshold = 0.7  # ìµœì†Œ ì‹ ë¢°ë„ ì„ê³„ê°’
        
        # ì„±ëŠ¥ ì €í•˜ ì„ê³„ê°’
        self.degradation_thresholds = {
            "cpu_usage": {"low": 70, "medium": 80, "high": 90, "critical": 95},
            "memory_usage": {"low": 75, "medium": 85, "high": 90, "critical": 95},
            "learning_cycle_time": {"low": 3.0, "medium": 5.0, "high": 8.0, "critical": 12.0},
            "error_rate": {"low": 0.05, "medium": 0.10, "high": 0.20, "critical": 0.30},
            "memory_growth_rate": {"low": 0.10, "medium": 0.20, "high": 0.30, "critical": 0.50},
            "system_complexity": {"low": 0.60, "medium": 0.75, "high": 0.85, "critical": 0.95}
        }
        
        # ì±—ì§€í”¼í‹° ì œì•ˆ íŠ¸ë¦¬ê±° ì¡°ê±´
        self.trigger_conditions = {
            "performance_decline_threshold": 0.08,  # 8% ì„±ëŠ¥ í•˜ë½í­
            "stagnation_detection_days": 3,  # 3ì¼ê°„ ì •ì²´ ê°ì§€
            "structure_exhaustion_threshold": 0.85  # 85% êµ¬ì¡° ê³ ê°ˆ ì„ê³„ê°’
        }
        
        # ë¦¬íŒ©í„°ë§ ìš°ì„ ìˆœìœ„ ë§¤í•‘
        self.priority_mapping = {
            DegradationLevel.LOW: RefactorPriority.LOW,
            DegradationLevel.MEDIUM: RefactorPriority.MEDIUM,
            DegradationLevel.HIGH: RefactorPriority.HIGH,
            DegradationLevel.CRITICAL: RefactorPriority.URGENT
        }
        
        logger.info("DegradationPredictor ì´ˆê¸°í™” ì™„ë£Œ")
    
    def start_prediction(self):
        """ì„±ëŠ¥ ì €í•˜ ì˜ˆì¸¡ì„ ì‹œì‘í•©ë‹ˆë‹¤."""
        if self.is_predicting:
            logger.warning("ì´ë¯¸ ì˜ˆì¸¡ ì¤‘ì…ë‹ˆë‹¤.")
            return
        
        self.is_predicting = True
        self.prediction_thread = threading.Thread(target=self._prediction_loop, daemon=True)
        self.prediction_thread.start()
        logger.info("ì„±ëŠ¥ ì €í•˜ ì˜ˆì¸¡ ì‹œì‘")
    
    def stop_prediction(self):
        """ì„±ëŠ¥ ì €í•˜ ì˜ˆì¸¡ì„ ì¤‘ì§€í•©ë‹ˆë‹¤."""
        self.is_predicting = False
        if self.prediction_thread:
            self.prediction_thread.join(timeout=5)
        logger.info("ì„±ëŠ¥ ì €í•˜ ì˜ˆì¸¡ ì¤‘ì§€")
    
    def _prediction_loop(self):
        """ì˜ˆì¸¡ ë£¨í”„"""
        while self.is_predicting:
            try:
                # ì„±ëŠ¥ ì €í•˜ ì˜ˆì¸¡ ì‹¤í–‰
                predictions = self._predict_degradations()
                
                # ë¦¬íŒ©í„°ë§ ê¶Œì¥ì‚¬í•­ ìƒì„±
                recommendations = self._generate_refactor_recommendations(predictions)
                
                # ê²°ê³¼ ì €ì¥
                self.predictions.extend(predictions)
                self.recommendations.extend(recommendations)
                
                # ì˜¤ë˜ëœ ì˜ˆì¸¡ ì •ë¦¬ (7ì¼ ì´ìƒ)
                self._cleanup_old_predictions()
                
                logger.info(f"ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ ì˜ˆì¸¡, {len(recommendations)}ê°œ ê¶Œì¥ì‚¬í•­")
                
                # ë‹¤ìŒ ì˜ˆì¸¡ê¹Œì§€ ëŒ€ê¸°
                time.sleep(self.prediction_interval_minutes * 60)
                
            except Exception as e:
                logger.error(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {e}")
                time.sleep(300)  # ì˜¤ë¥˜ ì‹œ 5ë¶„ ëŒ€ê¸°
    
    def _predict_degradations(self) -> List[DegradationPrediction]:
        """ì„±ëŠ¥ ì €í•˜ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤."""
        predictions = []
        
        try:
            # PerformanceHistoryì—ì„œ ìµœê·¼ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            import sys
            sys.path.append('.')
            from duri_brain.learning.performance_history import get_performance_history
            history = get_performance_history()
            
            # ì±—ì§€í”¼í‹° ì œì•ˆ íŠ¸ë¦¬ê±° ì¡°ê±´ í™•ì¸
            if self._check_chatgpt_trigger_conditions(history):
                logger.warning("ğŸš¨ ì±—ì§€í”¼í‹° ì œì•ˆ íŠ¸ë¦¬ê±° ì¡°ê±´ ì¶©ì¡± - ë¦¬íŒ©í„°ë§ í•„ìš”")
                # ê¸´ê¸‰ ë¦¬íŒ©í„°ë§ ì˜ˆì¸¡ ìƒì„±
                emergency_prediction = self._create_emergency_prediction()
                if emergency_prediction:
                    predictions.append(emergency_prediction)
            
            # ê° ì„±ëŠ¥ ì§€í‘œë³„ë¡œ ì˜ˆì¸¡
            for metric_name in self.degradation_thresholds.keys():
                try:
                    prediction = self._predict_metric_degradation(metric_name, history)
                    if prediction:
                        predictions.append(prediction)
                except Exception as e:
                    logger.error(f"{metric_name} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ì €í•˜ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        
        return predictions
    
    def _check_chatgpt_trigger_conditions(self, history) -> bool:
        """ì±—ì§€í”¼í‹° ì œì•ˆ íŠ¸ë¦¬ê±° ì¡°ê±´ì„ í™•ì¸í•©ë‹ˆë‹¤."""
        try:
            # 1. ìµœê·¼ 3ì¼ê°„ í‰ê·  ì„±ëŠ¥ í•˜ë½í­ > 8% í™•ì¸
            recent_trends = history.get_recent_trends(72)  # 3ì¼ = 72ì‹œê°„
            
            if len(recent_trends) < 5:  # ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸ í•„ìš”
                return False
            
            # ì„±ëŠ¥ í•˜ë½í­ ê³„ì‚°
            total_decline = 0.0
            decline_count = 0
            
            for trend in recent_trends:
                if trend.trend_direction == "degrading":
                    total_decline += abs(trend.change_rate)
                    decline_count += 1
            
            if decline_count > 0:
                avg_decline = total_decline / decline_count
                if avg_decline > self.trigger_conditions["performance_decline_threshold"] * 100:  # 8%
                    logger.warning(f"ğŸš¨ í‰ê·  ì„±ëŠ¥ í•˜ë½í­ {avg_decline:.1f}% > 8% ì„ê³„ê°’")
                    return True
            
            # 2. DuRi ë‚´ë¶€ íŒë‹¨ "ì •ì²´ + êµ¬ì¡° ê³ ê°ˆ" í™•ì¸
            if self._check_stagnation_and_exhaustion(history):
                logger.warning("ğŸš¨ ì •ì²´ + êµ¬ì¡° ê³ ê°ˆ ê°ì§€")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"íŠ¸ë¦¬ê±° ì¡°ê±´ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def _check_stagnation_and_exhaustion(self, history) -> bool:
        """ì •ì²´ì™€ êµ¬ì¡° ê³ ê°ˆì„ í™•ì¸í•©ë‹ˆë‹¤."""
        try:
            # ì‹œìŠ¤í…œ ë³µì¡ë„ê°€ 85% ì´ìƒì´ê³  ì„±ëŠ¥ ê°œì„ ì´ ì—†ëŠ” ê²½ìš°
            recent_trends = history.get_recent_trends(24)  # ìµœê·¼ 24ì‹œê°„
            
            complexity_trends = [t for t in recent_trends if t.metric_name == "system_complexity"]
            performance_trends = [t for t in recent_trends if t.metric_name in ["cpu_usage", "memory_usage", "learning_cycle_time"]]
            
            # ì‹œìŠ¤í…œ ë³µì¡ë„ê°€ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸
            if complexity_trends:
                latest_complexity = complexity_trends[-1].final_value
                if latest_complexity > self.trigger_conditions["structure_exhaustion_threshold"]:
                    # ì„±ëŠ¥ ê°œì„ ì´ ì—†ëŠ”ì§€ í™•ì¸
                    improving_count = sum(1 for t in performance_trends if t.trend_direction == "improving")
                    if improving_count == 0:
                        logger.warning(f"ğŸš¨ ì‹œìŠ¤í…œ ë³µì¡ë„ {latest_complexity:.1f}% > 85% ì„ê³„ê°’, ì„±ëŠ¥ ê°œì„  ì—†ìŒ")
                        return True
            
            return False
            
        except Exception as e:
            logger.error(f"ì •ì²´ ë° êµ¬ì¡° ê³ ê°ˆ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def _create_emergency_prediction(self) -> Optional[DegradationPrediction]:
        """ê¸´ê¸‰ ë¦¬íŒ©í„°ë§ ì˜ˆì¸¡ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            return DegradationPrediction(
                metric_name="emergency_refactor",
                current_value=0.0,
                predicted_value=0.0,
                time_horizon_hours=24,
                confidence=0.95,  # ë†’ì€ ì‹ ë¢°ë„
                degradation_level=DegradationLevel.CRITICAL,
                refactor_priority=RefactorPriority.URGENT,
                prediction_reason="ì±—ì§€í”¼í‹° ì œì•ˆ íŠ¸ë¦¬ê±° ì¡°ê±´ ì¶©ì¡±: ì„±ëŠ¥ í•˜ë½í­ > 8% ë˜ëŠ” ì •ì²´ + êµ¬ì¡° ê³ ê°ˆ",
                timestamp=datetime.now()
            )
        except Exception as e:
            logger.error(f"ê¸´ê¸‰ ì˜ˆì¸¡ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _predict_metric_degradation(self, metric_name: str, history) -> Optional[DegradationPrediction]:
        """íŠ¹ì • ì§€í‘œì˜ ì„±ëŠ¥ ì €í•˜ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤."""
        try:
            # ìµœê·¼ 48ì‹œê°„ ë°ì´í„° ì¡°íšŒ
            recent_trends = history.get_recent_trends(48)
            
            # í•´ë‹¹ ì§€í‘œì˜ ë°ì´í„° í•„í„°ë§
            metric_trends = [t for t in recent_trends if t.metric_name == metric_name]
            
            if len(metric_trends) < 3:
                return None  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŒ
            
            # ì„ í˜• íšŒê·€ë¥¼ ì‚¬ìš©í•œ ì˜ˆì¸¡
            prediction = self._linear_regression_prediction(metric_trends, metric_name)
            
            if prediction and prediction.confidence >= self.min_confidence_threshold:
                return prediction
            
        except Exception as e:
            logger.error(f"{metric_name} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        
        return None
    
    def _linear_regression_prediction(self, trends: List, metric_name: str) -> Optional[DegradationPrediction]:
        """ì„ í˜• íšŒê·€ë¥¼ ì‚¬ìš©í•œ ì˜ˆì¸¡"""
        try:
            # ì‹œê°„ê³¼ ê°’ ë°ì´í„° ì¤€ë¹„
            times = []
            values = []
            
            for trend in trends:
                # ì‹œê°„ì„ ì‹œê°„ ë‹¨ìœ„ë¡œ ë³€í™˜
                time_hours = (trend.end_time - trend.start_time).total_seconds() / 3600
                times.append(time_hours)
                values.append(trend.final_value)
            
            if len(times) < 2:
                return None
            
            # ì„ í˜• íšŒê·€ ëª¨ë¸ í•™ìŠµ
            X = np.array(times).reshape(-1, 1)
            y = np.array(values)
            
            model = LinearRegression()
            model.fit(X, y)
            
            # í˜„ì¬ ê°’ (ê°€ì¥ ìµœê·¼)
            current_value = values[-1]
            
            # ë¯¸ë˜ ì˜ˆì¸¡ (24ì‹œê°„ í›„)
            future_time = times[-1] + self.prediction_horizon_hours
            predicted_value = model.predict([[future_time]])[0]
            
            # ì‹ ë¢°ë„ ê³„ì‚° (RÂ² ì ìˆ˜ ê¸°ë°˜)
            confidence = max(0.0, min(1.0, model.score(X, y)))
            
            # ì„±ëŠ¥ ì €í•˜ ìˆ˜ì¤€ íŒë‹¨
            degradation_level = self._determine_degradation_level(metric_name, predicted_value)
            
            # ë¦¬íŒ©í„°ë§ ìš°ì„ ìˆœìœ„ ê²°ì •
            refactor_priority = self.priority_mapping.get(degradation_level, RefactorPriority.NONE)
            
            # ì˜ˆì¸¡ ì´ìœ  ìƒì„±
            prediction_reason = self._generate_prediction_reason(metric_name, current_value, predicted_value, confidence)
            
            return DegradationPrediction(
                metric_name=metric_name,
                current_value=current_value,
                predicted_value=predicted_value,
                time_horizon_hours=self.prediction_horizon_hours,
                confidence=confidence,
                degradation_level=degradation_level,
                refactor_priority=refactor_priority,
                prediction_reason=prediction_reason,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"ì„ í˜• íšŒê·€ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return None
    
    def _determine_degradation_level(self, metric_name: str, predicted_value: float) -> DegradationLevel:
        """ì„±ëŠ¥ ì €í•˜ ìˆ˜ì¤€ì„ íŒë‹¨í•©ë‹ˆë‹¤."""
        thresholds = self.degradation_thresholds.get(metric_name, {})
        
        if predicted_value >= thresholds.get("critical", float('inf')):
            return DegradationLevel.CRITICAL
        elif predicted_value >= thresholds.get("high", float('inf')):
            return DegradationLevel.HIGH
        elif predicted_value >= thresholds.get("medium", float('inf')):
            return DegradationLevel.MEDIUM
        elif predicted_value >= thresholds.get("low", float('inf')):
            return DegradationLevel.LOW
        else:
            return DegradationLevel.NONE
    
    def _generate_prediction_reason(self, metric_name: str, current_value: float, 
                                  predicted_value: float, confidence: float) -> str:
        """ì˜ˆì¸¡ ì´ìœ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        change_rate = ((predicted_value - current_value) / current_value) * 100 if current_value != 0 else 0
        
        if change_rate > 20:
            trend = "ê¸‰ê²©í•œ ì„±ëŠ¥ ì €í•˜"
        elif change_rate > 10:
            trend = "ì ì§„ì  ì„±ëŠ¥ ì €í•˜"
        elif change_rate > 0:
            trend = "ì•½ê°„ì˜ ì„±ëŠ¥ ì €í•˜"
        else:
            trend = "ì„±ëŠ¥ ì•ˆì •"
        
        return f"{metric_name}: {trend} ì˜ˆìƒ (ë³€í™”ìœ¨: {change_rate:.1f}%, ì‹ ë¢°ë„: {confidence:.1f})"
    
    def _generate_refactor_recommendations(self, predictions: List[DegradationPrediction]) -> List[RefactorRecommendation]:
        """ë¦¬íŒ©í„°ë§ ê¶Œì¥ì‚¬í•­ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        recommendations = []
        
        # ë†’ì€ ìš°ì„ ìˆœìœ„ ì˜ˆì¸¡ë§Œ í•„í„°ë§
        high_priority_predictions = [
            p for p in predictions 
            if p.refactor_priority in [RefactorPriority.HIGH, RefactorPriority.URGENT]
        ]
        
        for prediction in high_priority_predictions:
            try:
                recommendation = self._create_refactor_recommendation(prediction)
                if recommendation:
                    recommendations.append(recommendation)
            except Exception as e:
                logger.error(f"ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return recommendations
    
    def _create_refactor_recommendation(self, prediction: DegradationPrediction) -> Optional[RefactorRecommendation]:
        """íŠ¹ì • ì˜ˆì¸¡ì— ëŒ€í•œ ë¦¬íŒ©í„°ë§ ê¶Œì¥ì‚¬í•­ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # ì§€í‘œë³„ ë¦¬íŒ©í„°ë§ ì „ëµ ë§¤í•‘
            refactor_strategies = {
                "cpu_usage": {
                    "type": "algorithm_optimization",
                    "target_module": "learning_loop_manager",
                    "description": "í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ìµœì í™”ë¡œ CPU ì‚¬ìš©ëŸ‰ ê°ì†Œ",
                    "expected_impact": 0.25,
                    "implementation_cost": "medium",
                    "risk_level": "low"
                },
                "memory_usage": {
                    "type": "memory_optimization",
                    "target_module": "memory_sync",
                    "description": "ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™” ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°œì„ ",
                    "expected_impact": 0.30,
                    "implementation_cost": "medium",
                    "risk_level": "medium"
                },
                "learning_cycle_time": {
                    "type": "performance_optimization",
                    "target_module": "smart_learning_checker",
                    "description": "í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰ ì‹œê°„ ìµœì í™”",
                    "expected_impact": 0.40,
                    "implementation_cost": "high",
                    "risk_level": "medium"
                },
                "error_rate": {
                    "type": "error_handling_improvement",
                    "target_module": "fallback_handler",
                    "description": "ì˜¤ë¥˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ ê°•í™” ë° ì˜ˆì™¸ ì²˜ë¦¬ ê°œì„ ",
                    "expected_impact": 0.35,
                    "implementation_cost": "low",
                    "risk_level": "low"
                },
                "memory_growth_rate": {
                    "type": "memory_leak_fix",
                    "target_module": "performance_monitor",
                    "description": "ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ê°•í™”",
                    "expected_impact": 0.45,
                    "implementation_cost": "high",
                    "risk_level": "high"
                },
                "system_complexity": {
                    "type": "code_refactoring",
                    "target_module": "self_learning_orchestrator",
                    "description": "ì‹œìŠ¤í…œ ë³µì¡ë„ ê°ì†Œë¥¼ ìœ„í•œ ì½”ë“œ ë¦¬íŒ©í„°ë§",
                    "expected_impact": 0.20,
                    "implementation_cost": "high",
                    "risk_level": "high"
                }
            }
            
            strategy = refactor_strategies.get(prediction.metric_name, {})
            if not strategy:
                return None
            
            recommendation_id = f"REFACTOR_{prediction.metric_name.upper()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            reasoning = f"ì˜ˆì¸¡ëœ {prediction.metric_name} ì„±ëŠ¥ ì €í•˜ ({prediction.degradation_level.value})ì— ë”°ë¥¸ {strategy['description']}"
            
            return RefactorRecommendation(
                recommendation_id=recommendation_id,
                target_module=strategy.get("target_module", "unknown"),
                refactor_type=strategy.get("type", "general"),
                priority=prediction.refactor_priority,
                expected_impact=strategy.get("expected_impact", 0.0),
                implementation_cost=strategy.get("implementation_cost", "medium"),
                risk_level=strategy.get("risk_level", "medium"),
                description=strategy.get("description", "ì¼ë°˜ì ì¸ ì„±ëŠ¥ ìµœì í™”"),
                reasoning=reasoning,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"ë¦¬íŒ©í„°ë§ ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _cleanup_old_predictions(self):
        """ì˜¤ë˜ëœ ì˜ˆì¸¡ì„ ì •ë¦¬í•©ë‹ˆë‹¤."""
        try:
            cutoff_time = datetime.now() - timedelta(days=7)
            
            # ì˜¤ë˜ëœ ì˜ˆì¸¡ ì œê±°
            self.predictions = [
                p for p in self.predictions 
                if p.timestamp > cutoff_time
            ]
            
            # ì˜¤ë˜ëœ ê¶Œì¥ì‚¬í•­ ì œê±°
            self.recommendations = [
                r for r in self.recommendations 
                if r.timestamp > cutoff_time
            ]
            
        except Exception as e:
            logger.error(f"ì˜¤ë˜ëœ ì˜ˆì¸¡ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def get_recent_predictions(self, hours: int = 24) -> List[DegradationPrediction]:
        """ìµœê·¼ ì˜ˆì¸¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        return [
            p for p in self.predictions 
            if p.timestamp > cutoff_time
        ]
    
    def get_active_recommendations(self) -> List[RefactorRecommendation]:
        """í™œì„± ê¶Œì¥ì‚¬í•­ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
        # ìµœê·¼ 24ì‹œê°„ ë‚´ì˜ ë†’ì€ ìš°ì„ ìˆœìœ„ ê¶Œì¥ì‚¬í•­
        cutoff_time = datetime.now() - timedelta(hours=24)
        return [
            r for r in self.recommendations 
            if r.timestamp > cutoff_time and r.priority in [RefactorPriority.HIGH, RefactorPriority.URGENT]
        ]
    
    def get_prediction_summary(self) -> Dict[str, Any]:
        """ì˜ˆì¸¡ ìš”ì•½ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            recent_predictions = self.get_recent_predictions(24)
            active_recommendations = self.get_active_recommendations()
            
            # ê²½ê³  ìˆ˜ì¤€ë³„ ë¶„ë¥˜
            critical_predictions = [p for p in recent_predictions if p.degradation_level == DegradationLevel.CRITICAL]
            high_predictions = [p for p in recent_predictions if p.degradation_level == DegradationLevel.HIGH]
            medium_predictions = [p for p in recent_predictions if p.degradation_level == DegradationLevel.MEDIUM]
            
            return {
                "total_predictions": len(recent_predictions),
                "critical_predictions": len(critical_predictions),
                "high_predictions": len(high_predictions),
                "medium_predictions": len(medium_predictions),
                "active_recommendations": len(active_recommendations),
                "prediction_confidence_avg": np.mean([p.confidence for p in recent_predictions]) if recent_predictions else 0.0,
                "critical_metrics": [p.metric_name for p in critical_predictions],
                "high_priority_recommendations": [
                    {
                        "id": r.recommendation_id,
                        "target_module": r.target_module,
                        "priority": r.priority.value,
                        "expected_impact": f"{r.expected_impact:.1%}"
                    }
                    for r in active_recommendations
                ]
            }
            
        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_degradation_predictor: Optional[DegradationPredictor] = None

def get_degradation_predictor() -> DegradationPredictor:
    """DegradationPredictor ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global _degradation_predictor
    if _degradation_predictor is None:
        _degradation_predictor = DegradationPredictor()
    return _degradation_predictor

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    predictor = get_degradation_predictor()
    predictor.start_prediction()
    
    print("ğŸ”® ì„±ëŠ¥ ì €í•˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("â° 60ì´ˆê°„ ì˜ˆì¸¡ ì‹¤í–‰ ì¤‘...")
    
    time.sleep(60)
    
    summary = predictor.get_prediction_summary()
    print(f"ğŸ“Š ì˜ˆì¸¡ ìš”ì•½: {summary}")
    
    predictor.stop_prediction()
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ") 