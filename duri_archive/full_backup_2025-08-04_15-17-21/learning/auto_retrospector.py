"""
DuRi AutoRetrospector (ìë™ íšŒê³  ì‹œìŠ¤í…œ)

DuRiì˜ ìë™ íšŒê³  ë° ë©”íƒ€ í•™ìŠµ ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
"""

import logging
import time
import uuid
from collections import Counter, defaultdict
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

from duri_core.memory.memory_sync import MemoryType, get_memory_sync
from duri_core.memory.meta_learning_data import (
    ErrorPattern,
    ImprovementSuggestion,
    LearningStrategyUpdate,
    MetaLearningData,
    MetaLearningType,
    PerformancePattern,
    get_meta_learning_data_manager,
)
from duri_core.utils.fallback_handler import get_fallback_handler
from duri_core.utils.log_analyzer import get_log_analyzer

# ê¸°ì¡´ ì‹œìŠ¤í…œ import
from duri_core.utils.performance_monitor import get_performance_monitor

logger = logging.getLogger(__name__)


@dataclass
class AnalysisResult:
    """ë¶„ì„ ê²°ê³¼"""

    analysis_id: str
    analysis_type: str
    confidence: float
    findings: List[str]
    recommendations: List[str]
    performance_impact: float
    analysis_duration: float


class AutoRetrospector:
    """DuRiì˜ ìë™ íšŒê³  ë° ë©”íƒ€ í•™ìŠµ ë¶„ì„ ì‹œìŠ¤í…œ"""

    def __init__(self):
        """AutoRetrospector ì´ˆê¸°í™”"""
        self.performance_monitor = get_performance_monitor()
        self.log_analyzer = get_log_analyzer()
        self.fallback_handler = get_fallback_handler()
        self.memory_sync = get_memory_sync()
        self.meta_learning_manager = get_meta_learning_data_manager()

        # ë¶„ì„ ì„¤ì •
        self.analysis_interval = 3600  # 1ì‹œê°„ë§ˆë‹¤ ë¶„ì„
        self.last_analysis_time = None
        self.analysis_history: List[AnalysisResult] = []

        logger.info("AutoRetrospector ì´ˆê¸°í™” ì™„ë£Œ")

    def reflect_on_learning_cycle(
        self, improvement_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """í•™ìŠµ ì‚¬ì´í´ ì „ì²´ì— ëŒ€í•œ ë©”íƒ€ ë°˜ì„± - ê¸°ì¡´ reflect_on_chatgpt_feedback íŒ¨í„´ í™œìš©"""
        try:
            logger.info("ğŸ§  í•™ìŠµ ì‚¬ì´í´ ë©”íƒ€ ë°˜ì„± ì‹œì‘")

            reflection = {
                "timestamp": datetime.now().isoformat(),
                "learning_cycle_data": improvement_result,
                "accepted_criticisms": [],
                "disagreements": [],
                "improvement_proposal": {},
                "self_assessment": {},
                "meta_analysis": {},
            }

            # í•™ìŠµ ì•½ì  ë¶„ì„ (ê¸°ì¡´ íŒ¨í„´ í™œìš©)
            reflection["accepted_criticisms"] = self._analyze_learning_weaknesses(
                improvement_result
            )

            # í•™ìŠµ ì „ëµì— ëŒ€í•œ ì˜ê²¬ ì°¨ì´ ì‹ë³„
            reflection["disagreements"] = self._identify_learning_disagreements(
                improvement_result
            )

            # í•™ìŠµ ê°œì„ ì•ˆ ìƒì„±
            reflection["improvement_proposal"] = self._generate_learning_improvements(
                improvement_result
            )

            # í•™ìŠµ ì„±ê³¼ ìì²´ í‰ê°€
            reflection["self_assessment"] = self._assess_learning_performance(
                improvement_result
            )

            # ë©”íƒ€ ë¶„ì„ ê²°ê³¼
            reflection["meta_analysis"] = self._perform_meta_analysis(
                improvement_result
            )

            # ë°˜ì„± ê¸°ë¡ ì €ì¥
            self.analysis_history.append(
                AnalysisResult(
                    analysis_id=f"learning_cycle_{uuid.uuid4().hex[:8]}",
                    analysis_type="learning_cycle_reflection",
                    confidence=reflection["self_assessment"].get("confidence", 0.0),
                    findings=reflection["accepted_criticisms"],
                    recommendations=reflection["improvement_proposal"].get(
                        "specific_improvements", []
                    ),
                    performance_impact=reflection["self_assessment"].get(
                        "performance_score", 0.0
                    ),
                    analysis_duration=0.0,
                )
            )

            logger.info(
                f"âœ… í•™ìŠµ ì‚¬ì´í´ ë©”íƒ€ ë°˜ì„± ì™„ë£Œ - ì•½ì : {len(reflection['accepted_criticisms'])}ê°œ, ê°œì„ ì•ˆ: {len(reflection['improvement_proposal'].get('specific_improvements', []))}ê°œ"
            )
            return reflection

        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì‚¬ì´í´ ë©”íƒ€ ë°˜ì„± ì˜¤ë¥˜: {e}")
            return {
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "accepted_criticisms": [],
                "disagreements": [],
                "improvement_proposal": {},
                "self_assessment": {"confidence": 0.0, "performance_score": 0.0},
            }

    def _analyze_learning_weaknesses(
        self, improvement_result: Dict[str, Any]
    ) -> List[str]:
        """í•™ìŠµ ì•½ì  ë¶„ì„ - ê¸°ì¡´ íŒ¨í„´ í™œìš©"""
        weaknesses = []

        # í•™ìŠµ ì ìˆ˜ ê¸°ë°˜ ì•½ì  ì‹ë³„
        learning_score = improvement_result.get("learning_score", 0.0)
        if learning_score < 0.5:
            weaknesses.append(f"ì „ì²´ í•™ìŠµ ì ìˆ˜ê°€ ë‚®ìŒ (ì ìˆ˜: {learning_score:.3f})")

        # ììœ¨ ì•¡ì…˜ ê¸°ë°˜ ì•½ì  ì‹ë³„
        autonomous_actions = improvement_result.get("autonomous_actions", [])
        if len(autonomous_actions) < 2:
            weaknesses.append("ììœ¨ í•™ìŠµ ì•¡ì…˜ì´ ë¶€ì¡±í•¨")

        # ê°œì„  ë°©í–¥ ê¸°ë°˜ ì•½ì  ì‹ë³„
        improvement_direction = improvement_result.get("improvement_direction", {})
        if improvement_direction.get("needs_optimization"):
            weaknesses.append("ì„±ëŠ¥ ìµœì í™”ê°€ í•„ìš”í•¨")
        if improvement_direction.get("needs_adaptation"):
            weaknesses.append("í•™ìŠµ ë°©ì‹ ì ì‘ì´ í•„ìš”í•¨")
        if improvement_direction.get("needs_restructuring"):
            weaknesses.append("í•™ìŠµ êµ¬ì¡° ê°œì„ ì´ í•„ìš”í•¨")

        return weaknesses

    def _identify_learning_disagreements(
        self, improvement_result: Dict[str, Any]
    ) -> List[str]:
        """í•™ìŠµ ì „ëµì— ëŒ€í•œ ì˜ê²¬ ì°¨ì´ ì‹ë³„"""
        disagreements = []

        # í•™ìŠµ ì ìˆ˜ì™€ ì‹ ë¢°ë„ ê°„ì˜ ë¶ˆì¼ì¹˜
        learning_score = improvement_result.get("learning_score", 0.0)
        confidence = improvement_result.get("confidence", 0.0)
        if abs(learning_score - confidence) > 0.3:
            disagreements.append("í•™ìŠµ ì ìˆ˜ì™€ ì‹ ë¢°ë„ ê°„ì˜ ë¶ˆì¼ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤")

        # ììœ¨ ì•¡ì…˜ì˜ íš¨ê³¼ì„±ì— ëŒ€í•œ ì˜ê²¬ ì°¨ì´
        autonomous_actions = improvement_result.get("autonomous_actions", [])
        if autonomous_actions:
            impact_scores = [
                action.get("impact_score", 0.0) for action in autonomous_actions
            ]
            avg_impact = sum(impact_scores) / len(impact_scores)
            if avg_impact < 0.5:
                disagreements.append("ììœ¨ ì•¡ì…˜ì˜ ì˜ˆìƒ íš¨ê³¼ê°€ ë‚®ìŠµë‹ˆë‹¤")

        return disagreements

    def _generate_learning_improvements(
        self, improvement_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """í•™ìŠµ ê°œì„ ì•ˆ ìƒì„± - ê¸°ì¡´ íŒ¨í„´ í™œìš©"""
        improvements = {
            "specific_improvements": [],
            "priority": "medium",
            "reasoning": "",
            "code_examples": [],
            "structure_changes": [],
        }

        # í•™ìŠµ ì ìˆ˜ ê¸°ë°˜ ê°œì„ ì•ˆ
        learning_score = improvement_result.get("learning_score", 0.0)
        if learning_score < 0.4:
            improvements["priority"] = "critical"
            improvements["specific_improvements"].append(
                "í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì „ë©´ ì¬ê²€í†  í•„ìš”"
            )
        elif learning_score < 0.6:
            improvements["priority"] = "high"
            improvements["specific_improvements"].append("í•™ìŠµ ì „ëµ ë¶€ë¶„ì  ì¡°ì • í•„ìš”")

        # ììœ¨ ì•¡ì…˜ ê¸°ë°˜ ê°œì„ ì•ˆ
        autonomous_actions = improvement_result.get("autonomous_actions", [])
        for action in autonomous_actions:
            action_type = action.get("action_type", "")
            if action_type == "optimization":
                improvements["specific_improvements"].append(
                    "ì„±ëŠ¥ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ê°•í™”"
                )
            elif action_type == "adaptation":
                improvements["specific_improvements"].append(
                    "ì ì‘ì  í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜ ê°œì„ "
                )
            elif action_type == "restructuring":
                improvements["specific_improvements"].append("í•™ìŠµ êµ¬ì¡° ëª¨ë“ˆí™” ê°œì„ ")

        # ê°œì„  ì´ìœ  ìƒì„±
        improvements["reasoning"] = (
            f"í•™ìŠµ ì ìˆ˜ {learning_score:.3f}ë¥¼ {learning_score + 0.2:.3f}ë¡œ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ê°œì„ ì•ˆ"
        )

        return improvements

    def _assess_learning_performance(
        self, improvement_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """í•™ìŠµ ì„±ê³¼ ìì²´ í‰ê°€"""
        assessment = {
            "confidence": 0.0,
            "performance_score": 0.0,
            "strengths": [],
            "weaknesses": [],
            "overall_rating": "neutral",
        }

        # ì„±ê³¼ ì ìˆ˜ ê³„ì‚°
        learning_score = improvement_result.get("learning_score", 0.0)
        confidence = improvement_result.get("confidence", 0.0)
        assessment["performance_score"] = learning_score
        assessment["confidence"] = confidence

        # ê°•ì  ë¶„ì„
        if learning_score > 0.7:
            assessment["strengths"].append("ë†’ì€ í•™ìŠµ ì„±ê³¼ ë‹¬ì„±")
            assessment["overall_rating"] = "excellent"
        elif learning_score > 0.5:
            assessment["strengths"].append("ì•ˆì •ì ì¸ í•™ìŠµ ì§„í–‰")
            assessment["overall_rating"] = "good"
        else:
            assessment["weaknesses"].append("í•™ìŠµ ì„±ê³¼ ê°œì„  í•„ìš”")
            assessment["overall_rating"] = "needs_improvement"

        # ììœ¨ì„± í‰ê°€
        autonomous_actions = improvement_result.get("autonomous_actions", [])
        if len(autonomous_actions) > 2:
            assessment["strengths"].append("ë†’ì€ ììœ¨ í•™ìŠµ ëŠ¥ë ¥")
        else:
            assessment["weaknesses"].append("ììœ¨ í•™ìŠµ ëŠ¥ë ¥ í–¥ìƒ í•„ìš”")

        return assessment

    def _perform_meta_analysis(
        self, improvement_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë©”íƒ€ ë¶„ì„ ìˆ˜í–‰"""
        return {
            "learning_pattern": "autonomous_improvement",
            "cycle_efficiency": improvement_result.get("learning_score", 0.0),
            "autonomous_decision_quality": len(
                improvement_result.get("autonomous_actions", [])
            ),
            "improvement_potential": 1.0
            - improvement_result.get("learning_score", 0.0),
        }

    def should_run_analysis(self) -> bool:
        """ë¶„ì„ì„ ì‹¤í–‰í•´ì•¼ í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        if self.last_analysis_time is None:
            return True

        time_since_last = datetime.now() - self.last_analysis_time
        return time_since_last.total_seconds() >= self.analysis_interval

    def get_analysis_history(self, limit: int = 10) -> List[AnalysisResult]:
        """ë¶„ì„ ê¸°ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.analysis_history[-limit:]

    def get_auto_retrospector(self) -> "AutoRetrospector":
        """AutoRetrospector ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self


def get_auto_retrospector() -> AutoRetrospector:
    """AutoRetrospector ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return AutoRetrospector()
