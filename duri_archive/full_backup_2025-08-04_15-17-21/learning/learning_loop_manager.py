"""
DuRiì˜ í•™ìŠµ ë£¨í”„ ê´€ë¦¬ì

5ë‹¨ê³„ í•™ìŠµ ë£¨í”„ í†µí•© ì‹œìŠ¤í…œ
ëª¨ë°© â†’ ë°˜ë³µ â†’ í”¼ë“œë°± â†’ ë„ì „ â†’ ê°œì„ ì˜ ìˆœí™˜ êµ¬ì¡°ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
"""

import logging
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
import asyncio
import threading
import time

from .strategy_imitator import get_strategy_imitator, ImitationType
from .practice_engine import get_practice_engine, PracticeType
from .challenge_trigger import get_challenge_trigger, ChallengeType
from .self_improvement_engine import get_self_improvement_engine, ImprovementType
from .strategy_survival_judge import get_strategy_survival_judge
from .auto_retrospector import get_auto_retrospector
from duri_core.assessment.self_assessment_manager import get_self_assessment_manager
from duri_core.memory.strategy_graveyard import get_strategy_graveyard
from duri_brain.goals.goal_oriented_thinking import get_goal_oriented_thinking
from duri_brain.ethics.emotional_ethical_judgment import get_emotional_ethical_judgment
from duri_brain.goals.autonomous_goal_setting import get_autonomous_goal_setting
from duri_brain.creativity.advanced_creativity_system import get_advanced_creativity_system
from duri_core.memory.memory_sync import MemoryType

logger = logging.getLogger(__name__)

class LearningStage(Enum):
    """í•™ìŠµ ë‹¨ê³„"""
    IMITATION = "imitation"  # 1ë‹¨ê³„: ëª¨ë°©
    PRACTICE = "practice"    # 2ë‹¨ê³„: ë°˜ë³µ
    FEEDBACK = "feedback"    # 3ë‹¨ê³„: í”¼ë“œë°±
    CHALLENGE = "challenge"  # 4ë‹¨ê³„: ë„ì „
    IMPROVEMENT = "improvement"  # 5ë‹¨ê³„: ê°œì„ 

@dataclass
class LearningCycle:
    """í•™ìŠµ ì‚¬ì´í´"""
    cycle_id: str
    start_time: datetime
    end_time: Optional[datetime]
    stages_completed: List[LearningStage]
    current_stage: Optional[LearningStage]
    strategy: Dict[str, Any]
    modified_strategy: Optional[Dict[str, Any]]
    performance_metrics: Dict[str, float]
    success: bool

@dataclass
class LearningResult:
    """í•™ìŠµ ê²°ê³¼"""
    cycle: LearningCycle
    stage_results: Dict[LearningStage, Dict[str, Any]]
    overall_performance: float
    improvement_score: float
    recommendations: List[str]
    next_actions: List[str]

class LearningLoopManager:
    """
    DuRiì˜ í•™ìŠµ ë£¨í”„ ê´€ë¦¬ì
    
    5ë‹¨ê³„ í•™ìŠµ ë£¨í”„ë¥¼ í†µí•©í•˜ê³  ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """LearningLoopManager ì´ˆê¸°í™”"""
        self.learning_cycles: List[LearningCycle] = []
        self.current_cycle: Optional[LearningCycle] = None
        self.is_running = False
        self.loop_thread: Optional[threading.Thread] = None
        self.learning_cycle_count = 0  # í•™ìŠµ ì‚¬ì´í´ ì¹´ìš´í„° ì¶”ê°€
        
        # ê° ë‹¨ê³„ë³„ ëª¨ë“ˆ ì´ˆê¸°í™”
        self.strategy_imitator = get_strategy_imitator()
        self.practice_engine = get_practice_engine()
        self.challenge_trigger = get_challenge_trigger()
        self.self_improvement_engine = get_self_improvement_engine()
        self.strategy_survival_judge = get_strategy_survival_judge()
        self.strategy_graveyard = get_strategy_graveyard()
        
        # Meta Learning ëª¨ë“ˆ ì´ˆê¸°í™”
        self.auto_retrospector = get_auto_retrospector()
        
        # Self Assessment ëª¨ë“ˆ ì´ˆê¸°í™”
        self.self_assessment_manager = get_self_assessment_manager()
        
        # Goal Oriented Thinking ëª¨ë“ˆ ì´ˆê¸°í™”
        self.goal_oriented_thinking = get_goal_oriented_thinking()
        
        # Emotional Ethical Judgment ëª¨ë“ˆ ì´ˆê¸°í™”
        self.emotional_ethical_judgment = get_emotional_ethical_judgment()
        
        # Autonomous Goal Setting ëª¨ë“ˆ ì´ˆê¸°í™”
        self.autonomous_goal_setting = get_autonomous_goal_setting()
        
        # Advanced Creativity System ëª¨ë“ˆ ì´ˆê¸°í™”
        self.advanced_creativity_system = get_advanced_creativity_system()
        
        # í•™ìŠµ ì„¤ì •
        self.learning_config = {
            'max_cycles': 100,
            'cycle_timeout': 300,  # 5ë¶„
            'min_improvement_threshold': 0.1,
            'max_failure_streak': 5,
            'meta_learning_interval': 3600  # 1ì‹œê°„ë§ˆë‹¤ ë©”íƒ€ í•™ìŠµ
        }
        
        logger.info("LearningLoopManager ì´ˆê¸°í™” ì™„ë£Œ")
    
    def start_learning_loop(self, initial_strategy: Dict[str, Any], 
                           context: Optional[Dict[str, Any]] = None) -> str:
        """
        í•™ìŠµ ë£¨í”„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.
        
        Args:
            initial_strategy: ì´ˆê¸° ì „ëµ
            context: í•™ìŠµ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            str: í•™ìŠµ ì‚¬ì´í´ ID
        """
        if self.is_running:
            logger.warning("í•™ìŠµ ë£¨í”„ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return self.current_cycle.cycle_id if self.current_cycle else "unknown"
        
        cycle_id = f"learning_cycle_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        self.current_cycle = LearningCycle(
            cycle_id=cycle_id,
            start_time=datetime.now(),
            end_time=None,
            stages_completed=[],
            current_stage=None,
            strategy=initial_strategy,
            modified_strategy=None,
            performance_metrics={},
            success=False
        )
        
        self.is_running = True
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í•™ìŠµ ë£¨í”„ ì‹¤í–‰
        self.loop_thread = threading.Thread(
            target=self._run_learning_loop,
            args=(context,),
            daemon=True
        )
        self.loop_thread.start()
        
        logger.info(f"í•™ìŠµ ë£¨í”„ ì‹œì‘: {cycle_id}")
        return cycle_id
    
    def stop_learning_loop(self):
        """í•™ìŠµ ë£¨í”„ë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤."""
        self.is_running = False
        if self.current_cycle:
            self.current_cycle.end_time = datetime.now()
            self.learning_cycles.append(self.current_cycle)
            self.current_cycle = None
        
        logger.info("í•™ìŠµ ë£¨í”„ ì¤‘ì§€ë¨")
    
    def _run_learning_loop(self, context: Optional[Dict[str, Any]]):
        """í•™ìŠµ ë£¨í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            cycle_count = 0
            failure_streak = 0
            last_meta_learning_time = None
            start_time = time.time()
            
            while self.is_running and cycle_count < self.learning_config['max_cycles']:
                # íƒ€ì„ì•„ì›ƒ ì²´í¬
                if (time.time() - start_time) > self.learning_config['cycle_timeout']:
                    logger.warning(f"âš ï¸ í•™ìŠµ ë£¨í”„ íƒ€ì„ì•„ì›ƒ ({self.learning_config['cycle_timeout']}ì´ˆ)")
                    break
                
                cycle_count += 1
                logger.info(f"í•™ìŠµ ì‚¬ì´í´ {cycle_count} ì‹œì‘")
                
                # ë©”íƒ€ í•™ìŠµ ì‹¤í–‰ (ì£¼ê¸°ì )
                current_time = datetime.now()
                if (last_meta_learning_time is None or 
                    (current_time - last_meta_learning_time).total_seconds() >= self.learning_config['meta_learning_interval']):
                    
                    self._run_meta_learning_cycle()
                    last_meta_learning_time = current_time
                
                # ìê¸° í‰ê°€ ì‹¤í–‰ (ì£¼ê¸°ì )
                if self.self_assessment_manager.should_run_assessment():
                    self._run_self_assessment_cycle()
                
                # ëª©í‘œ ì§€í–¥ì  ì‚¬ê³  ì‹¤í–‰ (ì£¼ê¸°ì )
                if self.goal_oriented_thinking.should_set_new_goals():
                    self._run_goal_oriented_thinking_cycle()
                
                # ê°ì •/ìœ¤ë¦¬ íŒë‹¨ ì‹¤í–‰ (ì£¼ê¸°ì )
                if self._should_run_emotional_ethical_judgment():
                    self._run_emotional_ethical_judgment_cycle()
                
                # ììœ¨ ëª©í‘œ ì„¤ì • ì‹¤í–‰ (ì£¼ê¸°ì )
                if self.autonomous_goal_setting.should_generate_autonomous_goals():
                    self._run_autonomous_goal_setting_cycle()
                
                # ì°½ì˜ì„± ê³ ë„í™” ì‹¤í–‰ (ì£¼ê¸°ì )
                if self.advanced_creativity_system.should_enhance_creativity():
                    self._run_creativity_enhancement_cycle()
                
                # 5ë‹¨ê³„ í•™ìŠµ ë£¨í”„ ì‹¤í–‰ (íƒ€ì„ì•„ì›ƒ ë³´í˜¸)
                result = self._execute_learning_cycle_with_timeout(context)
                
                if result and result.cycle.success:
                    failure_streak = 0
                    logger.info(f"ì‚¬ì´í´ {cycle_count} ì„±ê³µ: ê°œì„ ì ìˆ˜ {result.improvement_score:.2f}")
                else:
                    failure_streak += 1
                    logger.warning(f"ì‚¬ì´í´ {cycle_count} ì‹¤íŒ¨ (ì—°ì† ì‹¤íŒ¨: {failure_streak})")
                
                # ì—°ì† ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
                if failure_streak >= self.learning_config['max_failure_streak']:
                    logger.error(f"ì—°ì† ì‹¤íŒ¨ë¡œ ì¸í•œ í•™ìŠµ ë£¨í”„ ì¤‘ë‹¨: {failure_streak}íšŒ")
                    break
                
                # ê°œì„ ì´ ë¯¸ë¯¸í•œ ê²½ìš° ì¤‘ë‹¨
                if result and result.improvement_score < self.learning_config['min_improvement_threshold']:
                    logger.info(f"ê°œì„ ì´ ë¯¸ë¯¸í•˜ì—¬ í•™ìŠµ ë£¨í”„ ì¤‘ë‹¨: {result.improvement_score:.2f}")
                    break
                
                # ë‹¤ìŒ ì‚¬ì´í´ì„ ìœ„í•œ ì „ëµ ì—…ë°ì´íŠ¸
                if result and result.cycle.modified_strategy:
                    self.current_cycle.strategy = result.cycle.modified_strategy
                
                time.sleep(1)  # ì‚¬ì´í´ ê°„ ê°„ê²©
            
            self.stop_learning_loop()
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ë£¨í”„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            self.stop_learning_loop()
    
    def _execute_learning_cycle_with_timeout(self, context: Optional[Dict[str, Any]]) -> Optional[LearningResult]:
        """íƒ€ì„ì•„ì›ƒ ë³´í˜¸ê°€ í¬í•¨ëœ í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰"""
        try:
            import threading
            import time
            
            # ê²°ê³¼ë¥¼ ì €ì¥í•  ë³€ìˆ˜
            result = {"learning_result": None, "error": None}
            
            def execute_cycle():
                try:
                    learning_result = self._execute_learning_cycle(context)
                    result["learning_result"] = learning_result
                except Exception as e:
                    result["error"] = str(e)
            
            # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‚¬ì´í´ ì‹¤í–‰
            thread = threading.Thread(target=execute_cycle, daemon=True)
            thread.start()
            
            # íƒ€ì„ì•„ì›ƒ ëŒ€ê¸° (60ì´ˆ)
            timeout = 60
            start_time = time.time()
            
            while thread.is_alive() and (time.time() - start_time) < timeout:
                time.sleep(0.1)  # 100ms ê°„ê²©ìœ¼ë¡œ ì²´í¬
            
            if thread.is_alive():
                logger.error(f"âŒ í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ)")
                return None
            
            if result["error"]:
                logger.error(f"âŒ í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰ ì‹¤íŒ¨: {result['error']}")
                return None
            
            return result["learning_result"]
            
        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _execute_imitation_stage(self, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """1ë‹¨ê³„: ëª¨ë°© ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            # ì°¸ì¡° ì „ëµ ì„ íƒ (ì‹œë®¬ë ˆì´ì…˜)
            reference_strategy = {
                'id': 'reference_strategy_001',
                'type': 'successful_pattern',
                'parameters': {'speed': 0.8, 'accuracy': 0.9, 'efficiency': 0.85},
                'execution_method': 'standard'
            }
            
            # ëª¨ë°© ìœ í˜• ê²°ì •
            imitation_type = ImitationType.ADAPTIVE_COPY
            
            # ëª¨ë°© ì‹¤í–‰
            imitation_result = self.strategy_imitator.imitate(
                reference_strategy=reference_strategy,
                imitation_type=imitation_type,
                context=context
            )
            
            return {
                'success': imitation_result.success,
                'imitated_strategy': imitation_result.imitated_strategy,
                'confidence': imitation_result.confidence,
                'adaptation_notes': imitation_result.adaptation_notes
            }
            
        except Exception as e:
            logger.error(f"ëª¨ë°© ë‹¨ê³„ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _execute_practice_stage(self, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """2ë‹¨ê³„: ë°˜ë³µ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            # ëª¨ë°©ëœ ì „ëµ ê°€ì ¸ì˜¤ê¸°
            imitated_strategy = self.current_cycle.strategy
            
            # ì—°ìŠµ ìœ í˜• ê²°ì •
            practice_type = PracticeType.REPETITION
            
            # ì—°ìŠµ ì‹¤í–‰
            practice_result = self.practice_engine.practice_strategy(
                strategy=imitated_strategy,
                practice_type=practice_type,
                iterations=10,
                context=context
            )
            
            return {
                'success': practice_result.success_rate > 0.5,
                'success_rate': practice_result.success_rate,
                'average_performance': practice_result.average_performance,
                'improvement_score': practice_result.improvement_score,
                'recommendations': practice_result.recommendations
            }
            
        except Exception as e:
            logger.error(f"ë°˜ë³µ ë‹¨ê³„ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _execute_feedback_stage(self, practice_result: Dict[str, Any], 
                              context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """3ë‹¨ê³„: í”¼ë“œë°± ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            # ì—°ìŠµ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í”¼ë“œë°± ìƒì„±
            feedback_data = {
                'performance_issues': [],
                'reliability_issues': [],
                'efficiency_issues': [],
                'positive_feedback': 0,
                'negative_feedback': 0,
                'issues_resolved': 0
            }
            
            # ì„±ëŠ¥ ê¸°ë°˜ í”¼ë“œë°±
            if practice_result.get('average_performance', 0) < 0.6:
                feedback_data['performance_issues'].append('ë‚®ì€ í‰ê·  ì„±ëŠ¥')
                feedback_data['negative_feedback'] += 1
            else:
                feedback_data['positive_feedback'] += 1
            
            # ì„±ê³µë¥  ê¸°ë°˜ í”¼ë“œë°±
            if practice_result.get('success_rate', 0) < 0.5:
                feedback_data['reliability_issues'].append('ë‚®ì€ ì„±ê³µë¥ ')
                feedback_data['negative_feedback'] += 1
            else:
                feedback_data['positive_feedback'] += 1
            
            # ê°œì„  ì ìˆ˜ ê¸°ë°˜ í”¼ë“œë°±
            if practice_result.get('improvement_score', 0) < 0.1:
                feedback_data['efficiency_issues'].append('ë¯¸ë¯¸í•œ ê°œì„ ')
                feedback_data['negative_feedback'] += 1
            else:
                feedback_data['positive_feedback'] += 1
                feedback_data['issues_resolved'] += 1
            
            return {
                'success': True,
                'feedback_data': feedback_data,
                'positive_feedback': feedback_data['positive_feedback'],
                'negative_feedback': feedback_data['negative_feedback'],
                'issues_resolved': feedback_data['issues_resolved']
            }
            
        except Exception as e:
            logger.error(f"í”¼ë“œë°± ë‹¨ê³„ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _execute_challenge_stage(self, feedback_result: Dict[str, Any], 
                               context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """4ë‹¨ê³„: ë„ì „ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            # ì „ëµ íˆìŠ¤í† ë¦¬ êµ¬ì„±
            strategy_history = {
                'success_streak': len(self.learning_cycles) if self.learning_cycles else 0,
                'performance_stability': 0.8,  # ì‹œë®¬ë ˆì´ì…˜
                'new_feedback_detected': feedback_result.get('negative_feedback', 0) > 0
            }
            
            # ë„ì „ íŒë‹¨
            challenge_decision = self.challenge_trigger.should_explore(
                strategy_history=strategy_history,
                current_context=context
            )
            
            return {
                'success': True,
                'should_challenge': challenge_decision.should_challenge,
                'challenge_type': challenge_decision.challenge_type.value if challenge_decision.challenge_type else None,
                'confidence': challenge_decision.confidence,
                'reasoning': challenge_decision.reasoning,
                'recommended_strategy': challenge_decision.recommended_strategy,
                'risk_level': challenge_decision.risk_level
            }
            
        except Exception as e:
            logger.error(f"ë„ì „ ë‹¨ê³„ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _execute_improvement_stage(self, challenge_result: Dict[str, Any], 
                                 context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """5ë‹¨ê³„: ê°œì„  ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            # í”¼ë“œë°± ë°ì´í„° êµ¬ì„±
            feedback_data = {
                'performance_issues': [],
                'reliability_issues': [],
                'efficiency_issues': [],
                'positive_feedback': 0,
                'negative_feedback': 0,
                'issues_resolved': 0
            }
            
            # ë„ì „ ê²°ê³¼ì— ë”°ë¥¸ í”¼ë“œë°± ì¡°ì •
            if challenge_result.get('should_challenge', False):
                feedback_data['positive_feedback'] += 1
            else:
                feedback_data['negative_feedback'] += 1
            
            # ê°œì„  ì‹¤í–‰
            improvement_result = self.self_improvement_engine.improve(
                old_strategy=self.current_cycle.strategy,
                feedback_data=feedback_data
            )
            
            return {
                'success': improvement_result.success,
                'improvement_score': improvement_result.improvement_score,
                'confidence_gain': improvement_result.confidence_gain,
                'changes_made': improvement_result.changes_made,
                'modified_strategy': improvement_result.improved_strategy
            }
            
        except Exception as e:
            logger.error(f"ê°œì„  ë‹¨ê³„ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _calculate_overall_performance(self, stage_results: Dict[LearningStage, Dict[str, Any]]) -> float:
        """ì „ì²´ ì„±ê³¼ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        performances = []
        
        # ê° ë‹¨ê³„ë³„ ì„±ê³¼ ìˆ˜ì§‘
        for stage, result in stage_results.items():
            if result.get('success', False):
                if stage == LearningStage.IMITATION:
                    performances.append(result.get('confidence', 0.0))
                elif stage == LearningStage.PRACTICE:
                    performances.append(result.get('average_performance', 0.0))
                elif stage == LearningStage.FEEDBACK:
                    feedback_ratio = result.get('positive_feedback', 0) / max(result.get('positive_feedback', 0) + result.get('negative_feedback', 1), 1)
                    performances.append(feedback_ratio)
                elif stage == LearningStage.CHALLENGE:
                    performances.append(result.get('confidence', 0.0))
                elif stage == LearningStage.IMPROVEMENT:
                    performances.append(result.get('improvement_score', 0.0))
        
        return sum(performances) / len(performances) if performances else 0.0
    
    def _calculate_improvement_score(self, stage_results: Dict[LearningStage, Dict[str, Any]]) -> float:
        """ê°œì„  ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        improvement_scores = []
        
        for stage, result in stage_results.items():
            if stage == LearningStage.PRACTICE:
                improvement_scores.append(result.get('improvement_score', 0.0))
            elif stage == LearningStage.IMPROVEMENT:
                improvement_scores.append(result.get('improvement_score', 0.0))
        
        return sum(improvement_scores) / len(improvement_scores) if improvement_scores else 0.0
    
    def _generate_recommendations(self, stage_results: Dict[LearningStage, Dict[str, Any]]) -> List[str]:
        """ì¶”ì²œì‚¬í•­ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        recommendations = []
        
        # ê° ë‹¨ê³„ë³„ ì¶”ì²œì‚¬í•­ ìˆ˜ì§‘
        for stage, result in stage_results.items():
            if stage == LearningStage.PRACTICE and result.get('recommendations'):
                recommendations.extend(result['recommendations'])
            elif stage == LearningStage.CHALLENGE and result.get('reasoning'):
                recommendations.extend(result['reasoning'])
        
        if not recommendations:
            recommendations.append("ëª¨ë“  ë‹¨ê³„ê°€ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return recommendations
    
    def _determine_next_actions(self, stage_results: Dict[LearningStage, Dict[str, Any]]) -> List[str]:
        """ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í•©ë‹ˆë‹¤."""
        next_actions = []
        
        # ê°œì„  ê²°ê³¼ì— ë”°ë¥¸ ë‹¤ìŒ í–‰ë™ ê²°ì •
        improvement_result = stage_results.get(LearningStage.IMPROVEMENT, {})
        if improvement_result.get('success', False):
            next_actions.append("ê°œì„ ëœ ì „ëµìœ¼ë¡œ ë‹¤ìŒ ì‚¬ì´í´ ì§„í–‰")
        else:
            next_actions.append("ì „ëµ ì¬ê²€í†  í•„ìš”")
        
        # ë„ì „ ê²°ê³¼ì— ë”°ë¥¸ í–‰ë™
        challenge_result = stage_results.get(LearningStage.CHALLENGE, {})
        if challenge_result.get('should_challenge', False):
            next_actions.append("ë„ì „ì  ì „ëµ ì‹œë„")
        
        return next_actions
    
    def _evaluate_strategy_survival(self, overall_performance: float, context: Optional[Dict[str, Any]]) -> Optional[Any]:
        """ì „ëµ ìƒì¡´ì„ í‰ê°€í•©ë‹ˆë‹¤."""
        try:
            strategy_id = self.current_cycle.cycle_id
            
            # ì „ëµ ë“±ë¡ (ì•„ì§ ë“±ë¡ë˜ì§€ ì•Šì€ ê²½ìš°)
            if not self.strategy_survival_judge.get_strategy_info(strategy_id):
                self.strategy_survival_judge.register_strategy(
                    strategy_id=strategy_id,
                    initial_performance=overall_performance,
                    initial_emotion=context.get('emotion', 'neutral') if context else 'neutral'
                )
            
            # ì„±ê³¼ ì—…ë°ì´íŠ¸
            self.strategy_survival_judge.update_strategy_performance(
                strategy_id=strategy_id,
                performance=overall_performance,
                emotion=context.get('emotion', 'neutral') if context else None
            )
            
            # ìƒì¡´ íŒë‹¨
            survival_decision = self.strategy_survival_judge.evaluate_strategy_survival(strategy_id)
            
            # íê¸°ëœ ê²½ìš° ë¬˜ì§€ì— ì•ˆì¥
            if survival_decision and survival_decision.action.value == 'discard':
                self._bury_failed_strategy(survival_decision, context)
            
            return survival_decision
            
        except Exception as e:
            logger.error(f"ì „ëµ ìƒì¡´ í‰ê°€ ì‹¤íŒ¨: {e}")
            return None
    
    def _bury_failed_strategy(self, survival_decision: Any, context: Optional[Dict[str, Any]]):
        """ì‹¤íŒ¨í•œ ì „ëµì„ ë¬˜ì§€ì— ì•ˆì¥í•©ë‹ˆë‹¤."""
        try:
            strategy_id = survival_decision.strategy_id
            strategy_info = self.strategy_survival_judge.get_strategy_info(strategy_id)
            
            if strategy_info:
                self.strategy_graveyard.bury_strategy(
                    strategy_id=strategy_id,
                    failure_reason=survival_decision.recommended_action,
                    context=context or {},
                    emotion_trend=[],
                    performance_history=[strategy_info['current_performance']],
                    modification_count=strategy_info['modification_count'],
                    strategy_age_days=strategy_info['age_days']
                )
                
                logger.info(f"ì „ëµ {strategy_id} ë¬˜ì§€ì— ì•ˆì¥ë¨")
                
        except Exception as e:
            logger.error(f"ì „ëµ ì•ˆì¥ ì‹¤íŒ¨: {e}")
    
    def get_learning_statistics(self) -> Dict[str, Any]:
        """í•™ìŠµ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        total_cycles = len(self.learning_cycles)
        successful_cycles = len([c for c in self.learning_cycles if c.success])
        
        stage_completion_rates = {}
        for stage in LearningStage:
            completed_count = sum(1 for c in self.learning_cycles if stage in c.stages_completed)
            stage_completion_rates[stage.value] = completed_count / total_cycles if total_cycles > 0 else 0
        
        avg_performance = sum(c.performance_metrics.get('overall_performance', 0) for c in self.learning_cycles) / total_cycles if total_cycles > 0 else 0
        avg_improvement = sum(c.performance_metrics.get('improvement_score', 0) for c in self.learning_cycles) / total_cycles if total_cycles > 0 else 0
        
        return {
            "total_cycles": total_cycles,
            "successful_cycles": successful_cycles,
            "success_rate": successful_cycles / total_cycles if total_cycles > 0 else 0,
            "stage_completion_rates": stage_completion_rates,
            "average_performance": avg_performance,
            "average_improvement": avg_improvement,
            "is_running": self.is_running,
            "survival_statistics": self.strategy_survival_judge.get_survival_statistics(),
            "graveyard_statistics": self.strategy_graveyard.get_failure_statistics()
        }
    
    def _run_meta_learning_cycle(self):
        """ë©”íƒ€ í•™ìŠµ ì‚¬ì´í´ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            logger.info("ë©”íƒ€ í•™ìŠµ ì‚¬ì´í´ ì‹œì‘")
            
            # AutoRetrospectorë¥¼ í†µí•œ ì¢…í•© ë¶„ì„
            meta_learning_data = self.auto_retrospector.run_comprehensive_analysis()
            
            # ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµ ì „ëµ ì—…ë°ì´íŠ¸
            self._update_learning_strategy_based_on_meta_analysis(meta_learning_data)
            
            logger.info("ë©”íƒ€ í•™ìŠµ ì‚¬ì´í´ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _update_learning_strategy_based_on_meta_analysis(self, meta_learning_data):
        """ë©”íƒ€ í•™ìŠµ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµ ì „ëµì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        try:
            # ê°œì„  ì œì•ˆ ì²˜ë¦¬
            for suggestion in meta_learning_data.improvement_suggestions:
                if suggestion.priority == "critical":
                    logger.warning(f"ì¤‘ìš”í•œ ê°œì„  ì œì•ˆ: {suggestion.description}")
                    self._apply_critical_improvement(suggestion)
                elif suggestion.priority == "high":
                    logger.info(f"ë†’ì€ ìš°ì„ ìˆœìœ„ ê°œì„  ì œì•ˆ: {suggestion.description}")
                    self._apply_high_priority_improvement(suggestion)
            
            # í•™ìŠµ ì „ëµ ì—…ë°ì´íŠ¸
            for strategy_update in meta_learning_data.learning_strategy_updates:
                logger.info(f"í•™ìŠµ ì „ëµ ì—…ë°ì´íŠ¸: {strategy_update.strategy_name}")
                self._apply_learning_strategy_update(strategy_update)
            
            # ì„±ëŠ¥ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ë°˜ì˜
            for pattern in meta_learning_data.performance_patterns:
                if pattern.pattern_type == "high_cpu_usage":
                    logger.warning("CPU ì‚¬ìš©ë¥ ì´ ë†’ì•„ í•™ìŠµ ê°•ë„ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤")
                    self._adjust_learning_intensity("reduce")
                elif pattern.pattern_type == "low_cpu_usage":
                    logger.info("CPU ì‚¬ìš©ë¥ ì´ ë‚®ì•„ í•™ìŠµ ê°•ë„ë¥¼ ì¦ê°€ì‹œí‚µë‹ˆë‹¤")
                    self._adjust_learning_intensity("increase")
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ì „ëµ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _apply_critical_improvement(self, suggestion):
        """ì¤‘ìš”í•œ ê°œì„  ì œì•ˆì„ ì ìš©í•©ë‹ˆë‹¤."""
        try:
            # ë”•ì…”ë„ˆë¦¬ì™€ ê°ì²´ ëª¨ë‘ ì§€ì›
            category = suggestion.get("category") if isinstance(suggestion, dict) else getattr(suggestion, "category", None)
            
            if category == "error_handling":
                # ì˜¤ë¥˜ ì²˜ë¦¬ ê°œì„ 
                self.learning_config['max_failure_streak'] = max(3, self.learning_config['max_failure_streak'] - 1)
                logger.info("ì˜¤ë¥˜ ì²˜ë¦¬ ê°œì„ : ì‹¤íŒ¨ í—ˆìš© íšŸìˆ˜ ê°ì†Œ")
            
            elif category == "performance":
                # ì„±ëŠ¥ ìµœì í™”
                self.learning_config['cycle_timeout'] = min(600, self.learning_config['cycle_timeout'] + 60)
                logger.info("ì„±ëŠ¥ ìµœì í™”: ì‚¬ì´í´ íƒ€ì„ì•„ì›ƒ ì¦ê°€")
            
            elif category == "learning":
                # í•™ìŠµ ì „ëµ ê°œì„ 
                self.learning_config['min_improvement_threshold'] = max(0.05, self.learning_config['min_improvement_threshold'] - 0.01)
                logger.info("í•™ìŠµ ì „ëµ ê°œì„ : ê°œì„  ì„ê³„ê°’ ì¡°ì •")
            
        except Exception as e:
            logger.error(f"ì¤‘ìš”í•œ ê°œì„  ì œì•ˆ ì ìš© ì‹¤íŒ¨: {e}")
    
    def _apply_high_priority_improvement(self, suggestion):
        """ë†’ì€ ìš°ì„ ìˆœìœ„ ê°œì„  ì œì•ˆì„ ì ìš©í•©ë‹ˆë‹¤."""
        try:
            # ë”•ì…”ë„ˆë¦¬ì™€ ê°ì²´ ëª¨ë‘ ì§€ì›
            category = suggestion.get("category") if isinstance(suggestion, dict) else getattr(suggestion, "category", None)
            
            if category == "learning":
                # í•™ìŠµ ì „ëµ ê°œì„ 
                self.learning_config['min_improvement_threshold'] = max(0.05, self.learning_config['min_improvement_threshold'] - 0.02)
                logger.info("í•™ìŠµ ì „ëµ ê°œì„ : ê°œì„  ì„ê³„ê°’ ì¡°ì •")
            
            elif category == "system_health":
                # ì‹œìŠ¤í…œ ê±´ê°•ë„ ê°œì„ 
                self.learning_config['meta_learning_interval'] = min(7200, self.learning_config['meta_learning_interval'] + 600)
                logger.info("ì‹œìŠ¤í…œ ê±´ê°•ë„ ê°œì„ : ë©”íƒ€ í•™ìŠµ ê°„ê²© ì¡°ì •")
            
        except Exception as e:
            logger.error(f"ë†’ì€ ìš°ì„ ìˆœìœ„ ê°œì„  ì œì•ˆ ì ìš© ì‹¤íŒ¨: {e}")
    
    def _apply_learning_strategy_update(self, strategy_update):
        """í•™ìŠµ ì „ëµ ì—…ë°ì´íŠ¸ë¥¼ ì ìš©í•©ë‹ˆë‹¤."""
        try:
            if strategy_update.strategy_name == "learning_efficiency":
                # í•™ìŠµ íš¨ìœ¨ì„± ê°œì„ 
                if strategy_update.current_performance < 0.5:
                    self.learning_config['min_improvement_threshold'] = max(0.05, self.learning_config['min_improvement_threshold'] - 0.01)
                    logger.info("í•™ìŠµ íš¨ìœ¨ì„± ê°œì„ : ê°œì„  ì„ê³„ê°’ ë‚®ì¶¤")
            
            elif strategy_update.strategy_name == "learning_speed":
                # í•™ìŠµ ì†ë„ ê°œì„ 
                if strategy_update.current_performance < 1.0:
                    self.learning_config['cycle_timeout'] = max(120, self.learning_config['cycle_timeout'] - 30)
                    logger.info("í•™ìŠµ ì†ë„ ê°œì„ : ì‚¬ì´í´ íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•")
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ì „ëµ ì—…ë°ì´íŠ¸ ì ìš© ì‹¤íŒ¨: {e}")
    
    def update_learning_strategy(self, retrospection: Dict[str, Any]) -> Dict[str, Any]:
        """ë°˜ì„± ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµ ì „ëµ ì—…ë°ì´íŠ¸ - ê¸°ì¡´ ë©”ì„œë“œë“¤ê³¼ ì™„ë²½ í†µí•©"""
        try:
            logger.info("ğŸ”„ í•™ìŠµ ì „ëµ ì—…ë°ì´íŠ¸ ì‹œì‘")
            
            result = {
                "applied_improvements": [],
                "strategy_updates": [],
                "meta_reflection_log": retrospection,
                "update_timestamp": datetime.now().isoformat()
            }
            
            # ë°˜ì„± ê²°ê³¼ì—ì„œ ê°œì„ ì•ˆ ì¶”ì¶œ
            improvement_proposal = retrospection.get("improvement_proposal", {})
            specific_improvements = improvement_proposal.get("specific_improvements", [])
            
            # ìš°ì„ ìˆœìœ„ë³„ ê°œì„ ì•ˆ ì ìš©
            for improvement in specific_improvements:
                if isinstance(improvement, dict):
                    priority = improvement.get("priority", "medium")
                    description = improvement.get("description", str(improvement))
                else:
                    priority = "medium"
                    description = str(improvement)
                
                if priority == "critical":
                    self._apply_critical_improvement({"category": "learning", "description": description})
                    result["applied_improvements"].append({"priority": "critical", "description": description})
                    logger.info(f"ğŸ”´ ì¤‘ìš” ê°œì„ ì•ˆ ì ìš©: {description}")
                
                elif priority == "high":
                    self._apply_high_priority_improvement({"category": "learning", "description": description})
                    result["applied_improvements"].append({"priority": "high", "description": description})
                    logger.info(f"ğŸŸ¡ ë†’ì€ ìš°ì„ ìˆœìœ„ ê°œì„ ì•ˆ ì ìš©: {description}")
                
                else:
                    # ì¼ë°˜ ê°œì„ ì•ˆì€ í•™ìŠµ ê°•ë„ ì¡°ì •
                    self._adjust_learning_intensity("increase")
                    result["strategy_updates"].append({"type": "intensity_increase", "description": description})
                    logger.info(f"ğŸŸ¢ ì¼ë°˜ ê°œì„ ì•ˆ ì ìš©: {description}")
            
            # ìì²´ í‰ê°€ ê²°ê³¼ ë°˜ì˜
            self_assessment = retrospection.get("self_assessment", {})
            if self_assessment.get("overall_rating") == "needs_improvement":
                self._adjust_learning_intensity("reduce")
                result["strategy_updates"].append({"type": "intensity_reduce", "reason": "ì„±ê³¼ ê°œì„  í•„ìš”"})
                logger.info("ğŸ“‰ ì„±ê³¼ ê°œì„ ì„ ìœ„í•´ í•™ìŠµ ê°•ë„ ê°ì†Œ")
            
            # ë©”íƒ€ ë°˜ì„± ë¡œê·¸ ì €ì¥
            self._log_meta_reflection(retrospection)
            
            logger.info(f"âœ… í•™ìŠµ ì „ëµ ì—…ë°ì´íŠ¸ ì™„ë£Œ - ì ìš©ëœ ê°œì„ ì•ˆ: {len(result['applied_improvements'])}ê°œ")
            return result
            
        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì „ëµ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
            return {
                "error": str(e),
                "applied_improvements": [],
                "strategy_updates": [],
                "meta_reflection_log": retrospection
            }

    def _log_meta_reflection(self, retrospection: Dict[str, Any]):
        """ë©”íƒ€ ë°˜ì„± ê²°ê³¼ ê¸°ë¡"""
        try:
            # ë°˜ì„± ê¸°ë¡ì„ ë©”ëª¨ë¦¬ì— ì €ì¥
            reflection_log = {
                "timestamp": datetime.now().isoformat(),
                "accepted_criticisms": retrospection.get("accepted_criticisms", []),
                "improvement_proposals": retrospection.get("improvement_proposal", {}).get("specific_improvements", []),
                "self_assessment": retrospection.get("self_assessment", {}),
                "meta_analysis": retrospection.get("meta_analysis", {})
            }
            
            # ë©”ëª¨ë¦¬ ë™ê¸°í™” ì‹œìŠ¤í…œì— ì €ì¥
            if hasattr(self, 'memory_sync'):
                self.memory_sync.store_data(
                    data_type=MemoryType.LEARNING_REFLECTION,
                    data=reflection_log,
                    metadata={"source": "learning_loop_manager", "type": "meta_reflection"}
                )
            
            logger.info("ğŸ“ ë©”íƒ€ ë°˜ì„± ë¡œê·¸ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ë©”íƒ€ ë°˜ì„± ë¡œê·¸ ì €ì¥ ì˜¤ë¥˜: {e}")

    def _adjust_learning_intensity(self, direction: str):
        """í•™ìŠµ ê°•ë„ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤."""
        try:
            if direction == "reduce":
                # í•™ìŠµ ê°•ë„ ê°ì†Œ
                self.learning_config['cycle_timeout'] = min(600, self.learning_config['cycle_timeout'] + 60)
                self.learning_config['meta_learning_interval'] = min(7200, self.learning_config['meta_learning_interval'] + 600)
                logger.info("í•™ìŠµ ê°•ë„ ê°ì†Œ")
            
            elif direction == "increase":
                # í•™ìŠµ ê°•ë„ ì¦ê°€
                self.learning_config['cycle_timeout'] = min(300, self.learning_config['cycle_timeout'] - 30)
                self.learning_config['meta_learning_interval'] = max(1800, self.learning_config['meta_learning_interval'] - 300)
                logger.info("í•™ìŠµ ê°•ë„ ì¦ê°€")
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ê°•ë„ ì¡°ì • ì‹¤íŒ¨: {e}")
    
    def get_current_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not self.current_cycle:
            return {"status": "idle", "message": "í•™ìŠµ ë£¨í”„ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ"}
        
        return {
            "status": "running" if self.is_running else "completed",
            "cycle_id": self.current_cycle.cycle_id,
            "current_stage": self.current_cycle.current_stage.value if self.current_cycle.current_stage else None,
            "stages_completed": [stage.value for stage in self.current_cycle.stages_completed],
            "start_time": self.current_cycle.start_time.isoformat(),
            "elapsed_time": (datetime.now() - self.current_cycle.start_time).total_seconds() if self.current_cycle.start_time else 0,
            "meta_learning_enabled": True,
            "last_meta_learning": self.auto_retrospector.last_analysis_time.isoformat() if self.auto_retrospector.last_analysis_time else None,
            "self_assessment_enabled": True,
            "last_self_assessment": self.self_assessment_manager.last_assessment_time.isoformat() if self.self_assessment_manager.last_assessment_time else None,
            "assessment_statistics": self.self_assessment_manager.get_assessment_statistics(),
            "goal_oriented_thinking_enabled": True,
            "goal_statistics": self.goal_oriented_thinking.get_goal_statistics(),
            "emotional_ethical_judgment_enabled": True,
            "judgment_statistics": self.emotional_ethical_judgment.get_judgment_statistics(),
            "autonomous_goal_setting_enabled": True,
            "autonomous_goal_statistics": self.autonomous_goal_setting.get_autonomous_goal_statistics(),
            "advanced_creativity_system_enabled": True,
            "creativity_statistics": self.advanced_creativity_system.get_creativity_statistics()
        }

    def _run_self_assessment_cycle(self):
        """ìê¸° í‰ê°€ ì‚¬ì´í´ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            logger.info("ìê¸° í‰ê°€ ì‚¬ì´í´ ì‹œì‘")
            
            # ì¢…í•© ìê¸° í‰ê°€ ì‹¤í–‰
            assessment_result = self.self_assessment_manager.run_comprehensive_assessment()
            
            # í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµ ì „ëµ ì¡°ì •
            self._adjust_learning_strategy_based_on_assessment(assessment_result)
            
            logger.info(f"ìê¸° í‰ê°€ ì‚¬ì´í´ ì™„ë£Œ - ì „ì²´ ì ìˆ˜: {assessment_result.overall_score:.2f}")
            
        except Exception as e:
            logger.error(f"ìê¸° í‰ê°€ ì‚¬ì´í´ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _adjust_learning_strategy_based_on_assessment(self, assessment_result):
        """í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµ ì „ëµì„ ì¡°ì •í•©ë‹ˆë‹¤."""
        try:
            overall_score = assessment_result.overall_score
            
            # ì „ì²´ ì ìˆ˜ì— ë”°ë¥¸ í•™ìŠµ ê°•ë„ ì¡°ì •
            if overall_score < 0.5:
                # ì ìˆ˜ê°€ ë‚®ì€ ê²½ìš° í•™ìŠµ ê°•ë„ ê°ì†Œ
                self.learning_config['cycle_timeout'] = min(600, self.learning_config['cycle_timeout'] + 60)
                self.learning_config['meta_learning_interval'] = min(7200, self.learning_config['meta_learning_interval'] + 600)
                logger.warning("ì „ì²´ ì ìˆ˜ê°€ ë‚®ì•„ í•™ìŠµ ê°•ë„ë¥¼ ê°ì†Œì‹œí‚µë‹ˆë‹¤")
                
            elif overall_score > 0.8:
                # ì ìˆ˜ê°€ ë†’ì€ ê²½ìš° í•™ìŠµ ê°•ë„ ì¦ê°€
                self.learning_config['cycle_timeout'] = max(120, self.learning_config['cycle_timeout'] - 30)
                self.learning_config['meta_learning_interval'] = max(1800, self.learning_config['meta_learning_interval'] - 300)
                logger.info("ì „ì²´ ì ìˆ˜ê°€ ë†’ì•„ í•™ìŠµ ê°•ë„ë¥¼ ì¦ê°€ì‹œí‚µë‹ˆë‹¤")
            
            # ì¤‘ìš” ì´ìŠˆì— ë”°ë¥¸ íŠ¹ë³„ ì¡°ì •
            for issue in assessment_result.critical_issues:
                if "CPU ì‚¬ìš©ë¥ " in issue:
                    self.learning_config['cycle_timeout'] = min(600, self.learning_config['cycle_timeout'] + 60)
                    logger.warning("CPU ì‚¬ìš©ë¥  ë¬¸ì œë¡œ í•™ìŠµ ê°•ë„ë¥¼ ê°ì†Œì‹œí‚µë‹ˆë‹¤")
                    
                elif "ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ " in issue:
                    self.learning_config['meta_learning_interval'] = min(7200, self.learning_config['meta_learning_interval'] + 600)
                    logger.warning("ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë¬¸ì œë¡œ ë©”íƒ€ í•™ìŠµ ê°„ê²©ì„ ëŠ˜ë¦½ë‹ˆë‹¤")
                    
                elif "ì˜¤ë¥˜ìœ¨" in issue:
                    self.learning_config['max_failure_streak'] = max(3, self.learning_config['max_failure_streak'] - 1)
                    logger.warning("ì˜¤ë¥˜ìœ¨ ë¬¸ì œë¡œ ì‹¤íŒ¨ í—ˆìš© íšŸìˆ˜ë¥¼ ê°ì†Œì‹œí‚µë‹ˆë‹¤")
            
            # ê°œì„  ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ì¡°ì •
            for priority in assessment_result.improvement_priorities:
                if "í•™ìŠµ" in priority:
                    self.learning_config['min_improvement_threshold'] = max(0.05, self.learning_config['min_improvement_threshold'] - 0.01)
                    logger.info("í•™ìŠµ ê°œì„ ì„ ìœ„í•´ ê°œì„  ì„ê³„ê°’ì„ ì¡°ì •í•©ë‹ˆë‹¤")
                    
        except Exception as e:
            logger.error(f"í•™ìŠµ ì „ëµ ì¡°ì • ì¤‘ ì˜¤ë¥˜: {e}")

    def _run_goal_oriented_thinking_cycle(self):
        """ëª©í‘œ ì§€í–¥ì  ì‚¬ê³  ì‚¬ì´í´ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            logger.info("ëª©í‘œ ì§€í–¥ì  ì‚¬ê³  ì‚¬ì´í´ ì‹œì‘")
            
            # ìƒˆë¡œìš´ ëª©í‘œ ìƒì„±
            new_goals = self.goal_oriented_thinking.generate_goals()
            
            if new_goals:
                logger.info(f"ìƒˆë¡œìš´ ëª©í‘œ {len(new_goals)}ê°œ ìƒì„±ë¨")
                for goal in new_goals:
                    logger.info(f"ëª©í‘œ: {goal.title} ({goal.category.value})")
                    # ëª©í‘œë¥¼ í™œì„± ëª©í‘œ ëª©ë¡ì— ì¶”ê°€
                    self.goal_oriented_thinking.active_goals.append(goal)
                    
                    # ëª©í‘œ ì‹¤í–‰ ê³„íš ìƒì„±
                    plan = self.goal_oriented_thinking.create_execution_plan(goal)
                    if plan:
                        logger.info(f"ëª©í‘œ ì‹¤í–‰ ê³„íš ìƒì„±: {plan.plan_id}")
            
            logger.info("ëª©í‘œ ì§€í–¥ì  ì‚¬ê³  ì‚¬ì´í´ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ëª©í‘œ ì§€í–¥ì  ì‚¬ê³  ì‚¬ì´í´ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

    def _should_run_emotional_ethical_judgment(self) -> bool:
        """ê°ì •/ìœ¤ë¦¬ íŒë‹¨ ëª¨ë“ˆì´ ì‹¤í–‰ë˜ì–´ì•¼ í•˜ëŠ”ì§€ ê²°ì •í•©ë‹ˆë‹¤."""
        # ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ (ì˜ˆ: 10ë²ˆì§¸ ì‚¬ì´í´ë§ˆë‹¤)
        return self.learning_cycle_count % 10 == 0

    def _run_emotional_ethical_judgment_cycle(self):
        """ê°ì •/ìœ¤ë¦¬ íŒë‹¨ ì‚¬ì´í´ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            logger.info("ê°ì •/ìœ¤ë¦¬ íŒë‹¨ ì‚¬ì´í´ ì‹œì‘")
            
            # í˜„ì¬ í•™ìŠµ ìƒí™©ì— ëŒ€í•œ ê°ì •/ìœ¤ë¦¬ íŒë‹¨
            situation = "í•™ìŠµ ë£¨í”„ ì‹¤í–‰ ì¤‘ - ì „ëµ ì ìš© ë° ê²°ê³¼ í‰ê°€"
            result = self.emotional_ethical_judgment.make_judgment(
                situation=situation,
                judgment_type=self.emotional_ethical_judgment.JudgmentType.HYBRID
            )
            
            if result:
                logger.info(f"ê°ì •/ìœ¤ë¦¬ íŒë‹¨ ì™„ë£Œ: {result.decision}")
                logger.info(f"ì ìˆ˜ - ìœ¤ë¦¬ì : {result.ethical_score:.2f}, ê°ì •ì : {result.emotional_score:.2f}, ì „ì²´: {result.overall_score:.2f}")
                
                # íŒë‹¨ ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
                self.memory_sync.store_experience(
                    experience_type=MemoryType.LEARNING_EXPERIENCE,
                    content=f"ê°ì •/ìœ¤ë¦¬ íŒë‹¨: {result.decision} - {result.reasoning}",
                    metadata={
                        "judgment_type": result.judgment_type.value,
                        "ethical_score": result.ethical_score,
                        "emotional_score": result.emotional_score,
                        "overall_score": result.overall_score,
                        "confidence": result.confidence.value
                    }
                )
            
            logger.info("ê°ì •/ìœ¤ë¦¬ íŒë‹¨ ì‚¬ì´í´ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ê°ì •/ìœ¤ë¦¬ íŒë‹¨ ì‚¬ì´í´ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

    def _run_autonomous_goal_setting_cycle(self):
        """ììœ¨ ëª©í‘œ ì„¤ì • ì‚¬ì´í´ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            logger.info("ììœ¨ ëª©í‘œ ì„¤ì • ì‚¬ì´í´ ì‹œì‘")
            
            # ììœ¨ ëª©í‘œ ìƒì„±
            new_autonomous_goals = self.autonomous_goal_setting.generate_autonomous_goals()
            
            if new_autonomous_goals:
                logger.info(f"ìƒˆë¡œìš´ ììœ¨ ëª©í‘œ {len(new_autonomous_goals)}ê°œ ìƒì„±ë¨")
                
                # ëª©í‘œ ìš°ì„ ìˆœìœ„ ê²°ì •
                prioritization_result = self.autonomous_goal_setting.prioritize_goals(new_autonomous_goals)
                
                if prioritization_result:
                    logger.info(f"ëª©í‘œ ìš°ì„ ìˆœìœ„ ê²°ì • ì™„ë£Œ: {prioritization_result.reasoning}")
                    
                    # ìƒìœ„ ëª©í‘œë“¤ì„ ë©”ëª¨ë¦¬ì— ì €ì¥
                    for goal in prioritization_result.goals[:3]:  # ìƒìœ„ 3ê°œ ëª©í‘œ
                        self.memory_sync.store_experience(
                            experience_type=MemoryType.LEARNING_EXPERIENCE,
                            content=f"ììœ¨ ëª©í‘œ: {goal.title} - {goal.description}",
                            metadata={
                                "goal_id": goal.goal_id,
                                "source": goal.source.value,
                                "priority_score": goal.priority_score,
                                "overall_score": goal.overall_score
                            }
                        )
            
            logger.info("ììœ¨ ëª©í‘œ ì„¤ì • ì‚¬ì´í´ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ììœ¨ ëª©í‘œ ì„¤ì • ì‚¬ì´í´ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

    def _run_creativity_enhancement_cycle(self):
        """ì°½ì˜ì„± ê³ ë„í™” ì‚¬ì´í´ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            logger.info("ì°½ì˜ì„± ê³ ë„í™” ì‚¬ì´í´ ì‹œì‘")
            
            # ì°½ì˜ì„± ì„¸ì…˜ ì‹¤í–‰
            session = self.advanced_creativity_system.run_creativity_session(
                context="ì‹œìŠ¤í…œ ì„±ëŠ¥ ë° ê¸°ëŠ¥ ê°œì„ ",
                duration_minutes=15
            )
            
            if session:
                logger.info(f"ì°½ì˜ì„± ì„¸ì…˜ ì™„ë£Œ: {len(session.ideas_generated)}ê°œ ì•„ì´ë””ì–´ ìƒì„±")
                logger.info(f"ì„¸ì…˜ í’ˆì§ˆ: {session.session_quality:.2f}")
                
                # ì°½ì˜ì  ì•„ì´ë””ì–´ë“¤ì„ ë©”ëª¨ë¦¬ì— ì €ì¥
                for idea in session.ideas_generated[:5]:  # ìƒìœ„ 5ê°œ ì•„ì´ë””ì–´
                    self.memory_sync.store_experience(
                        experience_type=MemoryType.LEARNING_EXPERIENCE,
                        content=f"ì°½ì˜ì  ì•„ì´ë””ì–´: {idea.title} - {idea.description}",
                        metadata={
                            "idea_id": idea.idea_id,
                            "creativity_type": idea.creativity_type.value,
                            "technique_used": idea.technique_used.value,
                            "overall_score": idea.overall_score
                        }
                    )
                
                # ì¸ì‚¬ì´íŠ¸ ë°œê²¬
                if session.insights_discovered:
                    logger.info(f"ë°œê²¬ëœ ì¸ì‚¬ì´íŠ¸: {session.insights_discovered}")
            
            logger.info("ì°½ì˜ì„± ê³ ë„í™” ì‚¬ì´í´ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì°½ì˜ì„± ê³ ë„í™” ì‚¬ì´í´ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

    def _execute_learning_cycle(self, context: Optional[Dict[str, Any]]) -> LearningResult:
        """ë‹¨ì¼ í•™ìŠµ ì‚¬ì´í´ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        stage_results = {}
        
        try:
            # 1ë‹¨ê³„: ëª¨ë°©
            logger.info("1ë‹¨ê³„: ëª¨ë°© ì‹œì‘")
            self.current_cycle.current_stage = LearningStage.IMITATION
            imitation_result = self._execute_imitation_stage(context)
            stage_results[LearningStage.IMITATION] = imitation_result
            self.current_cycle.stages_completed.append(LearningStage.IMITATION)
            
            # 2ë‹¨ê³„: ë°˜ë³µ
            logger.info("2ë‹¨ê³„: ë°˜ë³µ ì‹œì‘")
            self.current_cycle.current_stage = LearningStage.PRACTICE
            practice_result = self._execute_practice_stage(context)
            stage_results[LearningStage.PRACTICE] = practice_result
            self.current_cycle.stages_completed.append(LearningStage.PRACTICE)
            
            # 3ë‹¨ê³„: í”¼ë“œë°±
            logger.info("3ë‹¨ê³„: í”¼ë“œë°± ì‹œì‘")
            self.current_cycle.current_stage = LearningStage.FEEDBACK
            feedback_result = self._execute_feedback_stage(practice_result, context)
            stage_results[LearningStage.FEEDBACK] = feedback_result
            self.current_cycle.stages_completed.append(LearningStage.FEEDBACK)
            
            # 4ë‹¨ê³„: ë„ì „
            logger.info("4ë‹¨ê³„: ë„ì „ ì‹œì‘")
            self.current_cycle.current_stage = LearningStage.CHALLENGE
            challenge_result = self._execute_challenge_stage(feedback_result, context)
            stage_results[LearningStage.CHALLENGE] = challenge_result
            self.current_cycle.stages_completed.append(LearningStage.CHALLENGE)
            
            # 5ë‹¨ê³„: ê°œì„ 
            logger.info("5ë‹¨ê³„: ê°œì„  ì‹œì‘")
            self.current_cycle.current_stage = LearningStage.IMPROVEMENT
            improvement_result = self._execute_improvement_stage(challenge_result, context)
            stage_results[LearningStage.IMPROVEMENT] = improvement_result
            self.current_cycle.stages_completed.append(LearningStage.IMPROVEMENT)
            
            # ì „ì²´ ì„±ê³¼ ê³„ì‚°
            overall_performance = self._calculate_overall_performance(stage_results)
            improvement_score = self._calculate_improvement_score(stage_results)
            recommendations = self._generate_recommendations(stage_results)
            next_actions = self._determine_next_actions(stage_results)
            
            # ì „ëµ ìƒì¡´ íŒë‹¨
            survival_decision = self._evaluate_strategy_survival(overall_performance, context)
            
            # ì‚¬ì´í´ ì™„ë£Œ
            self.current_cycle.end_time = datetime.now()
            self.current_cycle.performance_metrics = {
                'overall_performance': overall_performance,
                'improvement_score': improvement_score,
                'survival_action': survival_decision.action.value if survival_decision else 'unknown'
            }
            self.current_cycle.success = improvement_score > 0.0
            
            if improvement_result.get('modified_strategy'):
                self.current_cycle.modified_strategy = improvement_result['modified_strategy']
            
            result = LearningResult(
                cycle=self.current_cycle,
                stage_results=stage_results,
                overall_performance=overall_performance,
                improvement_score=improvement_score,
                recommendations=recommendations,
                next_actions=next_actions
            )
            
            logger.info(f"í•™ìŠµ ì‚¬ì´í´ ì™„ë£Œ: ì„±ê³¼ {overall_performance:.2f}, ê°œì„ ì ìˆ˜ {improvement_score:.2f}")
            return result
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            self.current_cycle.end_time = datetime.now()
            self.current_cycle.success = False
            
            return LearningResult(
                cycle=self.current_cycle,
                stage_results=stage_results,
                overall_performance=0.0,
                improvement_score=0.0,
                recommendations=["í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"],
                next_actions=["ì˜¤ë¥˜ ìˆ˜ì • í›„ ì¬ì‹œë„"]
            )

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_learning_loop_manager = None

def get_learning_loop_manager() -> LearningLoopManager:
    """LearningLoopManager ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _learning_loop_manager
    if _learning_loop_manager is None:
        _learning_loop_manager = LearningLoopManager()
    return _learning_loop_manager 