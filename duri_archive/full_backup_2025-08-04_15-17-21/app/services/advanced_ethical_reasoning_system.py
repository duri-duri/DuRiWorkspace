#!/usr/bin/env python3
"""
AdvancedEthicalReasoningSystem - Phase 13.3
ê³ ê¸‰ ìœ¤ë¦¬ì  ì¶”ë¡  ì‹œìŠ¤í…œ

ëª©ì :
- ë³µì¡í•œ ìœ¤ë¦¬ì  ìƒí™©ì—ì„œì˜ ì •êµí•œ íŒë‹¨ ëŠ¥ë ¥
- ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„ ë° ê°€ì¹˜ ì¶©ëŒ í•´ê²°
- ê°€ì¡± ì¤‘ì‹¬ì˜ ë„ë•ì  ì„±ì¥ ì§€ì›
"""

import json
import logging
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EthicalPrinciple(Enum):
    """ìœ¤ë¦¬ì  ì›ì¹™"""

    AUTONOMY = "autonomy"
    BENEFICENCE = "beneficence"
    NON_MALEFICENCE = "non_maleficence"
    JUSTICE = "justice"
    FAMILY_HARMONY = "family_harmony"
    TRUTHFULNESS = "truthfulness"
    RESPECT = "respect"
    CARE = "care"


class DilemmaComplexity(Enum):
    """ë”œë ˆë§ˆ ë³µì¡ì„±"""

    SIMPLE = "simple"
    MODERATE = "moderate"
    COMPLEX = "complex"
    VERY_COMPLEX = "very_complex"


class ValueConflict(Enum):
    """ê°€ì¹˜ ì¶©ëŒ"""

    AUTONOMY_VS_BENEFICENCE = "autonomy_vs_beneficence"
    TRUTH_VS_HARM = "truth_vs_harm"
    JUSTICE_VS_CARE = "justice_vs_care"
    FAMILY_VS_INDIVIDUAL = "family_vs_individual"
    SHORT_TERM_VS_LONG_TERM = "short_term_vs_long_term"


class ReasoningMethod(Enum):
    """ì¶”ë¡  ë°©ë²•"""

    UTILITARIAN = "utilitarian"
    DEONTOLOGICAL = "deontological"
    VIRTUE_ETHICS = "virtue_ethics"
    CARE_ETHICS = "care_ethics"
    FAMILY_CENTRIC = "family_centric"


class MoralJudgment(Enum):
    """ë„ë•ì  íŒë‹¨"""

    CLEARLY_RIGHT = "clearly_right"
    PROBABLY_RIGHT = "probably_right"
    UNCLEAR = "unclear"
    PROBABLY_WRONG = "probably_wrong"
    CLEARLY_WRONG = "clearly_wrong"


@dataclass
class EthicalDilemma:
    """ìœ¤ë¦¬ì  ë”œë ˆë§ˆ"""

    id: str
    description: str
    complexity: DilemmaComplexity
    involved_principles: List[EthicalPrinciple]
    value_conflicts: List[ValueConflict]
    family_context: Dict[str, Any]
    stakeholders: List[str]
    potential_outcomes: List[str]
    timestamp: datetime


@dataclass
class EthicalAnalysis:
    """ìœ¤ë¦¬ì  ë¶„ì„"""

    id: str
    dilemma_id: str
    reasoning_method: ReasoningMethod
    principle_weights: Dict[EthicalPrinciple, float]
    conflict_resolution: Dict[ValueConflict, str]
    moral_judgment: MoralJudgment
    confidence_score: float
    reasoning_steps: List[str]
    family_impact: str
    timestamp: datetime


@dataclass
class EthicalRecommendation:
    """ìœ¤ë¦¬ì  ê¶Œê³ """

    id: str
    analysis_id: str
    recommended_action: str
    alternative_actions: List[str]
    expected_outcomes: List[str]
    risk_assessment: Dict[str, float]
    family_considerations: List[str]
    moral_justification: str
    implementation_steps: List[str]
    timestamp: datetime


class AdvancedEthicalReasoningSystem:
    """ê³ ê¸‰ ìœ¤ë¦¬ì  ì¶”ë¡  ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.ethical_dilemmas: List[EthicalDilemma] = []
        self.ethical_analyses: List[EthicalAnalysis] = []
        self.ethical_recommendations: List[EthicalRecommendation] = []
        self.family_values: Dict[str, float] = {}
        self.moral_development: Dict[str, Any] = {}

        logger.info("AdvancedEthicalReasoningSystem ì´ˆê¸°í™” ì™„ë£Œ")

    def analyze_ethical_dilemma(
        self,
        dilemma_description: str,
        family_context: Dict[str, Any],
        stakeholders: List[str],
        potential_outcomes: List[str],
    ) -> EthicalDilemma:
        """ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„"""
        dilemma_id = f"dilemma_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # ë³µì¡ì„± ë¶„ì„
        complexity = self._analyze_dilemma_complexity(dilemma_description, stakeholders)

        # ê´€ë ¨ ì›ì¹™ ì‹ë³„
        involved_principles = self._identify_involved_principles(
            dilemma_description, family_context
        )

        # ê°€ì¹˜ ì¶©ëŒ ì‹ë³„
        value_conflicts = self._identify_value_conflicts(
            involved_principles, family_context
        )

        dilemma = EthicalDilemma(
            id=dilemma_id,
            description=dilemma_description,
            complexity=complexity,
            involved_principles=involved_principles,
            value_conflicts=value_conflicts,
            family_context=family_context,
            stakeholders=stakeholders,
            potential_outcomes=potential_outcomes,
            timestamp=datetime.now(),
        )

        self.ethical_dilemmas.append(dilemma)
        logger.info(f"ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„ ì™„ë£Œ: {complexity.value}")

        return dilemma

    def _analyze_dilemma_complexity(
        self, description: str, stakeholders: List[str]
    ) -> DilemmaComplexity:
        """ë”œë ˆë§ˆ ë³µì¡ì„± ë¶„ì„"""
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë³µì¡ì„± ë¶„ì„
        complexity_keywords = {
            "simple": ["ë‹¨ìˆœ", "ëª…í™•", "ì§ê´€"],
            "moderate": ["ê³ ë ¤", "ê· í˜•", "ì¤‘ê°„"],
            "complex": ["ë³µì¡", "ë‹¤ì–‘", "ì¶©ëŒ"],
            "very_complex": ["ê·¹ë„", "ë‹¤ì¸µ", "ìƒì¶©"],
        }

        description_lower = description.lower()
        stakeholder_count = len(stakeholders)

        # í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°
        keyword_scores = {}
        for complexity, keywords in complexity_keywords.items():
            score = sum(1 for keyword in keywords if keyword in description_lower)
            keyword_scores[complexity] = score

        # ì´í•´ê´€ê³„ì ìˆ˜ì— ë”°ë¥¸ ë³µì¡ì„± ì¡°ì •
        if stakeholder_count <= 2:
            complexity_multiplier = 0.8
        elif stakeholder_count <= 4:
            complexity_multiplier = 1.0
        else:
            complexity_multiplier = 1.3

        # ìµœì¢… ë³µì¡ì„± ê²°ì •
        total_score = sum(keyword_scores.values()) * complexity_multiplier

        if total_score <= 1:
            return DilemmaComplexity.SIMPLE
        elif total_score <= 3:
            return DilemmaComplexity.MODERATE
        elif total_score <= 5:
            return DilemmaComplexity.COMPLEX
        else:
            return DilemmaComplexity.VERY_COMPLEX

    def _identify_involved_principles(
        self, description: str, family_context: Dict[str, Any]
    ) -> List[EthicalPrinciple]:
        """ê´€ë ¨ ì›ì¹™ ì‹ë³„"""
        principles = []

        # í‚¤ì›Œë“œ ê¸°ë°˜ ì›ì¹™ ì‹ë³„
        principle_keywords = {
            EthicalPrinciple.AUTONOMY: ["ììœ¨", "ì„ íƒ", "ì˜ì‚¬ê²°ì •", "ììœ "],
            EthicalPrinciple.BENEFICENCE: ["ì´ìµ", "ë„ì›€", "ì„ í–‰", "í˜œíƒ"],
            EthicalPrinciple.NON_MALEFICENCE: ["í•´ì•…", "ì†ìƒ", "ìœ„í—˜", "í”¼í•´"],
            EthicalPrinciple.JUSTICE: ["ê³µì •", "í‰ë“±", "ì •ì˜", "ê³µí‰"],
            EthicalPrinciple.FAMILY_HARMONY: ["ê°€ì¡±", "í™”í•©", "ì¡°í™”", "ë‹¨ê²°"],
            EthicalPrinciple.TRUTHFULNESS: ["ì§„ì‹¤", "ì •ì§", "ê±°ì§“", "ë¹„ë°€"],
            EthicalPrinciple.RESPECT: ["ì¡´ì¤‘", "ì¸ì •", "ë°°ë ¤", "ì˜ˆì˜"],
            EthicalPrinciple.CARE: ["ëŒë´„", "ë³´ì‚´í•Œ", "ê´€ì‹¬", "ì‚¬ë‘"],
        }

        description_lower = description.lower()

        for principle, keywords in principle_keywords.items():
            if any(keyword in description_lower for keyword in keywords):
                principles.append(principle)

        # ê°€ì¡± ë§¥ë½ì— ë”°ë¥¸ ì¶”ê°€ ì›ì¹™
        if family_context.get("has_children", False):
            principles.append(EthicalPrinciple.CARE)
        if family_context.get("has_elderly", False):
            principles.append(EthicalPrinciple.RESPECT)

        return list(set(principles))  # ì¤‘ë³µ ì œê±°

    def _identify_value_conflicts(
        self, principles: List[EthicalPrinciple], family_context: Dict[str, Any]
    ) -> List[ValueConflict]:
        """ê°€ì¹˜ ì¶©ëŒ ì‹ë³„"""
        conflicts = []

        # ì›ì¹™ ìŒì— ë”°ë¥¸ ì¶©ëŒ ì‹ë³„
        principle_pairs = [
            (EthicalPrinciple.AUTONOMY, EthicalPrinciple.BENEFICENCE),
            (EthicalPrinciple.TRUTHFULNESS, EthicalPrinciple.NON_MALEFICENCE),
            (EthicalPrinciple.JUSTICE, EthicalPrinciple.CARE),
            (EthicalPrinciple.FAMILY_HARMONY, EthicalPrinciple.AUTONOMY),
        ]

        for principle1, principle2 in principle_pairs:
            if principle1 in principles and principle2 in principles:
                if (principle1, principle2) == (
                    EthicalPrinciple.AUTONOMY,
                    EthicalPrinciple.BENEFICENCE,
                ):
                    conflicts.append(ValueConflict.AUTONOMY_VS_BENEFICENCE)
                elif (principle1, principle2) == (
                    EthicalPrinciple.TRUTHFULNESS,
                    EthicalPrinciple.NON_MALEFICENCE,
                ):
                    conflicts.append(ValueConflict.TRUTH_VS_HARM)
                elif (principle1, principle2) == (
                    EthicalPrinciple.JUSTICE,
                    EthicalPrinciple.CARE,
                ):
                    conflicts.append(ValueConflict.JUSTICE_VS_CARE)
                elif (principle1, principle2) == (
                    EthicalPrinciple.FAMILY_HARMONY,
                    EthicalPrinciple.AUTONOMY,
                ):
                    conflicts.append(ValueConflict.FAMILY_VS_INDIVIDUAL)

        return conflicts

    def conduct_ethical_reasoning(self, dilemma: EthicalDilemma) -> EthicalAnalysis:
        """ìœ¤ë¦¬ì  ì¶”ë¡  ìˆ˜í–‰"""
        analysis_id = f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # ì¶”ë¡  ë°©ë²• ì„ íƒ
        reasoning_method = self._select_reasoning_method(dilemma)

        # ì›ì¹™ ê°€ì¤‘ì¹˜ ê³„ì‚°
        principle_weights = self._calculate_principle_weights(dilemma, reasoning_method)

        # ê°€ì¹˜ ì¶©ëŒ í•´ê²°
        conflict_resolution = self._resolve_value_conflicts(dilemma, principle_weights)

        # ë„ë•ì  íŒë‹¨
        moral_judgment = self._make_moral_judgment(
            dilemma, principle_weights, conflict_resolution
        )

        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence_score = self._calculate_confidence_score(
            dilemma, reasoning_method, moral_judgment
        )

        # ì¶”ë¡  ë‹¨ê³„
        reasoning_steps = self._generate_reasoning_steps(
            dilemma, reasoning_method, principle_weights
        )

        # ê°€ì¡± ì˜í–¥ ë¶„ì„
        family_impact = self._analyze_family_impact(dilemma, moral_judgment)

        analysis = EthicalAnalysis(
            id=analysis_id,
            dilemma_id=dilemma.id,
            reasoning_method=reasoning_method,
            principle_weights=principle_weights,
            conflict_resolution=conflict_resolution,
            moral_judgment=moral_judgment,
            confidence_score=confidence_score,
            reasoning_steps=reasoning_steps,
            family_impact=family_impact,
            timestamp=datetime.now(),
        )

        self.ethical_analyses.append(analysis)
        logger.info(f"ìœ¤ë¦¬ì  ì¶”ë¡  ì™„ë£Œ: {reasoning_method.value}")

        return analysis

    def _select_reasoning_method(self, dilemma: EthicalDilemma) -> ReasoningMethod:
        """ì¶”ë¡  ë°©ë²• ì„ íƒ"""
        # ë”œë ˆë§ˆ íŠ¹ì„±ì— ë”°ë¥¸ ë°©ë²• ì„ íƒ
        if dilemma.complexity in [
            DilemmaComplexity.COMPLEX,
            DilemmaComplexity.VERY_COMPLEX,
        ]:
            return ReasoningMethod.FAMILY_CENTRIC
        elif EthicalPrinciple.CARE in dilemma.involved_principles:
            return ReasoningMethod.CARE_ETHICS
        elif EthicalPrinciple.JUSTICE in dilemma.involved_principles:
            return ReasoningMethod.DEONTOLOGICAL
        elif len(dilemma.potential_outcomes) > 3:
            return ReasoningMethod.UTILITARIAN
        else:
            return ReasoningMethod.VIRTUE_ETHICS

    def _calculate_principle_weights(
        self, dilemma: EthicalDilemma, reasoning_method: ReasoningMethod
    ) -> Dict[EthicalPrinciple, float]:
        """ì›ì¹™ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        weights = {}

        # ê¸°ë³¸ ê°€ì¤‘ì¹˜
        base_weights = {
            EthicalPrinciple.AUTONOMY: 0.8,
            EthicalPrinciple.BENEFICENCE: 0.9,
            EthicalPrinciple.NON_MALEFICENCE: 0.9,
            EthicalPrinciple.JUSTICE: 0.8,
            EthicalPrinciple.FAMILY_HARMONY: 1.0,
            EthicalPrinciple.TRUTHFULNESS: 0.7,
            EthicalPrinciple.RESPECT: 0.8,
            EthicalPrinciple.CARE: 0.9,
        }

        # ì¶”ë¡  ë°©ë²•ì— ë”°ë¥¸ ì¡°ì •
        method_adjustments = {
            ReasoningMethod.FAMILY_CENTRIC: {EthicalPrinciple.FAMILY_HARMONY: 1.2},
            ReasoningMethod.CARE_ETHICS: {EthicalPrinciple.CARE: 1.2},
            ReasoningMethod.DEONTOLOGICAL: {EthicalPrinciple.JUSTICE: 1.2},
            ReasoningMethod.UTILITARIAN: {EthicalPrinciple.BENEFICENCE: 1.2},
        }

        # ê°€ì¤‘ì¹˜ ê³„ì‚°
        for principle in dilemma.involved_principles:
            weight = base_weights.get(principle, 0.5)

            # ë°©ë²•ë³„ ì¡°ì • ì ìš©
            if reasoning_method in method_adjustments:
                adjustment = method_adjustments[reasoning_method].get(principle, 1.0)
                weight *= adjustment

            weights[principle] = min(1.0, weight)

        return weights

    def _resolve_value_conflicts(
        self, dilemma: EthicalDilemma, principle_weights: Dict[EthicalPrinciple, float]
    ) -> Dict[ValueConflict, str]:
        """ê°€ì¹˜ ì¶©ëŒ í•´ê²°"""
        resolutions = {}

        for conflict in dilemma.value_conflicts:
            if conflict == ValueConflict.AUTONOMY_VS_BENEFICENCE:
                if principle_weights.get(
                    EthicalPrinciple.BENEFICENCE, 0
                ) > principle_weights.get(EthicalPrinciple.AUTONOMY, 0):
                    resolutions[conflict] = "ì´ìµ ìš°ì„ , ììœ¨ ë³´ì¥"
                else:
                    resolutions[conflict] = "ììœ¨ ìš°ì„ , ì´ìµ ê³ ë ¤"

            elif conflict == ValueConflict.TRUTH_VS_HARM:
                if principle_weights.get(
                    EthicalPrinciple.NON_MALEFICENCE, 0
                ) > principle_weights.get(EthicalPrinciple.TRUTHFULNESS, 0):
                    resolutions[conflict] = "í•´ì•… ë°©ì§€ ìš°ì„ , ì§„ì‹¤ ì¡°ì ˆ"
                else:
                    resolutions[conflict] = "ì§„ì‹¤ ìš°ì„ , í•´ì•… ìµœì†Œí™”"

            elif conflict == ValueConflict.JUSTICE_VS_CARE:
                if principle_weights.get(
                    EthicalPrinciple.CARE, 0
                ) > principle_weights.get(EthicalPrinciple.JUSTICE, 0):
                    resolutions[conflict] = "ëŒë´„ ìš°ì„ , ê³µì • ê³ ë ¤"
                else:
                    resolutions[conflict] = "ê³µì • ìš°ì„ , ëŒë´„ ê³ ë ¤"

            elif conflict == ValueConflict.FAMILY_VS_INDIVIDUAL:
                if principle_weights.get(
                    EthicalPrinciple.FAMILY_HARMONY, 0
                ) > principle_weights.get(EthicalPrinciple.AUTONOMY, 0):
                    resolutions[conflict] = "ê°€ì¡± í™”í•© ìš°ì„ , ê°œì¸ ììœ¨ ê³ ë ¤"
                else:
                    resolutions[conflict] = "ê°œì¸ ììœ¨ ìš°ì„ , ê°€ì¡± í™”í•© ê³ ë ¤"

        return resolutions

    def _make_moral_judgment(
        self,
        dilemma: EthicalDilemma,
        principle_weights: Dict[EthicalPrinciple, float],
        conflict_resolution: Dict[ValueConflict, str],
    ) -> MoralJudgment:
        """ë„ë•ì  íŒë‹¨"""
        # ê°€ì¤‘ í‰ê·  ì ìˆ˜ ê³„ì‚°
        total_weight = sum(principle_weights.values())
        if total_weight == 0:
            return MoralJudgment.UNCLEAR

        weighted_score = sum(weight for weight in principle_weights.values())
        average_score = weighted_score / total_weight

        # ì¶©ëŒ í•´ê²° í’ˆì§ˆ í‰ê°€
        conflict_resolution_quality = len(conflict_resolution) / max(
            1, len(dilemma.value_conflicts)
        )

        # ìµœì¢… íŒë‹¨
        final_score = (average_score + conflict_resolution_quality) / 2

        if final_score >= 0.8:
            return MoralJudgment.CLEARLY_RIGHT
        elif final_score >= 0.6:
            return MoralJudgment.PROBABLY_RIGHT
        elif final_score >= 0.4:
            return MoralJudgment.UNCLEAR
        elif final_score >= 0.2:
            return MoralJudgment.PROBABLY_WRONG
        else:
            return MoralJudgment.CLEARLY_WRONG

    def _calculate_confidence_score(
        self,
        dilemma: EthicalDilemma,
        reasoning_method: ReasoningMethod,
        moral_judgment: MoralJudgment,
    ) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        base_confidence = 0.7

        # ë³µì¡ì„±ì— ë”°ë¥¸ ì¡°ì •
        complexity_adjustments = {
            DilemmaComplexity.SIMPLE: 0.1,
            DilemmaComplexity.MODERATE: 0.0,
            DilemmaComplexity.COMPLEX: -0.1,
            DilemmaComplexity.VERY_COMPLEX: -0.2,
        }
        base_confidence += complexity_adjustments.get(dilemma.complexity, 0.0)

        # íŒë‹¨ ëª…í™•ì„±ì— ë”°ë¥¸ ì¡°ì •
        judgment_adjustments = {
            MoralJudgment.CLEARLY_RIGHT: 0.2,
            MoralJudgment.PROBABLY_RIGHT: 0.1,
            MoralJudgment.UNCLEAR: 0.0,
            MoralJudgment.PROBABLY_WRONG: -0.1,
            MoralJudgment.CLEARLY_WRONG: -0.2,
        }
        base_confidence += judgment_adjustments.get(moral_judgment, 0.0)

        return max(0.0, min(1.0, base_confidence))

    def _generate_reasoning_steps(
        self,
        dilemma: EthicalDilemma,
        reasoning_method: ReasoningMethod,
        principle_weights: Dict[EthicalPrinciple, float],
    ) -> List[str]:
        """ì¶”ë¡  ë‹¨ê³„ ìƒì„±"""
        steps = []

        steps.append(f"1. ë”œë ˆë§ˆ ë³µì¡ì„± ë¶„ì„: {dilemma.complexity.value}")
        steps.append(f"2. ê´€ë ¨ ìœ¤ë¦¬ì  ì›ì¹™ ì‹ë³„: {len(dilemma.involved_principles)}ê°œ")
        steps.append(f"3. ì¶”ë¡  ë°©ë²• ì„ íƒ: {reasoning_method.value}")
        steps.append(f"4. ì›ì¹™ ê°€ì¤‘ì¹˜ ê³„ì‚°: {len(principle_weights)}ê°œ ì›ì¹™")
        steps.append(f"5. ê°€ì¹˜ ì¶©ëŒ í•´ê²°: {len(dilemma.value_conflicts)}ê°œ ì¶©ëŒ")
        steps.append("6. ë„ë•ì  íŒë‹¨ ë„ì¶œ")
        steps.append("7. ê°€ì¡± ì˜í–¥ ë¶„ì„")

        return steps

    def _analyze_family_impact(
        self, dilemma: EthicalDilemma, moral_judgment: MoralJudgment
    ) -> str:
        """ê°€ì¡± ì˜í–¥ ë¶„ì„"""
        if moral_judgment in [
            MoralJudgment.CLEARLY_RIGHT,
            MoralJudgment.PROBABLY_RIGHT,
        ]:
            return "ê°€ì¡± ê´€ê³„ì— ê¸ì •ì  ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤."
        elif moral_judgment == MoralJudgment.UNCLEAR:
            return "ê°€ì¡± ê´€ê³„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤. ì¶”ê°€ ë…¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            return "ê°€ì¡± ê´€ê³„ì— ë¶€ì •ì  ì˜í–¥ì„ ë¯¸ì¹  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. ëŒ€ì•ˆì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤."

    def generate_ethical_recommendation(
        self, analysis: EthicalAnalysis
    ) -> EthicalRecommendation:
        """ìœ¤ë¦¬ì  ê¶Œê³  ìƒì„±"""
        recommendation_id = f"recommendation_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # ê¶Œê³  í–‰ë™ ê²°ì •
        recommended_action = self._determine_recommended_action(analysis)

        # ëŒ€ì•ˆ í–‰ë™ ìƒì„±
        alternative_actions = self._generate_alternative_actions(analysis)

        # ì˜ˆìƒ ê²°ê³¼
        expected_outcomes = self._predict_expected_outcomes(
            analysis, recommended_action
        )

        # ìœ„í—˜ í‰ê°€
        risk_assessment = self._assess_risks(analysis, recommended_action)

        # ê°€ì¡± ê³ ë ¤ì‚¬í•­
        family_considerations = self._identify_family_considerations(analysis)

        # ë„ë•ì  ì •ë‹¹í™”
        moral_justification = self._generate_moral_justification(
            analysis, recommended_action
        )

        # êµ¬í˜„ ë‹¨ê³„
        implementation_steps = self._generate_implementation_steps(recommended_action)

        recommendation = EthicalRecommendation(
            id=recommendation_id,
            analysis_id=analysis.id,
            recommended_action=recommended_action,
            alternative_actions=alternative_actions,
            expected_outcomes=expected_outcomes,
            risk_assessment=risk_assessment,
            family_considerations=family_considerations,
            moral_justification=moral_justification,
            implementation_steps=implementation_steps,
            timestamp=datetime.now(),
        )

        self.ethical_recommendations.append(recommendation)
        logger.info(f"ìœ¤ë¦¬ì  ê¶Œê³  ìƒì„± ì™„ë£Œ: {moral_justification[:50]}...")

        return recommendation

    def _determine_recommended_action(self, analysis: EthicalAnalysis) -> str:
        """ê¶Œê³  í–‰ë™ ê²°ì •"""
        if analysis.moral_judgment == MoralJudgment.CLEARLY_RIGHT:
            return "ì¦‰ì‹œ ì‹¤í–‰ì„ ê¶Œê³ í•©ë‹ˆë‹¤."
        elif analysis.moral_judgment == MoralJudgment.PROBABLY_RIGHT:
            return "ì‹ ì¤‘í•œ ì‹¤í–‰ì„ ê¶Œê³ í•©ë‹ˆë‹¤."
        elif analysis.moral_judgment == MoralJudgment.UNCLEAR:
            return "ì¶”ê°€ ë…¼ì˜ í›„ ê²°ì •ì„ ê¶Œê³ í•©ë‹ˆë‹¤."
        elif analysis.moral_judgment == MoralJudgment.PROBABLY_WRONG:
            return "ëŒ€ì•ˆì„ ê³ ë ¤í•  ê²ƒì„ ê¶Œê³ í•©ë‹ˆë‹¤."
        else:
            return "ì‹¤í–‰ì„ ì¤‘ë‹¨í•  ê²ƒì„ ê¶Œê³ í•©ë‹ˆë‹¤."

    def _generate_alternative_actions(self, analysis: EthicalAnalysis) -> List[str]:
        """ëŒ€ì•ˆ í–‰ë™ ìƒì„±"""
        alternatives = []

        if analysis.moral_judgment in [
            MoralJudgment.UNCLEAR,
            MoralJudgment.PROBABLY_WRONG,
        ]:
            alternatives.append("ê°€ì¡±ê³¼ì˜ ìƒë‹´ì„ í†µí•œ í•©ì˜ ë„ì¶œ")
            alternatives.append("ë‹¨ê³„ì  ì ‘ê·¼ì„ í†µí•œ ì ì§„ì  í•´ê²°")
            alternatives.append("ì „ë¬¸ê°€ ì¡°ì–¸ì„ í†µí•œ ì™¸ë¶€ ì˜ê²¬ ìˆ˜ë ´")

        return alternatives

    def _predict_expected_outcomes(
        self, analysis: EthicalAnalysis, action: str
    ) -> List[str]:
        """ì˜ˆìƒ ê²°ê³¼ ì˜ˆì¸¡"""
        outcomes = []

        if analysis.moral_judgment in [
            MoralJudgment.CLEARLY_RIGHT,
            MoralJudgment.PROBABLY_RIGHT,
        ]:
            outcomes.append("ê°€ì¡± ê´€ê³„ì˜ ê°•í™”")
            outcomes.append("ë„ë•ì  ì„±ì¥ì˜ ì´‰ì§„")
            outcomes.append("ì‹ ë¢° ê´€ê³„ì˜ êµ¬ì¶•")
        else:
            outcomes.append("ê°€ì¡± ê°„ ê°ˆë“±ì˜ ê°€ëŠ¥ì„±")
            outcomes.append("ì‹ ë¢° ê´€ê³„ì˜ í›¼ì† ìœ„í—˜")
            outcomes.append("ë„ë•ì  í˜¼ë€ì˜ ì•¼ê¸°")

        return outcomes

    def _assess_risks(self, analysis: EthicalAnalysis, action: str) -> Dict[str, float]:
        """ìœ„í—˜ í‰ê°€"""
        risks = {}

        if analysis.moral_judgment == MoralJudgment.CLEARLY_WRONG:
            risks["ê°€ì¡± ê´€ê³„ í›¼ì†"] = 0.9
            risks["ë„ë•ì  í˜¼ë€"] = 0.8
            risks["ì‹ ë¢° ìƒì‹¤"] = 0.7
        elif analysis.moral_judgment == MoralJudgment.PROBABLY_WRONG:
            risks["ê°€ì¡± ê´€ê³„ ì•…í™”"] = 0.6
            risks["ê°ˆë“± ë°œìƒ"] = 0.5
        else:
            risks["ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼"] = 0.3
            risks["ê°€ì¡± ê°„ ì˜ê²¬ ì°¨ì´"] = 0.2

        return risks

    def _identify_family_considerations(self, analysis: EthicalAnalysis) -> List[str]:
        """ê°€ì¡± ê³ ë ¤ì‚¬í•­ ì‹ë³„"""
        considerations = []

        if analysis.family_impact.startswith("ê°€ì¡± ê´€ê³„ì— ê¸ì •ì "):
            considerations.append("ê°€ì¡± êµ¬ì„±ì›ë“¤ì˜ ì˜ê²¬ ìˆ˜ë ´")
            considerations.append("ë‹¨ê³„ì  ì‹¤í–‰ì„ í†µí•œ ì•ˆì •ì„± í™•ë³´")
        elif analysis.family_impact.startswith("ê°€ì¡± ê´€ê³„ì— ë¶€ì •ì "):
            considerations.append("ê°€ì¡± êµ¬ì„±ì›ë“¤ê³¼ì˜ ì¶©ë¶„í•œ ì†Œí†µ")
            considerations.append("ëŒ€ì•ˆì  í•´ê²°ì±… ëª¨ìƒ‰")
        else:
            considerations.append("ê°€ì¡± êµ¬ì„±ì›ë“¤ê³¼ì˜ ê³µë™ ë…¼ì˜")
            considerations.append("ì‹ ì¤‘í•œ ì ‘ê·¼ í•„ìš”")

        return considerations

    def _generate_moral_justification(
        self, analysis: EthicalAnalysis, action: str
    ) -> str:
        """ë„ë•ì  ì •ë‹¹í™” ìƒì„±"""
        if analysis.moral_judgment == MoralJudgment.CLEARLY_RIGHT:
            return f"{analysis.reasoning_method.value} ê´€ì ì—ì„œ {action}ì´ ê°€ì¥ ë„ë•ì ìœ¼ë¡œ ì •ë‹¹í•©ë‹ˆë‹¤."
        elif analysis.moral_judgment == MoralJudgment.PROBABLY_RIGHT:
            return f"{analysis.reasoning_method.value} ê´€ì ì—ì„œ {action}ì´ ëŒ€ì²´ë¡œ ë„ë•ì ìœ¼ë¡œ ì •ë‹¹í•©ë‹ˆë‹¤."
        else:
            return f"{analysis.reasoning_method.value} ê´€ì ì—ì„œ {action}ì˜ ë„ë•ì  ì •ë‹¹ì„±ì´ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤."

    def _generate_implementation_steps(self, action: str) -> List[str]:
        """êµ¬í˜„ ë‹¨ê³„ ìƒì„±"""
        steps = []

        if "ì¦‰ì‹œ ì‹¤í–‰" in action:
            steps.extend(
                [
                    "1. ê°€ì¡± êµ¬ì„±ì›ë“¤ì—ê²Œ ìƒí™© ì„¤ëª…",
                    "2. í•©ì˜ëœ ë°©í–¥ìœ¼ë¡œ ì‹¤í–‰",
                    "3. ê²°ê³¼ ëª¨ë‹ˆí„°ë§",
                ]
            )
        elif "ì‹ ì¤‘í•œ ì‹¤í–‰" in action:
            steps.extend(
                [
                    "1. ê°€ì¡± êµ¬ì„±ì›ë“¤ê³¼ ìƒë‹´",
                    "2. ë‹¨ê³„ì  ì‹¤í–‰ ê³„íš ìˆ˜ë¦½",
                    "3. ê° ë‹¨ê³„ë³„ í‰ê°€",
                ]
            )
        elif "ì¶”ê°€ ë…¼ì˜" in action:
            steps.extend(["1. ê°€ì¡± íšŒì˜ ì†Œì§‘", "2. ë‹¤ì–‘í•œ ê´€ì  ë…¼ì˜", "3. í•©ì˜ ë„ì¶œ"])
        else:
            steps.extend(
                ["1. í˜„ì¬ ìƒí™© ì¬ê²€í† ", "2. ëŒ€ì•ˆ ëª¨ìƒ‰", "3. ìƒˆë¡œìš´ ì ‘ê·¼ë²• ë„ì¶œ"]
            )

        return steps

    def get_ethical_statistics(self) -> Dict[str, Any]:
        """ìœ¤ë¦¬ì  í†µê³„"""
        total_dilemmas = len(self.ethical_dilemmas)
        total_analyses = len(self.ethical_analyses)
        total_recommendations = len(self.ethical_recommendations)

        # ë³µì¡ì„±ë³„ í†µê³„
        complexity_stats = {}
        for complexity in DilemmaComplexity:
            complexity_count = sum(
                1 for d in self.ethical_dilemmas if d.complexity == complexity
            )
            complexity_stats[complexity.value] = complexity_count

        # íŒë‹¨ë³„ í†µê³„
        judgment_stats = {}
        for judgment in MoralJudgment:
            judgment_count = sum(
                1 for a in self.ethical_analyses if a.moral_judgment == judgment
            )
            judgment_stats[judgment.value] = judgment_count

        # ì¶”ë¡  ë°©ë²•ë³„ í†µê³„
        method_stats = {}
        for method in ReasoningMethod:
            method_count = sum(
                1 for a in self.ethical_analyses if a.reasoning_method == method
            )
            method_stats[method.value] = method_count

        # í‰ê·  ì‹ ë¢°ë„
        avg_confidence = sum(a.confidence_score for a in self.ethical_analyses) / max(
            1, total_analyses
        )

        statistics = {
            "total_dilemmas": total_dilemmas,
            "total_analyses": total_analyses,
            "total_recommendations": total_recommendations,
            "complexity_statistics": complexity_stats,
            "judgment_statistics": judgment_stats,
            "method_statistics": method_stats,
            "average_confidence": avg_confidence,
            "last_updated": datetime.now().isoformat(),
        }

        logger.info("ìœ¤ë¦¬ì  í†µê³„ ìƒì„± ì™„ë£Œ")
        return statistics

    def export_ethical_data(self) -> Dict[str, Any]:
        """ìœ¤ë¦¬ì  ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        return {
            "ethical_dilemmas": [asdict(d) for d in self.ethical_dilemmas],
            "ethical_analyses": [asdict(a) for a in self.ethical_analyses],
            "ethical_recommendations": [
                asdict(r) for r in self.ethical_recommendations
            ],
            "family_values": self.family_values,
            "moral_development": self.moral_development,
            "export_date": datetime.now().isoformat(),
        }


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_advanced_ethical_reasoning_system():
    """ê³ ê¸‰ ìœ¤ë¦¬ì  ì¶”ë¡  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§  AdvancedEthicalReasoningSystem í…ŒìŠ¤íŠ¸ ì‹œì‘...")

    ethical_system = AdvancedEthicalReasoningSystem()

    # 1. ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„
    dilemma_description = "ì•„ì´ê°€ ìˆ™ì œë¥¼ í•˜ì§€ ì•Šì•„ì„œ ê±°ì§“ë§ì„ í–ˆëŠ”ë°, ì§„ì‹¤ì„ ë§í•˜ë©´ ì•„ì´ê°€ ìƒì²˜ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    family_context = {
        "has_children": True,
        "family_size": 4,
        "communication_style": "open",
    }
    stakeholders = ["ì•„ì´", "ë¶€ëª¨", "ê°€ì¡± ì „ì²´"]
    potential_outcomes = [
        "ì§„ì‹¤ì„ ë§í•´ì„œ ì•„ì´ê°€ ìƒì²˜ë°›ìŒ",
        "ê±°ì§“ë§ì„ ìœ ì§€í•´ì„œ ì‹ ë¢° ê´€ê³„ í›¼ì†",
        "ëŒ€í™”ë¥¼ í†µí•œ ì´í•´ì™€ ì„±ì¥",
    ]

    dilemma = ethical_system.analyze_ethical_dilemma(
        dilemma_description, family_context, stakeholders, potential_outcomes
    )

    print(f"âœ… ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„: {dilemma.complexity.value}")
    print(f"   ê´€ë ¨ ì›ì¹™: {len(dilemma.involved_principles)}ê°œ")
    print(f"   ê°€ì¹˜ ì¶©ëŒ: {len(dilemma.value_conflicts)}ê°œ")

    # 2. ìœ¤ë¦¬ì  ì¶”ë¡  ìˆ˜í–‰
    analysis = ethical_system.conduct_ethical_reasoning(dilemma)

    print(f"âœ… ìœ¤ë¦¬ì  ì¶”ë¡  ì™„ë£Œ: {analysis.reasoning_method.value}")
    print(f"   ë„ë•ì  íŒë‹¨: {analysis.moral_judgment.value}")
    print(f"   ì‹ ë¢°ë„: {analysis.confidence_score:.2f}")
    print(f"   ì¶”ë¡  ë‹¨ê³„: {len(analysis.reasoning_steps)}ê°œ")
    print(f"   ê°€ì¡± ì˜í–¥: {analysis.family_impact}")

    # 3. ìœ¤ë¦¬ì  ê¶Œê³  ìƒì„±
    recommendation = ethical_system.generate_ethical_recommendation(analysis)

    print(f"âœ… ìœ¤ë¦¬ì  ê¶Œê³  ìƒì„±: {recommendation.recommended_action}")
    print(f"   ëŒ€ì•ˆ í–‰ë™: {len(recommendation.alternative_actions)}ê°œ")
    print(f"   ì˜ˆìƒ ê²°ê³¼: {len(recommendation.expected_outcomes)}ê°œ")
    print(f"   ìœ„í—˜ í‰ê°€: {len(recommendation.risk_assessment)}ê°œ")
    print(f"   ê°€ì¡± ê³ ë ¤ì‚¬í•­: {len(recommendation.family_considerations)}ê°œ")
    print(f"   êµ¬í˜„ ë‹¨ê³„: {len(recommendation.implementation_steps)}ê°œ")

    # 4. í†µê³„
    statistics = ethical_system.get_ethical_statistics()
    print(f"âœ… ìœ¤ë¦¬ì  í†µê³„: {statistics['total_dilemmas']}ê°œ ë”œë ˆë§ˆ")
    print(f"   í‰ê·  ì‹ ë¢°ë„: {statistics['average_confidence']:.2f}")
    print(f"   ë³µì¡ì„±ë³„ í†µê³„: {statistics['complexity_statistics']}")
    print(f"   íŒë‹¨ë³„ í†µê³„: {statistics['judgment_statistics']}")
    print(f"   ë°©ë²•ë³„ í†µê³„: {statistics['method_statistics']}")

    # 5. ë°ì´í„° ë‚´ë³´ë‚´ê¸°
    export_data = ethical_system.export_ethical_data()
    print(f"âœ… ìœ¤ë¦¬ì  ë°ì´í„° ë‚´ë³´ë‚´ê¸°: {len(export_data['ethical_dilemmas'])}ê°œ ë”œë ˆë§ˆ")

    print("ğŸ‰ AdvancedEthicalReasoningSystem í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    test_advanced_ethical_reasoning_system()
