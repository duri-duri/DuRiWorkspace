#!/usr/bin/env python3
"""
EthicalConversationSystem - Phase 12.1
ìœ¤ë¦¬ì  ëŒ€í™” ì‹œìŠ¤í…œ

ëª©ì :
- ê°€ì¡± ì¤‘ì‹¬ì˜ ìœ¤ë¦¬ì  íŒë‹¨ê³¼ ëŒ€í™”
- ë„ë•ì  ë”œë ˆë§ˆ í•´ê²°
- ê°€ì¡± ê°€ì¹˜ ê¸°ë°˜ì˜ ìœ¤ë¦¬ì  ì¡°ì–¸
"""

import json
import logging
from dataclasses import asdict, dataclass
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EthicalPrinciple(Enum):
    """ìœ¤ë¦¬ ì›ì¹™"""

    FAMILY_HARMONY = "family_harmony"
    LOVE_AND_CARE = "love_and_care"
    HONESTY = "honesty"
    RESPECT = "respect"
    RESPONSIBILITY = "responsibility"
    FAIRNESS = "fairness"
    GROWTH = "growth"


class DilemmaType(Enum):
    """ë”œë ˆë§ˆ ìœ í˜•"""

    FAMILY_CONFLICT = "family_conflict"
    HONESTY_VS_HARM = "honesty_vs_harm"
    INDIVIDUAL_VS_FAMILY = "individual_vs_family"
    GROWTH_VS_STABILITY = "growth_vs_stability"
    TRADITION_VS_CHANGE = "tradition_vs_change"


class EthicalJudgment(Enum):
    """ìœ¤ë¦¬ì  íŒë‹¨"""

    CLEARLY_ETHICAL = "clearly_ethical"
    ETHICAL_WITH_CONDITIONS = "ethical_with_conditions"
    ETHICAL_DILEMMA = "ethical_dilemma"
    POTENTIALLY_UNETHICAL = "potentially_unethical"
    CLEARLY_UNETHICAL = "clearly_unethical"


@dataclass
class EthicalAnalysis:
    """ìœ¤ë¦¬ì  ë¶„ì„"""

    id: str
    dilemma_description: str
    dilemma_type: DilemmaType
    involved_principles: List[EthicalPrinciple]
    ethical_judgment: EthicalJudgment
    reasoning: str
    recommended_action: str
    alternative_actions: List[str]
    family_impact: str
    confidence_score: float
    timestamp: datetime


@dataclass
class EthicalConversation:
    """ìœ¤ë¦¬ì  ëŒ€í™”"""

    id: str
    conversation_topic: str
    family_context: Dict[str, Any]
    ethical_analysis: EthicalAnalysis
    conversation_flow: List[str]
    emotional_support: str
    guidance_provided: str
    follow_up_actions: List[str]
    timestamp: datetime


class EthicalConversationSystem:
    """ìœ¤ë¦¬ì  ëŒ€í™” ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.ethical_analyses: List[EthicalAnalysis] = []
        self.ethical_conversations: List[EthicalConversation] = []
        self.family_values: List[str] = ["ì‚¬ë‘", "ì†Œí†µ", "ì„±ì¥", "ì°½ì˜ì„±", "ì¡°í™”"]

        logger.info("EthicalConversationSystem ì´ˆê¸°í™” ì™„ë£Œ")

    def analyze_ethical_dilemma(
        self, dilemma_description: str, family_context: Dict[str, Any]
    ) -> EthicalAnalysis:
        """ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„"""
        analysis_id = f"ethical_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # ë”œë ˆë§ˆ ìœ í˜• íŒë‹¨
        dilemma_type = self._determine_dilemma_type(dilemma_description)

        # ê´€ë ¨ ìœ¤ë¦¬ ì›ì¹™ ì‹ë³„
        involved_principles = self._identify_involved_principles(
            dilemma_description, family_context
        )

        # ìœ¤ë¦¬ì  íŒë‹¨
        ethical_judgment = self._make_ethical_judgment(
            dilemma_description, involved_principles, family_context
        )

        # ì¶”ë¡  ê³¼ì •
        reasoning = self._generate_ethical_reasoning(
            dilemma_description, involved_principles, family_context
        )

        # ê¶Œì¥ í–‰ë™
        recommended_action = self._generate_recommended_action(
            dilemma_description, ethical_judgment, family_context
        )

        # ëŒ€ì•ˆ í–‰ë™ë“¤
        alternative_actions = self._generate_alternative_actions(
            dilemma_description, ethical_judgment, family_context
        )

        # ê°€ì¡± ì˜í–¥
        family_impact = self._assess_family_impact(recommended_action, family_context)

        # ì‹ ë¢°ë„ ì ìˆ˜
        confidence_score = self._calculate_confidence_score(
            ethical_judgment, involved_principles, family_context
        )

        analysis = EthicalAnalysis(
            id=analysis_id,
            dilemma_description=dilemma_description,
            dilemma_type=dilemma_type,
            involved_principles=involved_principles,
            ethical_judgment=ethical_judgment,
            reasoning=reasoning,
            recommended_action=recommended_action,
            alternative_actions=alternative_actions,
            family_impact=family_impact,
            confidence_score=confidence_score,
            timestamp=datetime.now(),
        )

        self.ethical_analyses.append(analysis)
        logger.info(f"ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„ ì™„ë£Œ: {ethical_judgment.value}")

        return analysis

    def _determine_dilemma_type(self, dilemma_description: str) -> DilemmaType:
        """ë”œë ˆë§ˆ ìœ í˜• íŒë‹¨"""
        description_lower = dilemma_description.lower()

        if any(
            word in description_lower
            for word in ["ê°€ì¡±", "ë¶€ëª¨", "ìì‹", "í˜•ì œ", "ê°ˆë“±"]
        ):
            return DilemmaType.FAMILY_CONFLICT
        elif any(
            word in description_lower for word in ["ê±°ì§“ë§", "ì§„ì‹¤", "ë§í•˜ë‹¤", "ìˆ¨ê¸°ë‹¤"]
        ):
            return DilemmaType.HONESTY_VS_HARM
        elif any(
            word in description_lower for word in ["ê°œì¸", "ìì‹ ", "ê°€ì¡±", "ì´ê¸°ì "]
        ):
            return DilemmaType.INDIVIDUAL_VS_FAMILY
        elif any(
            word in description_lower for word in ["ì„±ì¥", "ë³€í™”", "ì•ˆì •", "í˜„ìƒìœ ì§€"]
        ):
            return DilemmaType.GROWTH_VS_STABILITY
        elif any(
            word in description_lower for word in ["ì „í†µ", "ìƒˆë¡œìš´", "ë³€í™”", "ìŠµê´€"]
        ):
            return DilemmaType.TRADITION_VS_CHANGE
        else:
            return DilemmaType.FAMILY_CONFLICT  # ê¸°ë³¸ê°’

    def _identify_involved_principles(
        self, dilemma_description: str, family_context: Dict[str, Any]
    ) -> List[EthicalPrinciple]:
        """ê´€ë ¨ ìœ¤ë¦¬ ì›ì¹™ ì‹ë³„"""
        principles = []
        description_lower = dilemma_description.lower()

        # ê°€ì¡± ì¡°í™”
        if any(word in description_lower for word in ["ê°€ì¡±", "ì¡°í™”", "í™”í•©", "í‰í™”"]):
            principles.append(EthicalPrinciple.FAMILY_HARMONY)

        # ì‚¬ë‘ê³¼ ëŒë´„
        if any(word in description_lower for word in ["ì‚¬ë‘", "ëŒë´„", "ê´€ì‹¬", "ë°°ë ¤"]):
            principles.append(EthicalPrinciple.LOVE_AND_CARE)

        # ì •ì§
        if any(
            word in description_lower for word in ["ì •ì§", "ì§„ì‹¤", "ê±°ì§“ë§", "ì†”ì§"]
        ):
            principles.append(EthicalPrinciple.HONESTY)

        # ì¡´ì¤‘
        if any(word in description_lower for word in ["ì¡´ì¤‘", "ì¸ì •", "ì´í•´", "ìˆ˜ìš©"]):
            principles.append(EthicalPrinciple.RESPECT)

        # ì±…ì„
        if any(word in description_lower for word in ["ì±…ì„", "ì˜ë¬´", "ì•½ì†", "ë§¡ë‹¤"]):
            principles.append(EthicalPrinciple.RESPONSIBILITY)

        # ê³µì •ì„±
        if any(
            word in description_lower for word in ["ê³µì •", "í‰ë“±", "ì°¨ë³„", "ë¶ˆê³µì •"]
        ):
            principles.append(EthicalPrinciple.FAIRNESS)

        # ì„±ì¥
        if any(word in description_lower for word in ["ì„±ì¥", "ë°œì „", "í•™ìŠµ", "ë³€í™”"]):
            principles.append(EthicalPrinciple.GROWTH)

        # ê¸°ë³¸ ì›ì¹™ ì¶”ê°€
        if not principles:
            principles.extend(
                [EthicalPrinciple.FAMILY_HARMONY, EthicalPrinciple.LOVE_AND_CARE]
            )

        return list(set(principles))  # ì¤‘ë³µ ì œê±°

    def _make_ethical_judgment(
        self,
        dilemma_description: str,
        principles: List[EthicalPrinciple],
        family_context: Dict[str, Any],
    ) -> EthicalJudgment:
        """ìœ¤ë¦¬ì  íŒë‹¨"""
        # ê°€ì¡± ì¡°í™”ì™€ ì‚¬ë‘ì´ ìš°ì„ ì¸ ê²½ìš°
        if (
            EthicalPrinciple.FAMILY_HARMONY in principles
            and EthicalPrinciple.LOVE_AND_CARE in principles
        ):
            return EthicalJudgment.CLEARLY_ETHICAL

        # ì •ì§ê³¼ ê°€ì¡± ì¡°í™”ê°€ ì¶©ëŒí•˜ëŠ” ê²½ìš°
        if (
            EthicalPrinciple.HONESTY in principles
            and EthicalPrinciple.FAMILY_HARMONY in principles
        ):
            return EthicalJudgment.ETHICAL_DILEMMA

        # ê°œì¸ê³¼ ê°€ì¡±ì´ ì¶©ëŒí•˜ëŠ” ê²½ìš°
        if (
            EthicalPrinciple.RESPECT in principles
            and EthicalPrinciple.FAMILY_HARMONY in principles
        ):
            return EthicalJudgment.ETHICAL_WITH_CONDITIONS

        # ì„±ì¥ê³¼ ì•ˆì •ì´ ì¶©ëŒí•˜ëŠ” ê²½ìš°
        if (
            EthicalPrinciple.GROWTH in principles
            and EthicalPrinciple.FAMILY_HARMONY in principles
        ):
            return EthicalJudgment.ETHICAL_WITH_CONDITIONS

        # ê¸°ë³¸ì ìœ¼ë¡œ ê°€ì¡± ì¤‘ì‹¬
        return EthicalJudgment.CLEARLY_ETHICAL

    def _generate_ethical_reasoning(
        self,
        dilemma_description: str,
        principles: List[EthicalPrinciple],
        family_context: Dict[str, Any],
    ) -> str:
        """ìœ¤ë¦¬ì  ì¶”ë¡  ìƒì„±"""
        reasoning = f"ì´ ìƒí™©ì„ ë¶„ì„í•´ë³´ë©´, {', '.join([p.value for p in principles])} ì›ì¹™ì´ ê´€ë ¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. "

        if EthicalPrinciple.FAMILY_HARMONY in principles:
            reasoning += "ê°€ì¡±ì˜ ì¡°í™”ì™€ í‰í™”ê°€ ìµœìš°ì„ ì´ì–´ì•¼ í•©ë‹ˆë‹¤. "

        if EthicalPrinciple.LOVE_AND_CARE in principles:
            reasoning += "ì„œë¡œë¥¼ ì‚¬ë‘í•˜ê³  ëŒë³´ëŠ” ë§ˆìŒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. "

        if EthicalPrinciple.HONESTY in principles:
            reasoning += "ì •ì§í•¨ì€ ê°€ì¡± ê°„ì˜ ì‹ ë¢°ë¥¼ ìœ„í•œ ê¸°ë°˜ì…ë‹ˆë‹¤. "

        reasoning += f"ê°€ì¡±ì˜ ê°€ì¹˜ì¸ {', '.join(self.family_values)}ì„ ê³ ë ¤í•  ë•Œ, "
        reasoning += (
            "ëª¨ë“  êµ¬ì„±ì›ì´ í•¨ê»˜ ì„±ì¥í•  ìˆ˜ ìˆëŠ” ë°©í–¥ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¢‹ê² ìŠµë‹ˆë‹¤."
        )

        return reasoning

    def _generate_recommended_action(
        self,
        dilemma_description: str,
        judgment: EthicalJudgment,
        family_context: Dict[str, Any],
    ) -> str:
        """ê¶Œì¥ í–‰ë™ ìƒì„±"""
        if judgment == EthicalJudgment.CLEARLY_ETHICAL:
            return "ê°€ì¡±ê³¼ í•¨ê»˜ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ê³ , ì„œë¡œì˜ ë§ˆìŒì„ ì´í•´í•˜ë ¤ê³  ë…¸ë ¥í•˜ì„¸ìš”."
        elif judgment == EthicalJudgment.ETHICAL_WITH_CONDITIONS:
            return "ê°€ì¡±ì˜ ì¡°í™”ë¥¼ ìœ ì§€í•˜ë©´ì„œë„ ê°œì¸ì˜ ì„±ì¥ì„ ê³ ë ¤í•œ ê· í˜•ì¡íŒ ì ‘ê·¼ì„ ì‹œë„í•´ë³´ì„¸ìš”."
        elif judgment == EthicalJudgment.ETHICAL_DILEMMA:
            return "ì´ ìƒí™©ì€ ë³µì¡í•˜ë¯€ë¡œ, ê°€ì¡±ê³¼ í•¨ê»˜ ì¶©ë¶„í•œ ëŒ€í™”ë¥¼ í†µí•´ ìµœì„ ì˜ í•´ê²°ì±…ì„ ì°¾ì•„ë³´ì„¸ìš”."
        else:
            return "ê°€ì¡±ì˜ ê°€ì¹˜ë¥¼ ìš°ì„ ìœ¼ë¡œ í•˜ë˜, ëª¨ë“  êµ¬ì„±ì›ì˜ ì˜ê²¬ì„ ê²½ì²­í•˜ëŠ” ìì„¸ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤."

    def _generate_alternative_actions(
        self,
        dilemma_description: str,
        judgment: EthicalJudgment,
        family_context: Dict[str, Any],
    ) -> List[str]:
        """ëŒ€ì•ˆ í–‰ë™ë“¤ ìƒì„±"""
        alternatives = []

        if judgment == EthicalJudgment.ETHICAL_DILEMMA:
            alternatives.extend(
                [
                    "ê°€ì¡± íšŒì˜ë¥¼ í†µí•´ ëª¨ë“  êµ¬ì„±ì›ì˜ ì˜ê²¬ì„ ë“£ê¸°",
                    "ë‹¨ê³„ì ìœ¼ë¡œ ë³€í™”ë¥¼ ì‹œë„í•´ë³´ê¸°",
                    "ì „ë¬¸ê°€ì˜ ì¡°ì–¸ì„ êµ¬í•˜ê¸°",
                ]
            )
        else:
            alternatives.extend(
                [
                    "ì„œë¡œì˜ ì…ì¥ì„ ë°”ê¿”ì„œ ìƒê°í•´ë³´ê¸°",
                    "ì‹œê°„ì„ ë‘ê³  ì²œì²œíˆ ê²°ì •í•˜ê¸°",
                    "ê°€ì¡±ì˜ ê°€ì¹˜ë¥¼ ë‹¤ì‹œ í•œë²ˆ í™•ì¸í•˜ê¸°",
                ]
            )

        return alternatives

    def _assess_family_impact(
        self, recommended_action: str, family_context: Dict[str, Any]
    ) -> str:
        """ê°€ì¡± ì˜í–¥ í‰ê°€"""
        return "ì´ í–‰ë™ì€ ê°€ì¡± ê°„ì˜ ì†Œí†µì„ ì¦ì§„ì‹œí‚¤ê³ , ì„œë¡œì— ëŒ€í•œ ì´í•´ë¥¼ ê¹Šê²Œ í•  ê²ƒì…ë‹ˆë‹¤. ë‹¨ê¸°ì ìœ¼ë¡œëŠ” ì–´ë ¤ì›€ì´ ìˆì„ ìˆ˜ ìˆì§€ë§Œ, ì¥ê¸°ì ìœ¼ë¡œëŠ” ê°€ì¡±ì˜ ìœ ëŒ€ê°ì„ ê°•í™”í•  ê²ƒì…ë‹ˆë‹¤."

    def _calculate_confidence_score(
        self,
        judgment: EthicalJudgment,
        principles: List[EthicalPrinciple],
        family_context: Dict[str, Any],
    ) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        base_score = 0.8

        # íŒë‹¨ ìœ í˜•ì— ë”°ë¥¸ ì¡°ì •
        if judgment == EthicalJudgment.CLEARLY_ETHICAL:
            base_score += 0.1
        elif judgment == EthicalJudgment.ETHICAL_DILEMMA:
            base_score -= 0.1

        # ì›ì¹™ ìˆ˜ì— ë”°ë¥¸ ì¡°ì •
        if len(principles) <= 2:
            base_score += 0.05
        else:
            base_score -= 0.05

        return min(1.0, max(0.0, base_score))

    def conduct_ethical_conversation(
        self, topic: str, family_context: Dict[str, Any]
    ) -> EthicalConversation:
        """ìœ¤ë¦¬ì  ëŒ€í™” ìˆ˜í–‰"""
        conversation_id = (
            f"ethical_conversation_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )

        # ìœ¤ë¦¬ì  ë¶„ì„
        analysis = self.analyze_ethical_dilemma(topic, family_context)

        # ëŒ€í™” íë¦„ ìƒì„±
        conversation_flow = self._generate_conversation_flow(
            topic, analysis, family_context
        )

        # ê°ì •ì  ì§€ì›
        emotional_support = self._generate_emotional_support(analysis, family_context)

        # ì§€ë„ ì œê³µ
        guidance_provided = self._generate_guidance(analysis, family_context)

        # í›„ì† í–‰ë™
        follow_up_actions = self._generate_follow_up_actions(analysis, family_context)

        conversation = EthicalConversation(
            id=conversation_id,
            conversation_topic=topic,
            family_context=family_context,
            ethical_analysis=analysis,
            conversation_flow=conversation_flow,
            emotional_support=emotional_support,
            guidance_provided=guidance_provided,
            follow_up_actions=follow_up_actions,
            timestamp=datetime.now(),
        )

        self.ethical_conversations.append(conversation)
        logger.info(f"ìœ¤ë¦¬ì  ëŒ€í™” ì™„ë£Œ: {topic}")

        return conversation

    def _generate_conversation_flow(
        self, topic: str, analysis: EthicalAnalysis, family_context: Dict[str, Any]
    ) -> List[str]:
        """ëŒ€í™” íë¦„ ìƒì„±"""
        flow = [
            f"'{topic}'ì— ëŒ€í•´ í•¨ê»˜ ìƒê°í•´ë³´ê² ìŠµë‹ˆë‹¤.",
            f"ì´ ìƒí™©ì—ì„œ {', '.join([p.value for p in analysis.involved_principles])} ì›ì¹™ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.",
            analysis.reasoning,
            f"ê¶Œì¥í•˜ëŠ” í–‰ë™ì€: {analysis.recommended_action}",
            "ê°€ì¡±ê³¼ í•¨ê»˜ ì´ ë¬¸ì œë¥¼ í•´ê²°í•´ë‚˜ê°€ì‹œê¸¸ ë°”ëë‹ˆë‹¤.",
        ]
        return flow

    def _generate_emotional_support(
        self, analysis: EthicalAnalysis, family_context: Dict[str, Any]
    ) -> str:
        """ê°ì •ì  ì§€ì› ìƒì„±"""
        return "ì´ëŸ° ìƒí™©ì—ì„œ í˜¼ë€ìŠ¤ëŸ½ê³  ì–´ë ¤ìš´ ë§ˆìŒì´ ë“œì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê°€ì¡±ê³¼ í•¨ê»˜ë¼ë©´ ì–´ë–¤ ì–´ë ¤ì›€ë„ ì´ê²¨ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„œë¡œë¥¼ ë¯¿ê³  ì˜ì§€í•˜ëŠ” ë§ˆìŒì´ ì¤‘ìš”í•©ë‹ˆë‹¤."

    def _generate_guidance(
        self, analysis: EthicalAnalysis, family_context: Dict[str, Any]
    ) -> str:
        """ì§€ë„ ìƒì„±"""
        return f"ê°€ì¡±ì˜ ê°€ì¹˜ì¸ {', '.join(self.family_values)}ì„ ê¸°ì–µí•˜ì„¸ìš”. ì„œë¡œë¥¼ ì‚¬ë‘í•˜ê³  ì´í•´í•˜ëŠ” ë§ˆìŒìœ¼ë¡œ ì ‘ê·¼í•˜ë©´ ì¢‹ì€ í•´ê²°ì±…ì„ ì°¾ì„ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤."

    def _generate_follow_up_actions(
        self, analysis: EthicalAnalysis, family_context: Dict[str, Any]
    ) -> List[str]:
        """í›„ì† í–‰ë™ ìƒì„±"""
        actions = [
            "ê°€ì¡±ê³¼ ì •ê¸°ì ì¸ ëŒ€í™” ì‹œê°„ì„ ê°€ì§€ê¸°",
            "ì„œë¡œì˜ ê°ì •ì„ í‘œí˜„í•˜ëŠ” ì—°ìŠµí•˜ê¸°",
            "ê°€ì¡±ì˜ ê°€ì¹˜ë¥¼ ì •ê¸°ì ìœ¼ë¡œ í™•ì¸í•˜ê¸°",
        ]

        if analysis.ethical_judgment == EthicalJudgment.ETHICAL_DILEMMA:
            actions.append("í•„ìš”ì‹œ ì „ë¬¸ê°€ì˜ ë„ì›€ì„ êµ¬í•˜ê¸°")

        return actions

    def get_ethical_statistics(self) -> Dict[str, Any]:
        """ìœ¤ë¦¬ì  ëŒ€í™” í†µê³„"""
        total_analyses = len(self.ethical_analyses)
        total_conversations = len(self.ethical_conversations)

        # íŒë‹¨ ìœ í˜•ë³„ í†µê³„
        judgment_stats = {}
        for judgment in EthicalJudgment:
            judgment_analyses = [
                a for a in self.ethical_analyses if a.ethical_judgment == judgment
            ]
            judgment_stats[judgment.value] = len(judgment_analyses)

        # ë”œë ˆë§ˆ ìœ í˜•ë³„ í†µê³„
        dilemma_stats = {}
        for dilemma_type in DilemmaType:
            type_analyses = [
                a for a in self.ethical_analyses if a.dilemma_type == dilemma_type
            ]
            dilemma_stats[dilemma_type.value] = len(type_analyses)

        statistics = {
            "total_analyses": total_analyses,
            "total_conversations": total_conversations,
            "judgment_statistics": judgment_stats,
            "dilemma_statistics": dilemma_stats,
            "average_confidence": sum(a.confidence_score for a in self.ethical_analyses)
            / max(1, total_analyses),
            "last_updated": datetime.now().isoformat(),
        }

        logger.info("ìœ¤ë¦¬ì  ëŒ€í™” í†µê³„ ìƒì„± ì™„ë£Œ")
        return statistics

    def export_ethical_data(self) -> Dict[str, Any]:
        """ìœ¤ë¦¬ì  ëŒ€í™” ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        return {
            "ethical_analyses": [asdict(a) for a in self.ethical_analyses],
            "ethical_conversations": [asdict(c) for c in self.ethical_conversations],
            "family_values": self.family_values,
            "export_date": datetime.now().isoformat(),
        }


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_ethical_conversation_system():
    """ìœ¤ë¦¬ì  ëŒ€í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("âš–ï¸ EthicalConversationSystem í…ŒìŠ¤íŠ¸ ì‹œì‘...")

    ethical_system = EthicalConversationSystem()

    # 1. ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„
    family_context = {
        "family_type": "nuclear",
        "children_count": 2,
        "children_ages": [5, 8],
        "family_values": ["ì‚¬ë‘", "ì†Œí†µ", "ì„±ì¥", "ì°½ì˜ì„±"],
    }

    dilemma = "ì•„ì´ê°€ ê±°ì§“ë§ì„ í–ˆëŠ”ë°, ì§„ì‹¤ì„ ë§í•˜ë©´ ìƒì²˜ë°›ì„ ê²ƒ ê°™ì•„ìš”."
    analysis = ethical_system.analyze_ethical_dilemma(dilemma, family_context)

    print(f"âœ… ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„: {analysis.ethical_judgment.value}")
    print(f"   ë”œë ˆë§ˆ ìœ í˜•: {analysis.dilemma_type.value}")
    print(f"   ê´€ë ¨ ì›ì¹™: {[p.value for p in analysis.involved_principles]}")
    print(f"   ì‹ ë¢°ë„: {analysis.confidence_score:.2f}")
    print(f"   ê¶Œì¥ í–‰ë™: {analysis.recommended_action}")

    # 2. ìœ¤ë¦¬ì  ëŒ€í™” ìˆ˜í–‰
    conversation = ethical_system.conduct_ethical_conversation(dilemma, family_context)

    print(f"âœ… ìœ¤ë¦¬ì  ëŒ€í™” ì™„ë£Œ: {len(conversation.conversation_flow)}ê°œ ëŒ€í™” ë‹¨ê³„")
    print(f"   ê°ì •ì  ì§€ì›: {conversation.emotional_support}")
    print(f"   ì§€ë„: {conversation.guidance_provided}")
    print(f"   í›„ì† í–‰ë™: {len(conversation.follow_up_actions)}ê°œ")

    # 3. í†µê³„
    statistics = ethical_system.get_ethical_statistics()
    print(
        f"âœ… ìœ¤ë¦¬ì  ëŒ€í™” í†µê³„: {statistics['total_analyses']}ê°œ ë¶„ì„, {statistics['total_conversations']}ê°œ ëŒ€í™”"
    )
    print(f"   í‰ê·  ì‹ ë¢°ë„: {statistics['average_confidence']:.2f}")
    print(f"   íŒë‹¨ í†µê³„: {statistics['judgment_statistics']}")

    # 4. ë°ì´í„° ë‚´ë³´ë‚´ê¸°
    export_data = ethical_system.export_ethical_data()
    print(
        f"âœ… ìœ¤ë¦¬ì  ëŒ€í™” ë°ì´í„° ë‚´ë³´ë‚´ê¸°: {len(export_data['ethical_analyses'])}ê°œ ë¶„ì„"
    )

    print("ğŸ‰ EthicalConversationSystem í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    test_ethical_conversation_system()
