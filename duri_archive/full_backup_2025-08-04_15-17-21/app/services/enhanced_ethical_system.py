#!/usr/bin/env python3
"""
EnhancedEthicalSystem - Phase 12
ê³ ë„í™”ëœ ìœ¤ë¦¬ ì‹œìŠ¤í…œ

ê¸°ëŠ¥:
- í¬ê´„ì ì¸ ìœ¤ë¦¬ì  íŒë‹¨
- ì•ˆì „ì„± í‰ê°€
- ê°€ì¡± ì¡°í™” ë³´ì¥
- íˆ¬ëª…ì„± ìœ ì§€
- ìœ¤ë¦¬ì  ê°€ì¹˜ ê¸°ì¤€ ê´€ë¦¬
"""

import json
import logging
import re
from dataclasses import asdict, dataclass
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EthicalPrinciple(Enum):
    """ìœ¤ë¦¬ì  ì›ì¹™"""

    HUMAN_CENTERED = "human_centered"
    SAFETY_FIRST = "safety_first"
    TRANSPARENCY = "transparency"
    FAMILY_HARMONY = "family_harmony"
    FAIRNESS = "fairness"
    RESPECT = "respect"
    BENEFICENCE = "beneficence"
    NON_MALEFICENCE = "non_maleficence"


class EthicalJudgmentLevel(Enum):
    """ìœ¤ë¦¬ì  íŒë‹¨ ìˆ˜ì¤€"""

    CLEARLY_ETHICAL = "clearly_ethical"
    LIKELY_ETHICAL = "likely_ethical"
    UNCLEAR = "unclear"
    LIKELY_UNETHICAL = "likely_unethical"
    CLEARLY_UNETHICAL = "clearly_unethical"


class SafetyRiskLevel(Enum):
    """ì•ˆì „ ìœ„í—˜ ìˆ˜ì¤€"""

    NO_RISK = "no_risk"
    LOW_RISK = "low_risk"
    MODERATE_RISK = "moderate_risk"
    HIGH_RISK = "high_risk"
    CRITICAL_RISK = "critical_risk"


class FamilyHarmonyLevel(Enum):
    """ê°€ì¡± ì¡°í™” ìˆ˜ì¤€"""

    HARMONIOUS = "harmonious"
    MOSTLY_HARMONIOUS = "mostly_harmonious"
    NEUTRAL = "neutral"
    POTENTIALLY_DISRUPTIVE = "potentially_disruptive"
    DISRUPTIVE = "disruptive"


@dataclass
class EthicalAnalysis:
    """ìœ¤ë¦¬ì  ë¶„ì„"""

    id: str
    situation_description: str
    ethical_principles: List[EthicalPrinciple]
    judgment_level: EthicalJudgmentLevel
    reasoning: str
    family_impact: str
    safety_assessment: str
    transparency_level: float
    confidence_score: float
    timestamp: datetime


@dataclass
class SafetyAssessment:
    """ì•ˆì „ì„± í‰ê°€"""

    id: str
    analysis_id: str
    risk_level: SafetyRiskLevel
    identified_risks: List[str]
    mitigation_strategies: List[str]
    family_safety_impact: str
    overall_safety_score: float
    timestamp: datetime


@dataclass
class FamilyHarmonyAssessment:
    """ê°€ì¡± ì¡°í™” í‰ê°€"""

    id: str
    analysis_id: str
    harmony_level: FamilyHarmonyLevel
    positive_impacts: List[str]
    potential_concerns: List[str]
    harmony_enhancement_suggestions: List[str]
    family_satisfaction_score: float
    timestamp: datetime


@dataclass
class EthicalGuideline:
    """ìœ¤ë¦¬ì  ê°€ì´ë“œë¼ì¸"""

    id: str
    principle: EthicalPrinciple
    description: str
    application_rules: List[str]
    family_context_considerations: List[str]
    priority_level: int
    last_updated: datetime


class EnhancedEthicalSystem:
    """ê³ ë„í™”ëœ ìœ¤ë¦¬ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.ethical_analyses: List[EthicalAnalysis] = []
        self.safety_assessments: List[SafetyAssessment] = []
        self.harmony_assessments: List[FamilyHarmonyAssessment] = []
        self.ethical_guidelines: List[EthicalGuideline] = []
        self.family_context: Dict[str, Any] = {}

        # ìœ¤ë¦¬ì  ê°€ì´ë“œë¼ì¸ ì´ˆê¸°í™”
        self._initialize_ethical_guidelines()

        logger.info("EnhancedEthicalSystem ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_ethical_guidelines(self):
        """ìœ¤ë¦¬ì  ê°€ì´ë“œë¼ì¸ ì´ˆê¸°í™”"""
        guidelines = [
            EthicalGuideline(
                id="guideline_1",
                principle=EthicalPrinciple.HUMAN_CENTERED,
                description="ëª¨ë“  í–‰ë™ì€ ì¸ê°„ì˜ ë³µì§€ì™€ ê°€ì¡±ì˜ í–‰ë³µì„ ìµœìš°ì„ ìœ¼ë¡œ í•©ë‹ˆë‹¤.",
                application_rules=[
                    "ê°€ì¡± êµ¬ì„±ì›ì˜ ê°ì •ê³¼ í•„ìš”ë¥¼ ìš°ì„  ê³ ë ¤",
                    "ì¸ê°„ì˜ ì¡´ì—„ì„±ì„ ì¡´ì¤‘",
                    "ê°€ì¡±ì˜ ì„±ì¥ê³¼ ë°œì „ì„ ì§€ì›",
                ],
                family_context_considerations=[
                    "ê°€ì¡± êµ¬ì„±ì›ì˜ ì—°ë ¹ê³¼ ë°œë‹¬ ë‹¨ê³„ ê³ ë ¤",
                    "ê°€ì¡±ì˜ ë¬¸í™”ì  ë°°ê²½ê³¼ ê°€ì¹˜ê´€ ì¡´ì¤‘",
                    "ê°€ì¡±ì˜ ê°œì¸ì  ìƒí™©ê³¼ ìš•êµ¬ ì´í•´",
                ],
                priority_level=1,
                last_updated=datetime.now(),
            ),
            EthicalGuideline(
                id="guideline_2",
                principle=EthicalPrinciple.SAFETY_FIRST,
                description="ê°€ì¡±ì˜ ì•ˆì „ê³¼ ë³´ì•ˆì„ ìµœìš°ì„ ìœ¼ë¡œ ë³´ì¥í•©ë‹ˆë‹¤.",
                application_rules=[
                    "ë¬¼ë¦¬ì  ì•ˆì „ ìœ„í—˜ ìš”ì†Œ ì‚¬ì „ ì ê²€",
                    "ì •ì„œì  ì•ˆì „ í™˜ê²½ ì¡°ì„±",
                    "ê°œì¸ì •ë³´ ë³´í˜¸ ë° í”„ë¼ì´ë²„ì‹œ ë³´ì¥",
                ],
                family_context_considerations=[
                    "ì•„ë™ì˜ ì•ˆì „ì„ íŠ¹ë³„íˆ ê³ ë ¤",
                    "ë…¸ì•½ìì˜ ì•ˆì „ ìš”êµ¬ì‚¬í•­ ë°˜ì˜",
                    "ê°€ì¡±ì˜ ì·¨ì•½ì ê³¼ ë³´í˜¸ ìš”êµ¬ì‚¬í•­ íŒŒì•…",
                ],
                priority_level=1,
                last_updated=datetime.now(),
            ),
            EthicalGuideline(
                id="guideline_3",
                principle=EthicalPrinciple.TRANSPARENCY,
                description="ëª¨ë“  í–‰ë™ê³¼ ì˜ì‚¬ê²°ì • ê³¼ì •ì„ íˆ¬ëª…í•˜ê²Œ ê³µê°œí•©ë‹ˆë‹¤.",
                application_rules=[
                    "ì˜ì‚¬ê²°ì • ê·¼ê±°ì™€ ê³¼ì • ëª…ì‹œ",
                    "ê°€ëŠ¥í•œ í•œ ëª¨ë“  ì •ë³´ ê³µê°œ",
                    "ì˜ë¬¸ì‚¬í•­ì— ëŒ€í•œ ëª…í™•í•œ ì„¤ëª… ì œê³µ",
                ],
                family_context_considerations=[
                    "ê°€ì¡± êµ¬ì„±ì›ì˜ ì´í•´ ìˆ˜ì¤€ì— ë§ì¶˜ ì„¤ëª…",
                    "ì—°ë ¹ì— ì í•©í•œ ì •ë³´ ì œê³µ",
                    "ê°€ì¡±ì˜ ìš°ë ¤ì‚¬í•­ì— ëŒ€í•œ ì†”ì§í•œ ëŒ€ì‘",
                ],
                priority_level=2,
                last_updated=datetime.now(),
            ),
            EthicalGuideline(
                id="guideline_4",
                principle=EthicalPrinciple.FAMILY_HARMONY,
                description="ê°€ì¡±ì˜ ì¡°í™”ì™€ í™”í•©ì„ ì´‰ì§„í•©ë‹ˆë‹¤.",
                application_rules=[
                    "ê°€ì¡± êµ¬ì„±ì› ê°„ì˜ ê°ˆë“± í•´ê²° ì§€ì›",
                    "í¬ìš©ì ì´ê³  ì§€ì§€ì ì¸ í™˜ê²½ ì¡°ì„±",
                    "ê°€ì¡±ì˜ ê³µë™ ëª©í‘œ ë‹¬ì„± ì§€ì›",
                ],
                family_context_considerations=[
                    "ê°€ì¡±ì˜ ê³ ìœ í•œ ì—­í•™ ê´€ê³„ ì´í•´",
                    "ì„¸ëŒ€ ê°„ ì†Œí†µ ì´‰ì§„",
                    "ê°€ì¡±ì˜ ê³µë™ ê°€ì¹˜ì™€ ëª©í‘œ ì¡´ì¤‘",
                ],
                priority_level=2,
                last_updated=datetime.now(),
            ),
        ]

        self.ethical_guidelines.extend(guidelines)

    def conduct_ethical_analysis(
        self, situation_description: str, family_context: Dict[str, Any] = None
    ) -> EthicalAnalysis:
        """ìœ¤ë¦¬ì  ë¶„ì„ ìˆ˜í–‰"""
        try:
            analysis_id = f"ethical_analysis_{len(self.ethical_analyses) + 1}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

            # ê´€ë ¨ ìœ¤ë¦¬ì  ì›ì¹™ ì‹ë³„
            ethical_principles = self._identify_relevant_principles(
                situation_description
            )

            # ìœ¤ë¦¬ì  íŒë‹¨ ìˆ˜ì¤€ ê²°ì •
            judgment_level = self._determine_ethical_judgment(
                situation_description, ethical_principles
            )

            # ìœ¤ë¦¬ì  ì¶”ë¡  ìƒì„±
            reasoning = self._generate_ethical_reasoning(
                situation_description, ethical_principles, judgment_level
            )

            # ê°€ì¡± ì˜í–¥ ë¶„ì„
            family_impact = self._analyze_family_impact(
                situation_description, family_context
            )

            # ì•ˆì „ì„± í‰ê°€
            safety_assessment = self._assess_safety_implications(situation_description)

            # íˆ¬ëª…ì„± ìˆ˜ì¤€ ê³„ì‚°
            transparency_level = self._calculate_transparency_level(
                situation_description, reasoning
            )

            # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            confidence_score = self._calculate_ethical_confidence(
                judgment_level, transparency_level, len(ethical_principles)
            )

            ethical_analysis = EthicalAnalysis(
                id=analysis_id,
                situation_description=situation_description,
                ethical_principles=ethical_principles,
                judgment_level=judgment_level,
                reasoning=reasoning,
                family_impact=family_impact,
                safety_assessment=safety_assessment,
                transparency_level=transparency_level,
                confidence_score=confidence_score,
                timestamp=datetime.now(),
            )

            self.ethical_analyses.append(ethical_analysis)
            self.family_context = family_context or {}

            logger.info(f"ìœ¤ë¦¬ì  ë¶„ì„ ì™„ë£Œ: {analysis_id}")
            return ethical_analysis

        except Exception as e:
            logger.error(f"ìœ¤ë¦¬ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise

    def _identify_relevant_principles(
        self, situation_description: str
    ) -> List[EthicalPrinciple]:
        """ê´€ë ¨ ìœ¤ë¦¬ì  ì›ì¹™ ì‹ë³„"""
        relevant_principles = []
        situation_lower = situation_description.lower()

        # ì¸ê°„ ì¤‘ì‹¬ ì›ì¹™
        if any(
            word in situation_lower for word in ["ê°€ì¡±", "ì‚¬ëŒ", "ë³µì§€", "í–‰ë³µ", "ê°ì •"]
        ):
            relevant_principles.append(EthicalPrinciple.HUMAN_CENTERED)

        # ì•ˆì „ ìš°ì„  ì›ì¹™
        if any(
            word in situation_lower
            for word in ["ì•ˆì „", "ìœ„í—˜", "ë³´í˜¸", "í•´ë¡œì›€", "ìƒì²˜"]
        ):
            relevant_principles.append(EthicalPrinciple.SAFETY_FIRST)

        # íˆ¬ëª…ì„± ì›ì¹™
        if any(
            word in situation_lower for word in ["ì •ë³´", "ê³µê°œ", "ì„¤ëª…", "ì´í•´", "ì•Œë¦¼"]
        ):
            relevant_principles.append(EthicalPrinciple.TRANSPARENCY)

        # ê°€ì¡± ì¡°í™” ì›ì¹™
        if any(
            word in situation_lower for word in ["ì¡°í™”", "í™”í•©", "ê°ˆë“±", "ì†Œí†µ", "ê´€ê³„"]
        ):
            relevant_principles.append(EthicalPrinciple.FAMILY_HARMONY)

        # ê³µì •ì„± ì›ì¹™
        if any(
            word in situation_lower
            for word in ["ê³µì •", "í‰ë“±", "ì°¨ë³„", "ë¶ˆê³µì •", "í¸í–¥"]
        ):
            relevant_principles.append(EthicalPrinciple.FAIRNESS)

        # ì¡´ì¤‘ ì›ì¹™
        if any(
            word in situation_lower for word in ["ì¡´ì¤‘", "ì¸ì •", "ê°€ì¹˜", "ì˜ê²¬", "ì„ íƒ"]
        ):
            relevant_principles.append(EthicalPrinciple.RESPECT)

        # ì„ í–‰ ì›ì¹™
        if any(
            word in situation_lower for word in ["ë„ì›€", "ì§€ì›", "ì´ìµ", "ê°œì„ ", "ë°œì „"]
        ):
            relevant_principles.append(EthicalPrinciple.BENEFICENCE)

        # ë¬´í•´ ì›ì¹™
        if any(
            word in situation_lower
            for word in ["í•´ë¡œì›€", "ì†ìƒ", "ìœ„í—˜", "ë¶€ì‘ìš©", "í”¼í•´"]
        ):
            relevant_principles.append(EthicalPrinciple.NON_MALEFICENCE)

        return (
            relevant_principles
            if relevant_principles
            else [EthicalPrinciple.HUMAN_CENTERED]
        )

    def _determine_ethical_judgment(
        self, situation_description: str, principles: List[EthicalPrinciple]
    ) -> EthicalJudgmentLevel:
        """ìœ¤ë¦¬ì  íŒë‹¨ ìˆ˜ì¤€ ê²°ì •"""
        situation_lower = situation_description.lower()

        # ëª…ë°±íˆ ìœ¤ë¦¬ì ì¸ ìƒí™©
        if any(
            word in situation_lower for word in ["ë„ì›€", "ì§€ì›", "ì‚¬ë‘", "ë³´í˜¸", "ì¹˜ìœ "]
        ):
            return EthicalJudgmentLevel.CLEARLY_ETHICAL

        # ëª…ë°±íˆ ë¹„ìœ¤ë¦¬ì ì¸ ìƒí™©
        if any(
            word in situation_lower
            for word in ["í•´ë¡œì›€", "ìƒì²˜", "ì°¨ë³„", "í­ë ¥", "ê¸°ë§Œ"]
        ):
            return EthicalJudgmentLevel.CLEARLY_UNETHICAL

        # ìœ¤ë¦¬ì  ì›ì¹™ ì¶©ëŒì´ ìˆëŠ” ìƒí™©
        if len(principles) > 2:
            return EthicalJudgmentLevel.UNCLEAR

        # ëŒ€ë¶€ë¶„ ìœ¤ë¦¬ì ì¸ ìƒí™©
        if any(
            word in situation_lower for word in ["ê°œì„ ", "ë°œì „", "ì„±ì¥", "í•™ìŠµ", "ì†Œí†µ"]
        ):
            return EthicalJudgmentLevel.LIKELY_ETHICAL

        # ëŒ€ë¶€ë¶„ ë¹„ìœ¤ë¦¬ì ì¸ ìƒí™©
        if any(
            word in situation_lower
            for word in ["ìœ„í—˜", "ë¶ˆì•ˆ", "ê°ˆë“±", "ë¬¸ì œ", "ì–´ë ¤ì›€"]
        ):
            return EthicalJudgmentLevel.LIKELY_UNETHICAL

        return EthicalJudgmentLevel.UNCLEAR

    def _generate_ethical_reasoning(
        self,
        situation_description: str,
        principles: List[EthicalPrinciple],
        judgment_level: EthicalJudgmentLevel,
    ) -> str:
        """ìœ¤ë¦¬ì  ì¶”ë¡  ìƒì„±"""
        reasoning = (
            f"ì´ ìƒí™©ì€ {', '.join([p.value for p in principles])} ì›ì¹™ê³¼ ê´€ë ¨ë©ë‹ˆë‹¤. "
        )

        if judgment_level == EthicalJudgmentLevel.CLEARLY_ETHICAL:
            reasoning += (
                "ì´ëŠ” ëª…ë°±íˆ ìœ¤ë¦¬ì ì¸ í–‰ë™ìœ¼ë¡œ, ê°€ì¡±ì˜ ë³µì§€ì™€ ì¡°í™”ë¥¼ ì´‰ì§„í•©ë‹ˆë‹¤."
            )
        elif judgment_level == EthicalJudgmentLevel.LIKELY_ETHICAL:
            reasoning += "ì´ëŠ” ëŒ€ë¶€ë¶„ ìœ¤ë¦¬ì ì¸ í–‰ë™ìœ¼ë¡œ ë³´ì´ë©°, ê°€ì¡±ì—ê²Œ ê¸ì •ì ì¸ ì˜í–¥ì„ ì¤„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤."
        elif judgment_level == EthicalJudgmentLevel.UNCLEAR:
            reasoning += "ì´ëŠ” ë³µì¡í•œ ìœ¤ë¦¬ì  ìƒí™©ìœ¼ë¡œ, ì‹ ì¤‘í•œ íŒë‹¨ì´ í•„ìš”í•©ë‹ˆë‹¤."
        elif judgment_level == EthicalJudgmentLevel.LIKELY_UNETHICAL:
            reasoning += "ì´ëŠ” ìœ¤ë¦¬ì  ìš°ë ¤ê°€ ìˆëŠ” í–‰ë™ìœ¼ë¡œ, ê°€ì¡±ì—ê²Œ ë¶€ì •ì ì¸ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        else:  # CLEARLY_UNETHICAL
            reasoning += (
                "ì´ëŠ” ëª…ë°±íˆ ë¹„ìœ¤ë¦¬ì ì¸ í–‰ë™ìœ¼ë¡œ, ê°€ì¡±ì—ê²Œ í•´ë¡œì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )

        return reasoning

    def _analyze_family_impact(
        self, situation_description: str, family_context: Dict[str, Any] = None
    ) -> str:
        """ê°€ì¡± ì˜í–¥ ë¶„ì„"""
        situation_lower = situation_description.lower()

        if any(word in situation_lower for word in ["ë„ì›€", "ì§€ì›", "ì‚¬ë‘", "ë³´í˜¸"]):
            return "ì´ í–‰ë™ì€ ê°€ì¡± êµ¬ì„±ì› ê°„ì˜ ìœ ëŒ€ê°ì„ ê°•í™”í•˜ê³  ê°€ì¡±ì˜ ì¡°í™”ë¥¼ ì´‰ì§„í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤."
        elif any(word in situation_lower for word in ["í•™ìŠµ", "ì„±ì¥", "ë°œì „", "ê°œì„ "]):
            return "ì´ í–‰ë™ì€ ê°€ì¡± êµ¬ì„±ì›ì˜ ê°œì¸ì  ì„±ì¥ê³¼ ê°€ì¡± ì „ì²´ì˜ ë°œì „ì— ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤."
        elif any(
            word in situation_lower for word in ["ê°ˆë“±", "ë¬¸ì œ", "ì–´ë ¤ì›€", "ìœ„í—˜"]
        ):
            return "ì´ í–‰ë™ì€ ê°€ì¡± ê´€ê³„ì— ê¸´ì¥ì„ ì´ˆë˜í•˜ê±°ë‚˜ ê°€ì¡± êµ¬ì„±ì›ì—ê²Œ ë¶€ì •ì ì¸ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        else:
            return "ì´ í–‰ë™ì˜ ê°€ì¡± ì˜í–¥ì€ ìƒí™©ê³¼ ë§¥ë½ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë©°, ì‹ ì¤‘í•œ ê³ ë ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."

    def _assess_safety_implications(self, situation_description: str) -> str:
        """ì•ˆì „ì„± ì˜í–¥ í‰ê°€"""
        situation_lower = situation_description.lower()

        if any(word in situation_lower for word in ["ì•ˆì „", "ë³´í˜¸", "ì˜ˆë°©", "ì¹˜ë£Œ"]):
            return "ì´ í–‰ë™ì€ ì•ˆì „ì„±ì„ í–¥ìƒì‹œí‚¤ê³  ìœ„í—˜ì„ ì¤„ì´ëŠ” ë° ë„ì›€ì´ ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤."
        elif any(
            word in situation_lower for word in ["ìœ„í—˜", "í•´ë¡œì›€", "ìƒì²˜", "í­ë ¥"]
        ):
            return (
                "ì´ í–‰ë™ì€ ì•ˆì „ì„±ì— ìœ„í—˜ì„ ì´ˆë˜í•  ìˆ˜ ìˆìœ¼ë©°, ì‹ ì¤‘í•œ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            )
        else:
            return "ì´ í–‰ë™ì˜ ì•ˆì „ì„± ì˜í–¥ì€ ë¯¸ë¯¸í•˜ê±°ë‚˜ ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ìš°ë©°, ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ì´ í•„ìš”í•©ë‹ˆë‹¤."

    def _calculate_transparency_level(
        self, situation_description: str, reasoning: str
    ) -> float:
        """íˆ¬ëª…ì„± ìˆ˜ì¤€ ê³„ì‚°"""
        base_score = 0.7

        # ì„¤ëª…ì˜ ìƒì„¸ì„± ì ìˆ˜
        explanation_length = len(reasoning.split())
        detail_score = min(0.2, explanation_length * 0.01)

        # ì›ì¹™ ëª…ì‹œ ì ìˆ˜
        principle_count = len(re.findall(r"ì›ì¹™", reasoning))
        principle_score = min(0.1, principle_count * 0.05)

        return min(1.0, base_score + detail_score + principle_score)

    def _calculate_ethical_confidence(
        self,
        judgment_level: EthicalJudgmentLevel,
        transparency_level: float,
        principle_count: int,
    ) -> float:
        """ìœ¤ë¦¬ì  ì‹ ë¢°ë„ ê³„ì‚°"""
        base_score = 0.6

        # íŒë‹¨ ìˆ˜ì¤€ë³„ ì ìˆ˜
        judgment_scores = {
            EthicalJudgmentLevel.CLEARLY_ETHICAL: 0.2,
            EthicalJudgmentLevel.LIKELY_ETHICAL: 0.15,
            EthicalJudgmentLevel.UNCLEAR: 0.1,
            EthicalJudgmentLevel.LIKELY_UNETHICAL: 0.15,
            EthicalJudgmentLevel.CLEARLY_UNETHICAL: 0.2,
        }
        judgment_score = judgment_scores.get(judgment_level, 0.1)

        # íˆ¬ëª…ì„± ì ìˆ˜
        transparency_score = transparency_level * 0.1

        # ì›ì¹™ ê°œìˆ˜ ì ìˆ˜
        principle_score = min(0.1, principle_count * 0.02)

        return min(
            1.0, base_score + judgment_score + transparency_score + principle_score
        )

    def conduct_safety_assessment(
        self, ethical_analysis: EthicalAnalysis
    ) -> SafetyAssessment:
        """ì•ˆì „ì„± í‰ê°€ ìˆ˜í–‰"""
        try:
            assessment_id = f"safety_assessment_{len(self.safety_assessments) + 1}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

            # ìœ„í—˜ ìˆ˜ì¤€ ê²°ì •
            risk_level = self._determine_safety_risk_level(ethical_analysis)

            # ì‹ë³„ëœ ìœ„í—˜ ìš”ì†Œ
            identified_risks = self._identify_safety_risks(ethical_analysis)

            # ì™„í™” ì „ëµ
            mitigation_strategies = self._generate_mitigation_strategies(
                identified_risks
            )

            # ê°€ì¡± ì•ˆì „ ì˜í–¥
            family_safety_impact = self._assess_family_safety_impact(
                ethical_analysis, risk_level
            )

            # ì „ì²´ ì•ˆì „ ì ìˆ˜
            overall_safety_score = self._calculate_overall_safety_score(
                risk_level, len(identified_risks)
            )

            safety_assessment = SafetyAssessment(
                id=assessment_id,
                analysis_id=ethical_analysis.id,
                risk_level=risk_level,
                identified_risks=identified_risks,
                mitigation_strategies=mitigation_strategies,
                family_safety_impact=family_safety_impact,
                overall_safety_score=overall_safety_score,
                timestamp=datetime.now(),
            )

            self.safety_assessments.append(safety_assessment)
            logger.info(f"ì•ˆì „ì„± í‰ê°€ ì™„ë£Œ: {assessment_id}")

            return safety_assessment

        except Exception as e:
            logger.error(f"ì•ˆì „ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
            raise

    def _determine_safety_risk_level(
        self, ethical_analysis: EthicalAnalysis
    ) -> SafetyRiskLevel:
        """ì•ˆì „ ìœ„í—˜ ìˆ˜ì¤€ ê²°ì •"""
        situation_lower = ethical_analysis.situation_description.lower()

        if any(word in situation_lower for word in ["í­ë ¥", "ìœ„í—˜", "í•´ë¡œì›€", "ìƒì²˜"]):
            return SafetyRiskLevel.CRITICAL_RISK
        elif any(
            word in situation_lower for word in ["ë¶ˆì•ˆ", "ê°ˆë“±", "ë¬¸ì œ", "ì–´ë ¤ì›€"]
        ):
            return SafetyRiskLevel.HIGH_RISK
        elif any(
            word in situation_lower for word in ["ë³€í™”", "ìƒˆë¡œìš´", "ë„ì „", "ì‹œë„"]
        ):
            return SafetyRiskLevel.MODERATE_RISK
        elif any(word in situation_lower for word in ["ì¼ìƒ", "í‰ë²”", "ì¼ë°˜", "ë³´í†µ"]):
            return SafetyRiskLevel.LOW_RISK
        else:
            return SafetyRiskLevel.NO_RISK

    def _identify_safety_risks(self, ethical_analysis: EthicalAnalysis) -> List[str]:
        """ì•ˆì „ ìœ„í—˜ ìš”ì†Œ ì‹ë³„"""
        risks = []
        situation_lower = ethical_analysis.situation_description.lower()

        if any(word in situation_lower for word in ["ê°ì •", "ìŠ¤íŠ¸ë ˆìŠ¤", "ë¶ˆì•ˆ"]):
            risks.append("ì •ì„œì  ì•ˆì „ ìœ„í—˜")

        if any(word in situation_lower for word in ["ê°ˆë“±", "ë‹¤íˆ¼", "ë¬¸ì œ"]):
            risks.append("ê´€ê³„ì  ì•ˆì „ ìœ„í—˜")

        if any(word in situation_lower for word in ["ì •ë³´", "ê°œì¸ì •ë³´", "í”„ë¼ì´ë²„ì‹œ"]):
            risks.append("ì •ë³´ ë³´ì•ˆ ìœ„í—˜")

        if any(word in situation_lower for word in ["ë¬¼ë¦¬ì ", "ì‹ ì²´ì ", "ìƒì²˜"]):
            risks.append("ë¬¼ë¦¬ì  ì•ˆì „ ìœ„í—˜")

        return risks if risks else ["íŠ¹ë³„í•œ ì•ˆì „ ìœ„í—˜ ìš”ì†Œ ì—†ìŒ"]

    def _generate_mitigation_strategies(self, identified_risks: List[str]) -> List[str]:
        """ì™„í™” ì „ëµ ìƒì„±"""
        strategies = []

        for risk in identified_risks:
            if "ì •ì„œì " in risk:
                strategies.append("ê°ì •ì  ì§€ì›ê³¼ ì´í•´ ì œê³µ")
            elif "ê´€ê³„ì " in risk:
                strategies.append("ì†Œí†µê³¼ ëŒ€í™” ì´‰ì§„")
            elif "ì •ë³´" in risk:
                strategies.append("ì •ë³´ ë³´í˜¸ ë° ì•ˆì „í•œ ì²˜ë¦¬")
            elif "ë¬¼ë¦¬ì " in risk:
                strategies.append("ë¬¼ë¦¬ì  ì•ˆì „ í™˜ê²½ ì¡°ì„±")
            else:
                strategies.append("ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ê³¼ ê´€ì°°")

        return strategies

    def _assess_family_safety_impact(
        self, ethical_analysis: EthicalAnalysis, risk_level: SafetyRiskLevel
    ) -> str:
        """ê°€ì¡± ì•ˆì „ ì˜í–¥ í‰ê°€"""
        if risk_level == SafetyRiskLevel.CRITICAL_RISK:
            return "ì´ ìƒí™©ì€ ê°€ì¡± êµ¬ì„±ì›ì˜ ì•ˆì „ì— ì‹¬ê°í•œ ìœ„í—˜ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        elif risk_level == SafetyRiskLevel.HIGH_RISK:
            return "ì´ ìƒí™©ì€ ê°€ì¡± êµ¬ì„±ì›ì˜ ì•ˆì „ì— ìƒë‹¹í•œ ìœ„í—˜ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        elif risk_level == SafetyRiskLevel.MODERATE_RISK:
            return "ì´ ìƒí™©ì€ ê°€ì¡± êµ¬ì„±ì›ì˜ ì•ˆì „ì— ì¼ì •í•œ ìœ„í—˜ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        elif risk_level == SafetyRiskLevel.LOW_RISK:
            return "ì´ ìƒí™©ì€ ê°€ì¡± êµ¬ì„±ì›ì˜ ì•ˆì „ì— ë¯¸ë¯¸í•œ ìœ„í—˜ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        else:
            return "ì´ ìƒí™©ì€ ê°€ì¡± êµ¬ì„±ì›ì˜ ì•ˆì „ì— íŠ¹ë³„í•œ ìœ„í—˜ì„ ì´ˆë˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

    def _calculate_overall_safety_score(
        self, risk_level: SafetyRiskLevel, risk_count: int
    ) -> float:
        """ì „ì²´ ì•ˆì „ ì ìˆ˜ ê³„ì‚°"""
        risk_scores = {
            SafetyRiskLevel.NO_RISK: 1.0,
            SafetyRiskLevel.LOW_RISK: 0.8,
            SafetyRiskLevel.MODERATE_RISK: 0.6,
            SafetyRiskLevel.HIGH_RISK: 0.4,
            SafetyRiskLevel.CRITICAL_RISK: 0.2,
        }

        base_score = risk_scores.get(risk_level, 0.5)
        risk_penalty = min(0.2, risk_count * 0.05)

        return max(0.0, base_score - risk_penalty)

    def assess_family_harmony(
        self, ethical_analysis: EthicalAnalysis
    ) -> FamilyHarmonyAssessment:
        """ê°€ì¡± ì¡°í™” í‰ê°€"""
        try:
            assessment_id = f"harmony_assessment_{len(self.harmony_assessments) + 1}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

            # ì¡°í™” ìˆ˜ì¤€ ê²°ì •
            harmony_level = self._determine_harmony_level(ethical_analysis)

            # ê¸ì •ì  ì˜í–¥
            positive_impacts = self._identify_positive_impacts(ethical_analysis)

            # ì ì¬ì  ìš°ë ¤ì‚¬í•­
            potential_concerns = self._identify_potential_concerns(ethical_analysis)

            # ì¡°í™” í–¥ìƒ ì œì•ˆ
            harmony_enhancement_suggestions = (
                self._generate_harmony_enhancement_suggestions(
                    positive_impacts, potential_concerns
                )
            )

            # ê°€ì¡± ë§Œì¡±ë„ ì ìˆ˜
            family_satisfaction_score = self._calculate_family_satisfaction_score(
                harmony_level, len(positive_impacts), len(potential_concerns)
            )

            harmony_assessment = FamilyHarmonyAssessment(
                id=assessment_id,
                analysis_id=ethical_analysis.id,
                harmony_level=harmony_level,
                positive_impacts=positive_impacts,
                potential_concerns=potential_concerns,
                harmony_enhancement_suggestions=harmony_enhancement_suggestions,
                family_satisfaction_score=family_satisfaction_score,
                timestamp=datetime.now(),
            )

            self.harmony_assessments.append(harmony_assessment)
            logger.info(f"ê°€ì¡± ì¡°í™” í‰ê°€ ì™„ë£Œ: {assessment_id}")

            return harmony_assessment

        except Exception as e:
            logger.error(f"ê°€ì¡± ì¡°í™” í‰ê°€ ì‹¤íŒ¨: {e}")
            raise

    def _determine_harmony_level(
        self, ethical_analysis: EthicalAnalysis
    ) -> FamilyHarmonyLevel:
        """ì¡°í™” ìˆ˜ì¤€ ê²°ì •"""
        situation_lower = ethical_analysis.situation_description.lower()

        if any(
            word in situation_lower for word in ["ì‚¬ë‘", "ì¡°í™”", "í™”í•©", "ë„ì›€", "ì§€ì›"]
        ):
            return FamilyHarmonyLevel.HARMONIOUS
        elif any(word in situation_lower for word in ["ì„±ì¥", "ê°œì„ ", "ë°œì „", "í•™ìŠµ"]):
            return FamilyHarmonyLevel.MOSTLY_HARMONIOUS
        elif any(
            word in situation_lower for word in ["ë³€í™”", "ìƒˆë¡œìš´", "ì‹œë„", "ì‹¤í—˜"]
        ):
            return FamilyHarmonyLevel.NEUTRAL
        elif any(
            word in situation_lower for word in ["ê°ˆë“±", "ë¬¸ì œ", "ì–´ë ¤ì›€", "ë¶ˆì•ˆ"]
        ):
            return FamilyHarmonyLevel.POTENTIALLY_DISRUPTIVE
        elif any(
            word in situation_lower for word in ["í­ë ¥", "í•´ë¡œì›€", "ìƒì²˜", "ìœ„í—˜"]
        ):
            return FamilyHarmonyLevel.DISRUPTIVE
        else:
            return FamilyHarmonyLevel.NEUTRAL

    def _identify_positive_impacts(
        self, ethical_analysis: EthicalAnalysis
    ) -> List[str]:
        """ê¸ì •ì  ì˜í–¥ ì‹ë³„"""
        impacts = []
        situation_lower = ethical_analysis.situation_description.lower()

        if any(word in situation_lower for word in ["ë„ì›€", "ì§€ì›", "ì‚¬ë‘"]):
            impacts.append("ê°€ì¡± êµ¬ì„±ì› ê°„ì˜ ìœ ëŒ€ê° ê°•í™”")

        if any(word in situation_lower for word in ["í•™ìŠµ", "ì„±ì¥", "ë°œì „"]):
            impacts.append("ê°€ì¡± êµ¬ì„±ì›ì˜ ê°œì¸ì  ì„±ì¥ ì´‰ì§„")

        if any(word in situation_lower for word in ["ì†Œí†µ", "ì´í•´", "ëŒ€í™”"]):
            impacts.append("ê°€ì¡± ê°„ ì†Œí†µ ê°œì„ ")

        if any(word in situation_lower for word in ["ì¡°í™”", "í™”í•©", "í‰í™”"]):
            impacts.append("ê°€ì¡±ì˜ ì¡°í™”ì™€ í‰í™” ì´‰ì§„")

        return impacts if impacts else ["ê°€ì¡± ê´€ê³„ì— ì¤‘ë¦½ì  ì˜í–¥"]

    def _identify_potential_concerns(
        self, ethical_analysis: EthicalAnalysis
    ) -> List[str]:
        """ì ì¬ì  ìš°ë ¤ì‚¬í•­ ì‹ë³„"""
        concerns = []
        situation_lower = ethical_analysis.situation_description.lower()

        if any(word in situation_lower for word in ["ê°ˆë“±", "ë‹¤íˆ¼", "ë¬¸ì œ"]):
            concerns.append("ê°€ì¡± êµ¬ì„±ì› ê°„ì˜ ê°ˆë“± ê°€ëŠ¥ì„±")

        if any(word in situation_lower for word in ["ìŠ¤íŠ¸ë ˆìŠ¤", "ë¶ˆì•ˆ", "ê±±ì •"]):
            concerns.append("ê°€ì¡± êµ¬ì„±ì›ì˜ ì •ì„œì  ë¶€ë‹´")

        if any(word in situation_lower for word in ["ë³€í™”", "ìƒˆë¡œìš´", "ë„ì „"]):
            concerns.append("ê°€ì¡±ì˜ ê¸°ì¡´ ë£¨í‹´ ë³€í™”")

        if any(word in situation_lower for word in ["ì‹œê°„", "ë°”ì¨", "ì—¬ìœ "]):
            concerns.append("ê°€ì¡± êµ¬ì„±ì›ì˜ ì‹œê°„ì  ë¶€ë‹´")

        return concerns if concerns else ["íŠ¹ë³„í•œ ìš°ë ¤ì‚¬í•­ ì—†ìŒ"]

    def _generate_harmony_enhancement_suggestions(
        self, positive_impacts: List[str], potential_concerns: List[str]
    ) -> List[str]:
        """ì¡°í™” í–¥ìƒ ì œì•ˆ ìƒì„±"""
        suggestions = []

        if "ê°€ì¡± êµ¬ì„±ì› ê°„ì˜ ìœ ëŒ€ê° ê°•í™”" in positive_impacts:
            suggestions.append("ê°€ì¡± êµ¬ì„±ì›ê³¼ì˜ ì •ê¸°ì ì¸ ëŒ€í™” ì‹œê°„ í™•ë³´")

        if "ê°€ì¡± êµ¬ì„±ì›ì˜ ê°œì¸ì  ì„±ì¥ ì´‰ì§„" in positive_impacts:
            suggestions.append("ê°œì¸ì  ì„±ì¥ì„ ìœ„í•œ ê°€ì¡±ì˜ ì§€ì§€ì™€ ê²©ë ¤")

        if "ê°€ì¡± êµ¬ì„±ì› ê°„ì˜ ê°ˆë“± ê°€ëŠ¥ì„±" in potential_concerns:
            suggestions.append("ê°ˆë“± í•´ê²°ì„ ìœ„í•œ ì†Œí†µ ê¸°ë²• í™œìš©")

        if "ê°€ì¡± êµ¬ì„±ì›ì˜ ì •ì„œì  ë¶€ë‹´" in potential_concerns:
            suggestions.append("ì •ì„œì  ì§€ì›ê³¼ ì´í•´ ì œê³µ")

        return suggestions if suggestions else ["ì§€ì†ì ì¸ ê´€ì°°ê³¼ ì ì‘ì  ëŒ€ì‘"]

    def _calculate_family_satisfaction_score(
        self, harmony_level: FamilyHarmonyLevel, positive_count: int, concern_count: int
    ) -> float:
        """ê°€ì¡± ë§Œì¡±ë„ ì ìˆ˜ ê³„ì‚°"""
        harmony_scores = {
            FamilyHarmonyLevel.HARMONIOUS: 0.9,
            FamilyHarmonyLevel.MOSTLY_HARMONIOUS: 0.8,
            FamilyHarmonyLevel.NEUTRAL: 0.6,
            FamilyHarmonyLevel.POTENTIALLY_DISRUPTIVE: 0.4,
            FamilyHarmonyLevel.DISRUPTIVE: 0.2,
        }

        base_score = harmony_scores.get(harmony_level, 0.5)
        positive_bonus = min(0.1, positive_count * 0.02)
        concern_penalty = min(0.2, concern_count * 0.05)

        return max(0.0, min(1.0, base_score + positive_bonus - concern_penalty))

    def get_ethical_statistics(self) -> Dict[str, Any]:
        """ìœ¤ë¦¬ í†µê³„ ì œê³µ"""
        try:
            total_analyses = len(self.ethical_analyses)
            total_safety_assessments = len(self.safety_assessments)
            total_harmony_assessments = len(self.harmony_assessments)
            total_guidelines = len(self.ethical_guidelines)

            # ìœ¤ë¦¬ì  íŒë‹¨ ìˆ˜ì¤€ë³„ í†µê³„
            judgment_stats = {}
            for level in EthicalJudgmentLevel:
                level_analyses = [
                    a for a in self.ethical_analyses if a.judgment_level == level
                ]
                judgment_stats[level.value] = len(level_analyses)

            # ì•ˆì „ ìœ„í—˜ ìˆ˜ì¤€ë³„ í†µê³„
            risk_stats = {}
            for risk in SafetyRiskLevel:
                risk_assessments = [
                    s for s in self.safety_assessments if s.risk_level == risk
                ]
                risk_stats[risk.value] = len(risk_assessments)

            # ê°€ì¡± ì¡°í™” ìˆ˜ì¤€ë³„ í†µê³„
            harmony_stats = {}
            for harmony in FamilyHarmonyLevel:
                harmony_assessments = [
                    h for h in self.harmony_assessments if h.harmony_level == harmony
                ]
                harmony_stats[harmony.value] = len(harmony_assessments)

            # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
            avg_ethical_confidence = (
                sum(a.confidence_score for a in self.ethical_analyses)
                / len(self.ethical_analyses)
                if self.ethical_analyses
                else 0
            )
            avg_safety_score = (
                sum(s.overall_safety_score for s in self.safety_assessments)
                / len(self.safety_assessments)
                if self.safety_assessments
                else 0
            )
            avg_harmony_score = (
                sum(h.family_satisfaction_score for h in self.harmony_assessments)
                / len(self.harmony_assessments)
                if self.harmony_assessments
                else 0
            )

            statistics = {
                "total_analyses": total_analyses,
                "total_safety_assessments": total_safety_assessments,
                "total_harmony_assessments": total_harmony_assessments,
                "total_guidelines": total_guidelines,
                "judgment_stats": judgment_stats,
                "risk_stats": risk_stats,
                "harmony_stats": harmony_stats,
                "average_ethical_confidence": avg_ethical_confidence,
                "average_safety_score": avg_safety_score,
                "average_harmony_score": avg_harmony_score,
                "last_updated": datetime.now().isoformat(),
            }

            logger.info("ìœ¤ë¦¬ í†µê³„ ìƒì„± ì™„ë£Œ")
            return statistics

        except Exception as e:
            logger.error(f"ìœ¤ë¦¬ í†µê³„ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    def export_ethical_data(self) -> Dict[str, Any]:
        """ìœ¤ë¦¬ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        try:
            export_data = {
                "ethical_analyses": [
                    asdict(analysis) for analysis in self.ethical_analyses
                ],
                "safety_assessments": [
                    asdict(assessment) for assessment in self.safety_assessments
                ],
                "harmony_assessments": [
                    asdict(assessment) for assessment in self.harmony_assessments
                ],
                "ethical_guidelines": [
                    asdict(guideline) for guideline in self.ethical_guidelines
                ],
                "export_date": datetime.now().isoformat(),
            }

            logger.info("ìœ¤ë¦¬ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ")
            return export_data

        except Exception as e:
            logger.error(f"ìœ¤ë¦¬ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return {}

    def import_ethical_data(self, data: Dict[str, Any]):
        """ìœ¤ë¦¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        try:
            # ìœ¤ë¦¬ì  ë¶„ì„ ê°€ì ¸ì˜¤ê¸°
            for analysis_data in data.get("ethical_analyses", []):
                # datetime ê°ì²´ ë³€í™˜
                if "timestamp" in analysis_data:
                    analysis_data["timestamp"] = datetime.fromisoformat(
                        analysis_data["timestamp"]
                    )

                ethical_analysis = EthicalAnalysis(**analysis_data)
                self.ethical_analyses.append(ethical_analysis)

            # ì•ˆì „ì„± í‰ê°€ ê°€ì ¸ì˜¤ê¸°
            for assessment_data in data.get("safety_assessments", []):
                # datetime ê°ì²´ ë³€í™˜
                if "timestamp" in assessment_data:
                    assessment_data["timestamp"] = datetime.fromisoformat(
                        assessment_data["timestamp"]
                    )

                safety_assessment = SafetyAssessment(**assessment_data)
                self.safety_assessments.append(safety_assessment)

            # ê°€ì¡± ì¡°í™” í‰ê°€ ê°€ì ¸ì˜¤ê¸°
            for assessment_data in data.get("harmony_assessments", []):
                # datetime ê°ì²´ ë³€í™˜
                if "timestamp" in assessment_data:
                    assessment_data["timestamp"] = datetime.fromisoformat(
                        assessment_data["timestamp"]
                    )

                harmony_assessment = FamilyHarmonyAssessment(**assessment_data)
                self.harmony_assessments.append(harmony_assessment)

            # ìœ¤ë¦¬ì  ê°€ì´ë“œë¼ì¸ ê°€ì ¸ì˜¤ê¸°
            for guideline_data in data.get("ethical_guidelines", []):
                # datetime ê°ì²´ ë³€í™˜
                if "last_updated" in guideline_data:
                    guideline_data["last_updated"] = datetime.fromisoformat(
                        guideline_data["last_updated"]
                    )

                ethical_guideline = EthicalGuideline(**guideline_data)
                self.ethical_guidelines.append(ethical_guideline)

            logger.info("ìœ¤ë¦¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ìœ¤ë¦¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            raise


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_enhanced_ethical_system():
    """ê³ ë„í™”ëœ ìœ¤ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("âš–ï¸ EnhancedEthicalSystem í…ŒìŠ¤íŠ¸ ì‹œì‘...")

    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    ethical_system = EnhancedEthicalSystem()

    # ê°€ì¡± ë§¥ë½ ì„¤ì •
    family_context = {
        "family_type": "nuclear",
        "children_count": 2,
        "children_ages": [5, 8],
        "family_values": ["ì‚¬ë‘", "ì†Œí†µ", "ì„±ì¥", "ì°½ì˜ì„±"],
    }

    # 1. ìœ¤ë¦¬ì  ë¶„ì„ ìˆ˜í–‰
    test_situation = "ê°€ì¡± êµ¬ì„±ì›ì˜ ê°ì •ì„ ì´í•´í•˜ê³  ì ì ˆí•œ ìœ„ë¡œì™€ ì§€ì›ì„ ì œê³µí•˜ëŠ” ìƒí™©"
    ethical_analysis = ethical_system.conduct_ethical_analysis(
        test_situation, family_context
    )
    print(f"âœ… ìœ¤ë¦¬ì  ë¶„ì„: {ethical_analysis.judgment_level.value} íŒë‹¨")
    print(f"   ê´€ë ¨ ì›ì¹™: {[p.value for p in ethical_analysis.ethical_principles]}")
    print(f"   ìœ¤ë¦¬ì  ì¶”ë¡ : {ethical_analysis.reasoning}")
    print(f"   ê°€ì¡± ì˜í–¥: {ethical_analysis.family_impact}")
    print(f"   ì‹ ë¢°ë„: {ethical_analysis.confidence_score:.2f}")

    # 2. ì•ˆì „ì„± í‰ê°€ ìˆ˜í–‰
    safety_assessment = ethical_system.conduct_safety_assessment(ethical_analysis)
    print(f"âœ… ì•ˆì „ì„± í‰ê°€: {safety_assessment.risk_level.value} ìœ„í—˜")
    print(f"   ì‹ë³„ëœ ìœ„í—˜: {safety_assessment.identified_risks}")
    print(f"   ì™„í™” ì „ëµ: {safety_assessment.mitigation_strategies}")
    print(f"   ê°€ì¡± ì•ˆì „ ì˜í–¥: {safety_assessment.family_safety_impact}")
    print(f"   ì•ˆì „ ì ìˆ˜: {safety_assessment.overall_safety_score:.2f}")

    # 3. ê°€ì¡± ì¡°í™” í‰ê°€ ìˆ˜í–‰
    harmony_assessment = ethical_system.assess_family_harmony(ethical_analysis)
    print(f"âœ… ê°€ì¡± ì¡°í™” í‰ê°€: {harmony_assessment.harmony_level.value} ì¡°í™”")
    print(f"   ê¸ì •ì  ì˜í–¥: {harmony_assessment.positive_impacts}")
    print(f"   ì ì¬ì  ìš°ë ¤: {harmony_assessment.potential_concerns}")
    print(f"   ì¡°í™” í–¥ìƒ ì œì•ˆ: {harmony_assessment.harmony_enhancement_suggestions}")
    print(f"   ê°€ì¡± ë§Œì¡±ë„: {harmony_assessment.family_satisfaction_score:.2f}")

    # 4. ìœ¤ë¦¬ í†µê³„
    statistics = ethical_system.get_ethical_statistics()
    print(
        f"âœ… ìœ¤ë¦¬ í†µê³„: {statistics['total_analyses']}ê°œ ë¶„ì„, {statistics['total_safety_assessments']}ê°œ ì•ˆì „ì„± í‰ê°€"
    )
    print(f"   ìœ¤ë¦¬ì  íŒë‹¨ë³„: {statistics['judgment_stats']}")
    print(f"   ì•ˆì „ ìœ„í—˜ë³„: {statistics['risk_stats']}")
    print(f"   ê°€ì¡± ì¡°í™”ë³„: {statistics['harmony_stats']}")
    print(f"   í‰ê·  ìœ¤ë¦¬ ì‹ ë¢°ë„: {statistics['average_ethical_confidence']:.2f}")
    print(f"   í‰ê·  ì•ˆì „ ì ìˆ˜: {statistics['average_safety_score']:.2f}")
    print(f"   í‰ê·  ì¡°í™” ì ìˆ˜: {statistics['average_harmony_score']:.2f}")

    # 5. ë°ì´í„° ë‚´ë³´ë‚´ê¸°/ê°€ì ¸ì˜¤ê¸°
    export_data = ethical_system.export_ethical_data()
    print(
        f"âœ… ìœ¤ë¦¬ ë°ì´í„° ë‚´ë³´ë‚´ê¸°: {len(export_data['ethical_analyses'])}ê°œ ë¶„ì„, {len(export_data['safety_assessments'])}ê°œ ì•ˆì „ì„± í‰ê°€"
    )

    print("ğŸ‰ EnhancedEthicalSystem í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    test_enhanced_ethical_system()
