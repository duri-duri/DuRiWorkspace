#!/usr/bin/env python3
"""
ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ì´ í†µí•©ëœ DuRi ìê°€ì§„í™” AI ì‹œìŠ¤í…œ
"""

import functools
import os
import sys
import time
from datetime import datetime
from typing import Any, Dict

import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware

# ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), "duri_modules"))

# ëª¨ë“ˆí™”ëœ ì‹œìŠ¤í…œ import
from duri_modules import (  # noqa: E402
    chatgpt_evaluator,
    conversation_store,
    dashboard_generator,
    duri_chatgpt_discussion,
    duri_self_reflector,
    meta_loop_system,
    performance_tracker,
)

app = FastAPI(
    title="DuRi Monitored Self-Evolving AI System",
    description="ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ì´ í†µí•©ëœ ìê°€ì§„í™” AI ì‹œìŠ¤í…œ",
    version="3.0.0",
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def track_performance(endpoint_name: str):
    """ì„±ëŠ¥ ì¶”ì  ë°ì½”ë ˆì´í„°"""

    def decorator(func):
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            success = True
            error_message = None

            try:
                result = await func(*args, **kwargs)
                return result
            except Exception as e:
                success = False
                error_message = str(e)
                raise
            finally:
                response_time = time.time() - start_time
                performance_tracker.track_request(endpoint_name, response_time, success, error_message)

        return wrapper

    return decorator


@app.get("/health")
@track_performance("health_check")
async def health_check():
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ (ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í¬í•¨)"""
    health_data = performance_tracker.get_health_check()

    return {
        "status": "healthy",
        "service": "duri-monitored-system",
        "timestamp": datetime.now().isoformat(),
        "modules": {
            "evaluation": "âœ… loaded",
            "reflection": "âœ… loaded",
            "discussion": "âœ… loaded",
            "data_store": "âœ… loaded",
            "performance_tracker": "âœ… loaded",
        },
        "performance": health_data,
    }


@app.post("/chatgpt-evaluate")
@track_performance("chatgpt_evaluate")
async def chatgpt_evaluate_response(evaluation_request: Dict[str, Any]):
    """ChatGPT 6ì°¨ì› í‰ê°€ (ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í¬í•¨)"""
    try:
        duri_response = evaluation_request.get("duri_response", "")
        user_question = evaluation_request.get("user_question", "")

        if not duri_response or not user_question:
            raise HTTPException(status_code=400, detail="duri_responseì™€ user_questionì´ í•„ìš”í•©ë‹ˆë‹¤")

        # ëª¨ë“ˆí™”ëœ í‰ê°€ ì‹œìŠ¤í…œ ì‚¬ìš©
        evaluation_result = chatgpt_evaluator.evaluate_response(duri_response, user_question)

        # í•™ìŠµ ë©”íŠ¸ë¦­ ì¶”ì 
        performance_tracker.track_learning_metric(
            "evaluation_score",
            evaluation_result.get("total_score", 0.0),
            {"user_question": user_question[:50]},
        )

        return {
            "status": "success",
            "evaluation": evaluation_result,
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        print(f"âŒ ChatGPT í‰ê°€ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))  # noqa: B904


@app.post("/duri-self-reflect")
@track_performance("duri_self_reflect")
async def duri_self_reflect_endpoint(reflection_request: Dict[str, Any]):
    """DuRi ìê¸°ì„±ì°° (ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í¬í•¨)"""
    try:
        chatgpt_evaluation = reflection_request.get("chatgpt_evaluation", {})
        original_response = reflection_request.get("original_response", "")
        user_question = reflection_request.get("user_question", "")

        if not chatgpt_evaluation or not original_response:
            raise HTTPException(
                status_code=400,
                detail="chatgpt_evaluationê³¼ original_responseê°€ í•„ìš”í•©ë‹ˆë‹¤",
            )

        # ëª¨ë“ˆí™”ëœ ìê¸°ì„±ì°° ì‹œìŠ¤í…œ ì‚¬ìš©
        reflection_result = duri_self_reflector.reflect_on_chatgpt_feedback(
            chatgpt_evaluation, original_response, user_question
        )

        # í•™ìŠµ ë©”íŠ¸ë¦­ ì¶”ì 
        improvement_count = len(reflection_result.get("improvement_proposal", {}).get("specific_improvements", []))
        performance_tracker.track_learning_metric(
            "improvement_suggestions",
            improvement_count,
            {"reflection_depth": len(reflection_result.get("accepted_criticisms", []))},
        )

        return {
            "status": "success",
            "reflection": reflection_result,
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        print(f"âŒ DuRi ìê¸°ì„±ì°° ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))  # noqa: B904


@app.post("/capture-conversation")
@track_performance("capture_conversation")
async def capture_conversation_endpoint(conversation_data: Dict[str, Any]):
    """ì‹¤ì œ ëŒ€í™” ë°ì´í„° ìˆ˜ì§‘ (ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í¬í•¨)"""
    try:
        user_input = conversation_data.get("user_input", "")
        duri_response = conversation_data.get("duri_response", "")
        metadata = conversation_data.get("metadata", {})

        if not user_input or not duri_response:
            raise HTTPException(status_code=400, detail="user_inputê³¼ duri_responseê°€ í•„ìš”í•©ë‹ˆë‹¤")

        # ëŒ€í™” ë°ì´í„° ì €ì¥
        conversation_id = conversation_store.store_conversation(user_input, duri_response, metadata)

        # í•™ìŠµ ë©”íŠ¸ë¦­ ì¶”ì 
        learning_value = conversation_store._calculate_learning_value(user_input, duri_response)
        performance_tracker.track_learning_metric(
            "conversation_learning_value",
            learning_value,
            {"conversation_id": conversation_id},
        )

        # ìë™ í•™ìŠµ ë£¨í”„ ì‹œì‘ (ê¸°ë³¸ í™œì„±í™”)
        auto_learn = conversation_data.get("auto_learn", True)  # ê¸°ë³¸ê°’ì„ Trueë¡œ ë³€ê²½

        if auto_learn:
            print(f"ğŸ”„ ìë™ í•™ìŠµ ë£¨í”„ ì‹œì‘: {conversation_id}")

            # 1ë‹¨ê³„: ChatGPT í‰ê°€
            evaluation_result = chatgpt_evaluator.evaluate_response(duri_response, user_input)
            print(f"   ğŸ“Š ChatGPT í‰ê°€ ì™„ë£Œ: ì´ì  {evaluation_result.get('total_score', 0):.3f}")

            # 2ë‹¨ê³„: DuRi ìê¸°ì„±ì°°
            reflection_result = duri_self_reflector.reflect_on_chatgpt_feedback(
                evaluation_result, duri_response, user_input
            )
            print(
                f"   ğŸ¤” DuRi ìê¸°ì„±ì°° ì™„ë£Œ: {len(reflection_result.get('improvement_proposal', {}).get('specific_improvements', []))}ê°œ ê°œì„ ì•ˆ"  # noqa: E501
            )

            # 3ë‹¨ê³„: DuRi-ChatGPT ë…¼ì˜ (ì„ íƒì )
            discussion_result = None
            if evaluation_result.get("total_score", 0) < 0.5:  # ë‚®ì€ ì ìˆ˜ì¼ ë•Œë§Œ ë…¼ì˜
                discussion_result = duri_chatgpt_discussion.initiate_discussion(
                    reflection_result.get("improvement_proposal", {}), evaluation_result
                )
                print(f"   ğŸ“¥ DuRi-ChatGPT ë…¼ì˜ ì™„ë£Œ: í•©ì˜ ìˆ˜ì¤€ {discussion_result.get('agreement_level', 0):.2f}")

            # 4ë‹¨ê³„: í•™ìŠµ ê²°ê³¼ ì €ì¥ ë° ë©”íƒ€ ë£¨í”„ ì¤€ë¹„
            learning_data = {
                "conversation_id": conversation_id,
                "evaluation": evaluation_result,
                "reflection": reflection_result,
                "discussion": discussion_result,
                "auto_learn_enabled": True,
                "learning_cycle_completed": True,
                "timestamp": datetime.now().isoformat(),
            }

            # ë©”íƒ€ í•™ìŠµ ë©”íŠ¸ë¦­ ì¶”ì 
            performance_tracker.track_learning_metric(
                "learning_cycle_completion",
                1.0,
                {
                    "conversation_id": conversation_id,
                    "evaluation_score": evaluation_result.get("total_score", 0),
                    "improvement_count": len(
                        reflection_result.get("improvement_proposal", {}).get("specific_improvements", [])
                    ),
                },
            )

            return {
                "status": "success",
                "conversation_id": conversation_id,
                "message": "ëŒ€í™” ì €ì¥ ë° ìë™ í•™ìŠµ ë£¨í”„ ì™„ë£Œ",
                "learning_data": learning_data,
                "learning_summary": {
                    "evaluation_score": evaluation_result.get("total_score", 0),
                    "improvement_suggestions": len(
                        reflection_result.get("improvement_proposal", {}).get("specific_improvements", [])
                    ),
                    "discussion_held": discussion_result is not None,
                    "cycle_completed": True,
                },
                "timestamp": datetime.now().isoformat(),
            }

        return {
            "status": "success",
            "conversation_id": conversation_id,
            "message": "ëŒ€í™” ì €ì¥ ì™„ë£Œ",
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        print(f"âŒ ëŒ€í™” ì €ì¥ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))  # noqa: B904


@app.get("/performance-summary")
async def get_performance_summary():
    """ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ"""
    try:
        summary = performance_tracker.get_performance_summary()
        return {
            "status": "success",
            "summary": summary,
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        print(f"âŒ ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))  # noqa: B904


@app.get("/system-health")
async def get_system_health():
    """ì‹œìŠ¤í…œ ê±´ê°•ë„ í™•ì¸"""
    try:
        health_data = performance_tracker.get_health_check()
        return {
            "status": "success",
            "health": health_data,
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ê±´ê°•ë„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))  # noqa: B904


@app.get("/learning-statistics")
async def get_learning_statistics():
    """í•™ìŠµ í†µê³„ ì¡°íšŒ"""
    try:
        stats = conversation_store.get_learning_statistics()
        patterns = conversation_store.extract_learning_patterns()

        return {
            "status": "success",
            "statistics": stats,
            "learning_patterns": patterns,
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        print(f"âŒ í•™ìŠµ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))  # noqa: B904


@app.post("/meta-evaluate-improvement")
@track_performance("meta_evaluate_improvement")
async def meta_evaluate_improvement_endpoint(meta_request: Dict[str, Any]):
    """ë©”íƒ€ ë£¨í”„: ê°œì„  íš¨ê³¼ í‰ê°€"""
    try:
        original_response = meta_request.get("original_response", "")
        improved_response = meta_request.get("improved_response", "")
        user_question = meta_request.get("user_question", "")
        original_evaluation = meta_request.get("original_evaluation", {})

        if not original_response or not improved_response or not user_question:
            raise HTTPException(
                status_code=400,
                detail="original_response, improved_response, user_questionì´ í•„ìš”í•©ë‹ˆë‹¤",
            )

        # ë©”íƒ€ ë£¨í”„ ì‹œìŠ¤í…œ ì‚¬ìš©
        meta_result = meta_loop_system.evaluate_improvement_effect(
            original_response, improved_response, user_question, original_evaluation
        )

        # ê°œì„  í”¼ë“œë°± ìƒì„±
        feedback = meta_loop_system.generate_improvement_feedback(meta_result)

        # í•™ìŠµ ë©”íŠ¸ë¦­ ì¶”ì 
        improvement_score = meta_result.get("meta_score", 0)
        performance_tracker.track_learning_metric(
            "meta_improvement_score",
            improvement_score,
            {
                "improvement_success": feedback.get("improvement_success", False),
                "overall_improvement": feedback.get("overall_improvement", 0),
            },
        )

        return {
            "status": "success",
            "meta_evaluation": meta_result,
            "feedback": feedback,
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        print(f"âŒ ë©”íƒ€ í‰ê°€ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))  # noqa: B904


@app.get("/meta-learning-statistics")
async def get_meta_learning_statistics():
    """ë©”íƒ€ í•™ìŠµ í†µê³„ ì¡°íšŒ"""
    try:
        meta_stats = meta_loop_system.get_meta_learning_statistics()

        return {
            "status": "success",
            "meta_learning_statistics": meta_stats,
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        print(f"âŒ ë©”íƒ€ í•™ìŠµ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))  # noqa: B904


@app.get("/dashboard")
async def get_dashboard():
    """ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
    try:
        # í•„ìš”í•œ ë°ì´í„° ìˆ˜ì§‘
        performance_summary = performance_tracker.get_performance_summary()
        learning_stats = conversation_store.get_learning_statistics()
        meta_stats = meta_loop_system.get_meta_learning_statistics()

        # ëŒ€ì‹œë³´ë“œ ìƒì„±
        dashboard_path = dashboard_generator.generate_dashboard(performance_summary, learning_stats, meta_stats)

        return {
            "status": "success",
            "dashboard_path": dashboard_path,
            "dashboard_url": f"file://{dashboard_path}",
            "message": "ëŒ€ì‹œë³´ë“œê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤",
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        print(f"âŒ ëŒ€ì‹œë³´ë“œ ìƒì„± ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))  # noqa: B904


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8088)
