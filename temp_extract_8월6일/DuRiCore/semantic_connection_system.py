#!/usr/bin/env python3
"""
DuRi ì˜ë¯¸ ê¸°ë°˜ ë…¸ë“œ ì—°ê²° ì‹œìŠ¤í…œ - Phase 1-3 Week 3 Day 5
ë™ì  ì¶”ë¡  ê·¸ë˜í”„ì˜ ì˜ë¯¸ ê¸°ë°˜ ë…¸ë“œ ì—°ê²°ì„ ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ

ê¸°ëŠ¥:
1. ê³ ê¸‰ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
2. ì˜ë¯¸ ê¸°ë°˜ ì—°ê²° ìƒì„±
3. ì—°ê²° ìµœì í™”
4. ì—°ê²° ê²€ì¦
"""

import asyncio
import json
import re
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict, field
from enum import Enum
import logging
from collections import defaultdict, deque
import heapq
import random
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í…ŒìŠ¤íŠ¸ìš© í´ë˜ìŠ¤ë“¤ (ë¨¼ì € ì •ì˜)
class NodeType(Enum):
    """ë…¸ë“œ ìœ í˜• - í™•ì¥ëœ ë²„ì „"""
    PREMISE = "premise"
    INFERENCE = "inference"
    CONCLUSION = "conclusion"
    COUNTER_ARGUMENT = "counter_argument"
    EVIDENCE = "evidence"
    ASSUMPTION = "assumption"
    HYPOTHESIS = "hypothesis"
    CONSTRAINT = "constraint"
    ALTERNATIVE = "alternative"
    INTEGRATION = "integration"

class EdgeType(Enum):
    """ì—£ì§€ ìœ í˜• - í™•ì¥ëœ ë²„ì „"""
    SUPPORTS = "supports"
    CONTRADICTS = "contradicts"
    INFERS = "infers"
    ASSUMES = "assumes"
    EVIDENCES = "evidences"
    CONSTRAINS = "constrains"
    ALTERNATES = "alternates"
    INTEGRATES = "integrates"
    CHALLENGES = "challenges"
    REFINES = "refines"

@dataclass
class DynamicReasoningNode:
    """ë™ì  ì¶”ë¡  ë…¸ë“œ (í…ŒìŠ¤íŠ¸ìš©)"""
    node_id: str
    node_type: NodeType
    content: str
    confidence: float
    source: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    semantic_vector: Optional[np.ndarray] = None
    activation_level: float = 1.0
    importance_score: float = 0.5

@dataclass
class DynamicReasoningEdge:
    """ë™ì  ì¶”ë¡  ì—£ì§€ (í…ŒìŠ¤íŠ¸ìš©)"""
    edge_id: str
    source_node: str
    target_node: str
    edge_type: EdgeType
    strength: float
    reasoning: str
    semantic_similarity: float = 0.0
    logical_validity: float = 0.0

@dataclass
class DynamicReasoningGraph:
    """ë™ì  ì¶”ë¡  ê·¸ë˜í”„ (í…ŒìŠ¤íŠ¸ìš©)"""
    graph_id: str
    nodes: Dict[str, DynamicReasoningNode] = field(default_factory=dict)
    edges: Dict[str, DynamicReasoningEdge] = field(default_factory=dict)

class SemanticSimilarityEngine:
    """ê³ ê¸‰ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ì—”ì§„"""
    
    def __init__(self):
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words='english',
            ngram_range=(1, 2),
            min_df=1,
            max_df=0.95
        )
        self.similarity_cache = {}
        self.max_cache_size = 1000
        self.similarity_methods = self._initialize_similarity_methods()
    
    def _initialize_similarity_methods(self) -> Dict[str, Dict[str, Any]]:
        """ìœ ì‚¬ë„ ê³„ì‚° ë°©ë²• ì´ˆê¸°í™”"""
        return {
            "cosine_similarity": {
                "weight": 0.4,
                "description": "ì½”ì‚¬ì¸ ìœ ì‚¬ë„"
            },
            "keyword_overlap": {
                "weight": 0.3,
                "description": "í‚¤ì›Œë“œ ì¤‘ë³µë„"
            },
            "semantic_distance": {
                "weight": 0.2,
                "description": "ì˜ë¯¸ì  ê±°ë¦¬"
            },
            "context_similarity": {
                "weight": 0.1,
                "description": "ì»¨í…ìŠ¤íŠ¸ ìœ ì‚¬ë„"
            }
        }
    
    async def calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """ê³ ê¸‰ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°"""
        # ìºì‹œ í™•ì¸
        cache_key = f"{hash(text1)}_{hash(text2)}"
        if cache_key in self.similarity_cache:
            return self.similarity_cache[cache_key]
        
        # 1. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        cosine_sim = await self._calculate_cosine_similarity(text1, text2)
        
        # 2. í‚¤ì›Œë“œ ì¤‘ë³µë„ ê³„ì‚°
        keyword_overlap = await self._calculate_keyword_overlap(text1, text2)
        
        # 3. ì˜ë¯¸ì  ê±°ë¦¬ ê³„ì‚°
        semantic_distance = await self._calculate_semantic_distance(text1, text2)
        
        # 4. ì»¨í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
        context_similarity = await self._calculate_context_similarity(text1, text2)
        
        # ì¢…í•© ìœ ì‚¬ë„ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì¡°ì •)
        overall_similarity = (
            cosine_sim * 0.3 +  # 0.4 -> 0.3
            keyword_overlap * 0.4 +  # 0.3 -> 0.4
            semantic_distance * 0.2 +  # 0.2 -> 0.2
            context_similarity * 0.1   # 0.1 -> 0.1
        )
        
        # ìµœì†Œ ìœ ì‚¬ë„ ë³´ì¥
        overall_similarity = max(overall_similarity, keyword_overlap * 0.5)
        
        # ìºì‹œì— ì €ì¥
        if len(self.similarity_cache) < self.max_cache_size:
            self.similarity_cache[cache_key] = overall_similarity
        
        return overall_similarity
    
    async def _calculate_cosine_similarity(self, text1: str, text2: str) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            # TF-IDF ë²¡í„°í™”
            texts = [text1, text2]
            tfidf_matrix = self.vectorizer.fit_transform(texts)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            return float(similarity)
        except Exception as e:
            logger.warning(f"ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    async def _calculate_keyword_overlap(self, text1: str, text2: str) -> float:
        """í‚¤ì›Œë“œ ì¤‘ë³µë„ ê³„ì‚°"""
        # í‚¤ì›Œë“œ ì¶”ì¶œ (í•œê¸€ê³¼ ì˜ë¬¸ ëª¨ë‘ ì§€ì›)
        keywords1 = set(re.findall(r'[ê°€-í£a-zA-Z]+', text1.lower()))
        keywords2 = set(re.findall(r'[ê°€-í£a-zA-Z]+', text2.lower()))
        
        # ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œë§Œ í•„í„°ë§ (ê¸¸ì´ê°€ 2 ì´ìƒì¸ ë‹¨ì–´)
        keywords1 = {word for word in keywords1 if len(word) >= 2}
        keywords2 = {word for word in keywords2 if len(word) >= 2}
        
        if not keywords1 or not keywords2:
            return 0.0
        
        # ì¤‘ë³µ í‚¤ì›Œë“œ ìˆ˜
        intersection = len(keywords1.intersection(keywords2))
        union = len(keywords1.union(keywords2))
        
        # Jaccard ìœ ì‚¬ë„ ê³„ì‚°
        jaccard_similarity = intersection / union if union > 0 else 0.0
        
        # ì¶”ê°€ ê°€ì¤‘ì¹˜: ê³µí†µ í‚¤ì›Œë“œì˜ ì¤‘ìš”ë„
        common_keywords = keywords1.intersection(keywords2)
        if common_keywords:
            # ê³µí†µ í‚¤ì›Œë“œê°€ ë§ì„ìˆ˜ë¡ ë†’ì€ ìœ ì‚¬ë„
            keyword_bonus = min(0.4, len(common_keywords) * 0.15)
            jaccard_similarity += keyword_bonus
        
        # ìµœì†Œ ìœ ì‚¬ë„ ë³´ì¥ (ê³µí†µ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´)
        if common_keywords:
            jaccard_similarity = max(jaccard_similarity, 0.2)
        
        return min(1.0, jaccard_similarity)
    
    async def _calculate_semantic_distance(self, text1: str, text2: str) -> float:
        """ì˜ë¯¸ì  ê±°ë¦¬ ê³„ì‚°"""
        # ê°„ë‹¨í•œ ì˜ë¯¸ì  ê±°ë¦¬ ê³„ì‚° (1 - ìœ ì‚¬ë„)
        keyword_overlap = await self._calculate_keyword_overlap(text1, text2)
        semantic_distance = 1.0 - keyword_overlap
        
        # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (ê±°ë¦¬ê°€ ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬ë„ ë†’ìŒ)
        return max(0.0, 1.0 - semantic_distance)
    
    async def _calculate_context_similarity(self, text1: str, text2: str) -> float:
        """ì»¨í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        # ë¬¸ë§¥ì  í‚¤ì›Œë“œ ì¶”ì¶œ
        context_keywords1 = self._extract_context_keywords(text1)
        context_keywords2 = self._extract_context_keywords(text2)
        
        if not context_keywords1 or not context_keywords2:
            return 0.0
        
        # ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ì¤‘ë³µë„
        intersection = len(context_keywords1.intersection(context_keywords2))
        union = len(context_keywords1.union(context_keywords2))
        
        return intersection / union if union > 0 else 0.0
    
    def _extract_context_keywords(self, text: str) -> Set[str]:
        """ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œë§Œ ì¶”ì¶œ (ê¸¸ì´ê°€ 3 ì´ìƒì¸ ë‹¨ì–´)
        keywords = set()
        words = re.findall(r'\w+', text.lower())
        
        for word in words:
            if len(word) >= 3 and word not in ['the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'man', 'new', 'now', 'old', 'see', 'two', 'way', 'who', 'boy', 'did', 'its', 'let', 'put', 'say', 'she', 'too', 'use']:
                keywords.add(word)
        
        return keywords

class SemanticConnectionGenerator:
    """ì˜ë¯¸ ê¸°ë°˜ ì—°ê²° ìƒì„± ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.similarity_engine = SemanticSimilarityEngine()
        self.connection_rules = self._initialize_connection_rules()
        self.connection_cache = {}
        self.max_cache_size = 1000
    
    def _initialize_connection_rules(self) -> Dict[str, Dict[str, Any]]:
        """ì—°ê²° ê·œì¹™ ì´ˆê¸°í™”"""
        return {
            "high_similarity": {
                "threshold": 0.7,
                "edge_types": ["supports", "evidences", "infers"],
                "strength_range": (0.8, 1.0),
                "description": "ë†’ì€ ìœ ì‚¬ë„ ì—°ê²°"
            },
            "medium_similarity": {
                "threshold": 0.4,
                "edge_types": ["supports", "infers", "assumes"],
                "strength_range": (0.5, 0.8),
                "description": "ì¤‘ê°„ ìœ ì‚¬ë„ ì—°ê²°"
            },
            "low_similarity": {
                "threshold": 0.2,
                "edge_types": ["challenges", "contradicts", "constrains"],
                "strength_range": (0.3, 0.6),
                "description": "ë‚®ì€ ìœ ì‚¬ë„ ì—°ê²°"
            }
        }
    
    async def generate_semantic_connections(self, graph: 'DynamicReasoningGraph', 
                                          target_node: 'DynamicReasoningNode',
                                          max_connections: int = 5) -> List['DynamicReasoningEdge']:
        """ì˜ë¯¸ ê¸°ë°˜ ì—°ê²° ìƒì„±"""
        logger.info(f"ì˜ë¯¸ ê¸°ë°˜ ì—°ê²° ìƒì„± ì‹œì‘: {target_node.node_id}")
        
        connections = []
        potential_connections = []
        
        # ëª¨ë“  ë…¸ë“œì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        for node_id, node in graph.nodes.items():
            if node_id != target_node.node_id:
                similarity = await self.similarity_engine.calculate_semantic_similarity(
                    target_node.content, node.content
                )
                
                if similarity > 0.05:  # 0.1 -> 0.05 (ì„ê³„ê°’ ë‚®ì¶¤)
                    potential_connections.append((node, similarity))
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        potential_connections.sort(key=lambda x: x[1], reverse=True)
        
        # ì—°ê²° ìƒì„±
        for node, similarity in potential_connections[:max_connections]:
            edge = await self._create_semantic_connection(target_node, node, similarity)
            if edge:
                connections.append(edge)
        
        logger.info(f"ì˜ë¯¸ ê¸°ë°˜ ì—°ê²° ìƒì„± ì™„ë£Œ: {len(connections)}ê°œ ìƒì„±")
        return connections
    
    async def _create_semantic_connection(self, source_node: 'DynamicReasoningNode', 
                                        target_node: 'DynamicReasoningNode', 
                                        similarity: float) -> Optional['DynamicReasoningEdge']:
        """ì˜ë¯¸ì  ì—°ê²° ìƒì„±"""
        # ì—°ê²° ê·œì¹™ ê²°ì •
        connection_rule = self._determine_connection_rule(similarity)
        if not connection_rule:
            return None
        
        # ì—£ì§€ ìœ í˜• ê²°ì •
        edge_type = self._determine_edge_type(source_node, target_node, similarity)
        if not edge_type:
            return None
        
        # ê°•ë„ ê³„ì‚°
        strength_range = connection_rule["strength_range"]
        strength = random.uniform(*strength_range) * similarity
        
        # ì¶”ë¡  ìƒì„±
        reasoning = self._generate_connection_reasoning(source_node, target_node, edge_type, similarity)
        
        # ì—£ì§€ ìƒì„±
        edge_id = f"semantic_edge_{source_node.node_id}_{target_node.node_id}_{int(datetime.now().timestamp())}"
        edge = DynamicReasoningEdge(
            edge_id=edge_id,
            source_node=source_node.node_id,
            target_node=target_node.node_id,
            edge_type=edge_type,
            strength=strength,
            reasoning=reasoning,
            semantic_similarity=similarity
        )
        
        return edge
    
    def _determine_connection_rule(self, similarity: float) -> Optional[Dict[str, Any]]:
        """ì—°ê²° ê·œì¹™ ê²°ì •"""
        for rule_name, rule in self.connection_rules.items():
            if similarity >= rule["threshold"]:
                return rule
        return None
    
    def _determine_edge_type(self, source_node: 'DynamicReasoningNode', 
                           target_node: 'DynamicReasoningNode', 
                           similarity: float) -> Optional[EdgeType]:
        """ì—£ì§€ ìœ í˜• ê²°ì •"""
        # ë…¸ë“œ ìœ í˜•ì— ë”°ë¥¸ ê¸°ë³¸ ì—£ì§€ ìœ í˜•
        source_type = source_node.node_type.value
        target_type = target_node.node_type.value
        
        # ìœ ì‚¬ë„ ê¸°ë°˜ ì—£ì§€ ìœ í˜• ê²°ì •
        if similarity >= 0.5:
            # ë†’ì€ ìœ ì‚¬ë„: ì§€ì› ê´€ê³„
            if source_type == "premise" and target_type in ["inference", "conclusion"]:
                return EdgeType.SUPPORTS
            elif source_type == "evidence" and target_type in ["inference", "conclusion"]:
                return EdgeType.EVIDENCES
            elif source_type == "inference" and target_type == "conclusion":
                return EdgeType.INFERS
            elif source_type == "premise" and target_type == "evidence":
                return EdgeType.SUPPORTS
            else:
                return EdgeType.SUPPORTS
        elif similarity >= 0.3:
            # ì¤‘ê°„ ìœ ì‚¬ë„: ì¶”ë¡  ê´€ê³„
            if source_type == "premise" and target_type == "inference":
                return EdgeType.SUPPORTS
            elif source_type == "inference" and target_type == "conclusion":
                return EdgeType.INFERS
            elif source_type == "evidence" and target_type in ["inference", "conclusion"]:
                return EdgeType.EVIDENCES
            else:
                return EdgeType.INFERS
        elif similarity >= 0.2:
            # ë‚®ì€ ìœ ì‚¬ë„: ë„ì „ ê´€ê³„
            if source_type == "counter_argument" and target_type in ["conclusion", "inference"]:
                return EdgeType.CHALLENGES
            elif source_type in ["premise", "evidence"] and target_type == "counter_argument":
                return EdgeType.CHALLENGES
            else:
                return EdgeType.CHALLENGES
        else:
            # ë§¤ìš° ë‚®ì€ ìœ ì‚¬ë„: ë„ì „ ê´€ê³„
            return EdgeType.CHALLENGES
    
    def _generate_connection_reasoning(self, source_node: 'DynamicReasoningNode', 
                                     target_node: 'DynamicReasoningNode', 
                                     edge_type: EdgeType, 
                                     similarity: float) -> str:
        """ì—°ê²° ì¶”ë¡  ìƒì„±"""
        reasoning_templates = {
            EdgeType.SUPPORTS: [
                "ì˜ë¯¸ì  ìœ ì‚¬ë„ {similarity:.2f}ë¡œ {source}ê°€ {target}ë¥¼ ì§€ì›í•¨",
                "ìœ ì‚¬í•œ ë§¥ë½ìœ¼ë¡œ {source}ê°€ {target}ë¥¼ ê°•í™”í•¨",
                "ì˜ë¯¸ì  ì—°ê´€ì„± {similarity:.2f}ë¡œ {source}ê°€ {target}ë¥¼ í™•ì¦í•¨"
            ],
            EdgeType.INFERS: [
                "ì˜ë¯¸ì  ìœ ì‚¬ë„ {similarity:.2f}ë¡œ {source}ì—ì„œ {target}ë¥¼ ì¶”ë¡ í•¨",
                "ìœ ì‚¬í•œ ë§¥ë½ìœ¼ë¡œ {source}ê°€ {target}ì™€ ì—°ê²°ë¨",
                "ì˜ë¯¸ì  ì—°ê´€ì„± {similarity:.2f}ë¡œ {source}ì—ì„œ {target}ë¡œ ì „ê°œë¨"
            ],
            EdgeType.EVIDENCES: [
                "ì˜ë¯¸ì  ìœ ì‚¬ë„ {similarity:.2f}ë¡œ {source}ê°€ {target}ì˜ ì¦ê±°ê°€ ë¨",
                "ìœ ì‚¬í•œ ë§¥ë½ìœ¼ë¡œ {source}ê°€ {target}ë¥¼ ì…ì¦í•¨",
                "ì˜ë¯¸ì  ì—°ê´€ì„± {similarity:.2f}ë¡œ {source}ê°€ {target}ë¥¼ ì§€ì§€í•¨"
            ],
            EdgeType.CHALLENGES: [
                "ì˜ë¯¸ì  ì°¨ì´ {similarity:.2f}ë¡œ {source}ê°€ {target}ì— ë„ì „í•¨",
                "ë‹¤ë¥¸ ê´€ì ìœ¼ë¡œ {source}ê°€ {target}ì— ì˜ë¬¸ì„ ì œê¸°í•¨",
                "ì˜ë¯¸ì  ì°¨ì´ {similarity:.2f}ë¡œ {source}ê°€ {target}ë¥¼ ë°˜ë°•í•¨"
            ]
        }
        
        templates = reasoning_templates.get(edge_type, [f"{edge_type.value}: {{source}} -> {{target}}"])
        template = random.choice(templates)
        
        return template.format(
            source=source_node.content[:20],
            target=target_node.content[:20],
            similarity=similarity
        )

class ConnectionOptimizer:
    """ì—°ê²° ìµœì í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.optimization_strategies = self._initialize_optimization_strategies()
        self.optimization_cache = {}
        self.max_cache_size = 1000
    
    def _initialize_optimization_strategies(self) -> Dict[str, Dict[str, Any]]:
        """ìµœì í™” ì „ëµ ì´ˆê¸°í™”"""
        return {
            "strength_optimization": {
                "weight": 0.4,
                "description": "ê°•ë„ ìµœì í™”"
            },
            "similarity_optimization": {
                "weight": 0.3,
                "description": "ìœ ì‚¬ë„ ìµœì í™”"
            },
            "connectivity_optimization": {
                "weight": 0.2,
                "description": "ì—°ê²°ì„± ìµœì í™”"
            },
            "balance_optimization": {
                "weight": 0.1,
                "description": "ê· í˜• ìµœì í™”"
            }
        }
    
    async def optimize_connections(self, graph: 'DynamicReasoningGraph') -> Dict[str, Any]:
        """ì—°ê²° ìµœì í™”"""
        logger.info(f"ì—°ê²° ìµœì í™” ì‹œì‘: {len(graph.edges)}ê°œ ì—£ì§€")
        
        optimization_results = {
            "optimized_edges": [],
            "removed_edges": [],
            "new_edges": [],
            "optimization_metrics": {}
        }
        
        # 1. ê°•ë„ ìµœì í™”
        strength_optimized = await self._optimize_edge_strengths(graph)
        optimization_results["optimized_edges"].extend(strength_optimized)
        
        # 2. ìœ ì‚¬ë„ ìµœì í™”
        similarity_optimized = await self._optimize_similarity_based_connections(graph)
        optimization_results["optimized_edges"].extend(similarity_optimized)
        
        # 3. ì—°ê²°ì„± ìµœì í™”
        connectivity_optimized = await self._optimize_connectivity(graph)
        optimization_results["optimized_edges"].extend(connectivity_optimized)
        
        # 4. ê· í˜• ìµœì í™”
        balance_optimized = await self._optimize_balance(graph)
        optimization_results["optimized_edges"].extend(balance_optimized)
        
        # ìµœì í™” ë©”íŠ¸ë¦­ ê³„ì‚°
        optimization_results["optimization_metrics"] = await self._calculate_optimization_metrics(graph)
        
        logger.info(f"ì—°ê²° ìµœì í™” ì™„ë£Œ: {len(optimization_results['optimized_edges'])}ê°œ ìµœì í™”ë¨")
        return optimization_results
    
    async def _optimize_edge_strengths(self, graph: 'DynamicReasoningGraph') -> List[str]:
        """ì—£ì§€ ê°•ë„ ìµœì í™”"""
        optimized_edges = []
        
        for edge_id, edge in graph.edges.items():
            # ê°•ë„ê°€ ë„ˆë¬´ ë‚®ê±°ë‚˜ ë†’ì€ ì—£ì§€ ì¡°ì •
            if edge.strength < 0.3:
                # ë„ˆë¬´ ì•½í•œ ì—£ì§€ ê°•í™”
                edge.strength = min(0.8, edge.strength * 1.5)
                optimized_edges.append(edge_id)
            elif edge.strength > 0.9:
                # ë„ˆë¬´ ê°•í•œ ì—£ì§€ ì¡°ì •
                edge.strength = max(0.7, edge.strength * 0.9)
                optimized_edges.append(edge_id)
        
        return optimized_edges
    
    async def _optimize_similarity_based_connections(self, graph: 'DynamicReasoningGraph') -> List[str]:
        """ìœ ì‚¬ë„ ê¸°ë°˜ ì—°ê²° ìµœì í™”"""
        optimized_edges = []
        
        for edge_id, edge in graph.edges.items():
            # ìœ ì‚¬ë„ì™€ ê°•ë„ì˜ ë¶ˆì¼ì¹˜ ì¡°ì •
            if edge.semantic_similarity > 0.7 and edge.strength < 0.6:
                # ë†’ì€ ìœ ì‚¬ë„ì§€ë§Œ ë‚®ì€ ê°•ë„ì¸ ê²½ìš° ê°•í™”
                edge.strength = min(0.9, edge.strength * 1.3)
                optimized_edges.append(edge_id)
            elif edge.semantic_similarity < 0.3 and edge.strength > 0.7:
                # ë‚®ì€ ìœ ì‚¬ë„ì§€ë§Œ ë†’ì€ ê°•ë„ì¸ ê²½ìš° ì¡°ì •
                edge.strength = max(0.4, edge.strength * 0.8)
                optimized_edges.append(edge_id)
        
        return optimized_edges
    
    async def _optimize_connectivity(self, graph: 'DynamicReasoningGraph') -> List[str]:
        """ì—°ê²°ì„± ìµœì í™”"""
        optimized_edges = []
        
        # ê³ ë¦½ëœ ë…¸ë“œ ì°¾ê¸°
        isolated_nodes = []
        for node_id in graph.nodes:
            has_connection = False
            for edge in graph.edges.values():
                if edge.source_node == node_id or edge.target_node == node_id:
                    has_connection = True
                    break
            
            if not has_connection:
                isolated_nodes.append(node_id)
        
        # ê³ ë¦½ëœ ë…¸ë“œë“¤ì„ ë‹¤ë¥¸ ë…¸ë“œì™€ ì—°ê²°
        for node_id in isolated_nodes:
            node = graph.nodes.get(node_id)
            if node:
                # ê°€ì¥ ìœ ì‚¬í•œ ë…¸ë“œ ì°¾ê¸°
                best_similarity = 0.0
                best_target = None
                
                for target_id, target_node in graph.nodes.items():
                    if target_id != node_id:
                        # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚°
                        similarity = await self._calculate_simple_similarity(node.content, target_node.content)
                        if similarity > best_similarity:
                            best_similarity = similarity
                            best_target = target_id
                
                # ìƒˆë¡œìš´ ì—£ì§€ ìƒì„±
                if best_target and best_similarity > 0.2:
                    new_edge_id = f"optimized_edge_{node_id}_{best_target}"
                    new_edge = DynamicReasoningEdge(
                        edge_id=new_edge_id,
                        source_node=node_id,
                        target_node=best_target,
                        edge_type=EdgeType.SUPPORTS,
                        strength=best_similarity,
                        reasoning=f"ì—°ê²°ì„± ìµœì í™”ë¥¼ í†µí•œ ì—°ê²° (ìœ ì‚¬ë„: {best_similarity:.2f})",
                        semantic_similarity=best_similarity
                    )
                    graph.edges[new_edge_id] = new_edge
                    optimized_edges.append(new_edge_id)
        
        return optimized_edges
    
    async def _optimize_balance(self, graph: 'DynamicReasoningGraph') -> List[str]:
        """ê· í˜• ìµœì í™”"""
        optimized_edges = []
        
        # ë…¸ë“œë³„ ì—°ê²° ìˆ˜ ê³„ì‚°
        node_connections = defaultdict(int)
        for edge in graph.edges.values():
            node_connections[edge.source_node] += 1
            node_connections[edge.target_node] += 1
        
        # ì—°ê²°ì´ ë„ˆë¬´ ë§ê±°ë‚˜ ì ì€ ë…¸ë“œ ì¡°ì •
        avg_connections = sum(node_connections.values()) / len(node_connections) if node_connections else 0
        
        for node_id, connection_count in node_connections.items():
            if connection_count > avg_connections * 2:
                # ì—°ê²°ì´ ë„ˆë¬´ ë§ì€ ë…¸ë“œì˜ ì¼ë¶€ ì—°ê²° ê°•ë„ ê°ì†Œ
                for edge in graph.edges.values():
                    if edge.source_node == node_id or edge.target_node == node_id:
                        if edge.strength > 0.6:
                            edge.strength *= 0.9
                            optimized_edges.append(edge.edge_id)
        
        return optimized_edges
    
    async def _calculate_optimization_metrics(self, graph: 'DynamicReasoningGraph') -> Dict[str, float]:
        """ìµœì í™” ë©”íŠ¸ë¦­ ê³„ì‚°"""
        metrics = {}
        
        # í‰ê·  ê°•ë„
        edge_strengths = [edge.strength for edge in graph.edges.values()]
        metrics["avg_strength"] = sum(edge_strengths) / len(edge_strengths) if edge_strengths else 0.0
        
        # í‰ê·  ìœ ì‚¬ë„
        edge_similarities = [edge.semantic_similarity for edge in graph.edges.values()]
        metrics["avg_similarity"] = sum(edge_similarities) / len(edge_similarities) if edge_similarities else 0.0
        
        # ì—°ê²°ì„±
        total_nodes = len(graph.nodes)
        total_edges = len(graph.edges)
        metrics["connectivity"] = total_edges / (total_nodes * (total_nodes - 1) / 2) if total_nodes > 1 else 0.0
        
        # ê°•ë„ í‘œì¤€í¸ì°¨ (ë‚®ì„ìˆ˜ë¡ ê· í˜•ì )
        if edge_strengths:
            strength_std = np.std(edge_strengths)
            metrics["strength_balance"] = max(0.0, 1.0 - strength_std)
        else:
            metrics["strength_balance"] = 0.0
        
        return metrics
    
    async def _calculate_simple_similarity(self, text1: str, text2: str) -> float:
        """ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚°"""
        keywords1 = set(re.findall(r'\w+', text1.lower()))
        keywords2 = set(re.findall(r'\w+', text2.lower()))
        
        if not keywords1 or not keywords2:
            return 0.0
        
        intersection = len(keywords1.intersection(keywords2))
        union = len(keywords1.union(keywords2))
        
        return intersection / union if union > 0 else 0.0

class ConnectionValidator:
    """ì—°ê²° ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.validation_criteria = self._initialize_validation_criteria()
    
    def _initialize_validation_criteria(self) -> Dict[str, Dict[str, Any]]:
        """ê²€ì¦ ê¸°ì¤€ ì´ˆê¸°í™”"""
        return {
            "connection_quality": {
                "weight": 0.3,
                "description": "ì—°ê²° í’ˆì§ˆ"
            },
            "semantic_coherence": {
                "weight": 0.3,
                "description": "ì˜ë¯¸ì  ì¼ê´€ì„±"
            },
            "structural_balance": {
                "weight": 0.2,
                "description": "êµ¬ì¡°ì  ê· í˜•"
            },
            "connection_efficiency": {
                "weight": 0.2,
                "description": "ì—°ê²° íš¨ìœ¨ì„±"
            }
        }
    
    async def validate_connections(self, graph: 'DynamicReasoningGraph') -> Dict[str, Any]:
        """ì—°ê²° ê²€ì¦"""
        logger.info(f"ì—°ê²° ê²€ì¦ ì‹œì‘: {len(graph.edges)}ê°œ ì—£ì§€")
        
        # 1. ì—°ê²° í’ˆì§ˆ í‰ê°€
        connection_quality = await self._evaluate_connection_quality(graph)
        
        # 2. ì˜ë¯¸ì  ì¼ê´€ì„± í‰ê°€
        semantic_coherence = await self._evaluate_semantic_coherence(graph)
        
        # 3. êµ¬ì¡°ì  ê· í˜• í‰ê°€
        structural_balance = await self._evaluate_structural_balance(graph)
        
        # 4. ì—°ê²° íš¨ìœ¨ì„± í‰ê°€
        connection_efficiency = await self._evaluate_connection_efficiency(graph)
        
        # ì¢…í•© ê²€ì¦ ì ìˆ˜
        overall_score = (
            connection_quality * self.validation_criteria["connection_quality"]["weight"] +
            semantic_coherence * self.validation_criteria["semantic_coherence"]["weight"] +
            structural_balance * self.validation_criteria["structural_balance"]["weight"] +
            connection_efficiency * self.validation_criteria["connection_efficiency"]["weight"]
        )
        
        return {
            "overall_score": overall_score,
            "connection_quality": connection_quality,
            "semantic_coherence": semantic_coherence,
            "structural_balance": structural_balance,
            "connection_efficiency": connection_efficiency,
            "validation_details": {
                "total_edges": len(graph.edges),
                "avg_strength": sum(edge.strength for edge in graph.edges.values()) / len(graph.edges) if graph.edges else 0.0,
                "avg_similarity": sum(edge.semantic_similarity for edge in graph.edges.values()) / len(graph.edges) if graph.edges else 0.0,
                "connectivity": len(graph.edges) / (len(graph.nodes) * (len(graph.nodes) - 1) / 2) if len(graph.nodes) > 1 else 0.0
            }
        }
    
    async def _evaluate_connection_quality(self, graph: 'DynamicReasoningGraph') -> float:
        """ì—°ê²° í’ˆì§ˆ í‰ê°€"""
        if not graph.edges:
            return 0.0
        
        # ê°•ë„ì™€ ìœ ì‚¬ë„ì˜ ì¼ì¹˜ë„
        quality_scores = []
        for edge in graph.edges.values():
            # ê°•ë„ì™€ ìœ ì‚¬ë„ê°€ ì¼ì¹˜í•˜ëŠ” ì •ë„
            strength_similarity_match = 1.0 - abs(edge.strength - edge.semantic_similarity)
            quality_scores.append(strength_similarity_match)
        
        return sum(quality_scores) / len(quality_scores)
    
    async def _evaluate_semantic_coherence(self, graph: 'DynamicReasoningGraph') -> float:
        """ì˜ë¯¸ì  ì¼ê´€ì„± í‰ê°€"""
        if not graph.edges:
            return 0.0
        
        # í‰ê·  ì˜ë¯¸ì  ìœ ì‚¬ë„
        similarities = [edge.semantic_similarity for edge in graph.edges.values()]
        return sum(similarities) / len(similarities)
    
    async def _evaluate_structural_balance(self, graph: 'DynamicReasoningGraph') -> float:
        """êµ¬ì¡°ì  ê· í˜• í‰ê°€"""
        if not graph.nodes:
            return 0.0
        
        # ë…¸ë“œë³„ ì—°ê²° ìˆ˜ì˜ í‘œì¤€í¸ì°¨ (ë‚®ì„ìˆ˜ë¡ ê· í˜•ì )
        node_connections = defaultdict(int)
        for edge in graph.edges.values():
            node_connections[edge.source_node] += 1
            node_connections[edge.target_node] += 1
        
        if not node_connections:
            return 0.0
        
        connection_counts = list(node_connections.values())
        connection_std = np.std(connection_counts)
        avg_connections = sum(connection_counts) / len(connection_counts)
        
        # í‘œì¤€í¸ì°¨ê°€ ì‘ì„ìˆ˜ë¡ ê· í˜•ì 
        balance_score = max(0.0, 1.0 - (connection_std / avg_connections) if avg_connections > 0 else 0.0)
        return balance_score
    
    async def _evaluate_connection_efficiency(self, graph: 'DynamicReasoningGraph') -> float:
        """ì—°ê²° íš¨ìœ¨ì„± í‰ê°€"""
        if not graph.nodes:
            return 0.0
        
        # ì—°ê²°ì„± ë¹„ìœ¨ (ì‹¤ì œ ì—°ê²° / ê°€ëŠ¥í•œ ìµœëŒ€ ì—°ê²°)
        total_nodes = len(graph.nodes)
        total_edges = len(graph.edges)
        max_possible_edges = total_nodes * (total_nodes - 1) / 2
        
        if max_possible_edges == 0:
            return 0.0
        
        connectivity_ratio = total_edges / max_possible_edges
        
        # ì ì ˆí•œ ì—°ê²°ì„± (ë„ˆë¬´ ë§ê±°ë‚˜ ì ìœ¼ë©´ ë¹„íš¨ìœ¨ì )
        optimal_connectivity = 0.3  # 30% ì—°ê²°ì„±ì´ ìµœì 
        efficiency = 1.0 - abs(connectivity_ratio - optimal_connectivity) / optimal_connectivity
        efficiency = max(0.0, efficiency)
        
        return efficiency

async def test_semantic_connection_system():
    """ì˜ë¯¸ ê¸°ë°˜ ë…¸ë“œ ì—°ê²° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("=== ì˜ë¯¸ ê¸°ë°˜ ë…¸ë“œ ì—°ê²° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘ (Phase 1-3 Week 3 Day 5) ===")
    
    # í…ŒìŠ¤íŠ¸ ê·¸ë˜í”„ ìƒì„±
    graph = DynamicReasoningGraph(graph_id="test_graph")
    
    # ì´ˆê¸° ë…¸ë“œë“¤ ìƒì„±
    initial_nodes = {
        "node1": DynamicReasoningNode("node1", NodeType.PREMISE, "ìœ¤ë¦¬ì  í–‰ë™ì€ ì˜³ë‹¤", 0.8, "test"),
        "node2": DynamicReasoningNode("node2", NodeType.INFERENCE, "ìœ¤ë¦¬ì  í–‰ë™ ë¶„ì„: ë„ë•ì  ì˜ë¬´", 0.7, "test"),
        "node3": DynamicReasoningNode("node3", NodeType.CONCLUSION, "ìœ¤ë¦¬ì  í–‰ë™ í•„ìš”: ìµœì¢… íŒë‹¨", 0.9, "test"),
        "node4": DynamicReasoningNode("node4", NodeType.EVIDENCE, "ìœ¤ë¦¬ì  í–‰ë™ì˜ ì¦ê±°: ì‚¬íšŒì  ì´ìµ", 0.8, "test"),
        "node5": DynamicReasoningNode("node5", NodeType.COUNTER_ARGUMENT, "ìœ¤ë¦¬ì  í–‰ë™ì˜ ë°˜ë¡ : ììœ  ì œí•œ", 0.6, "test")
    }
    
    graph.nodes = initial_nodes
    
    print(f"\nğŸ“Š ì´ˆê¸° ê·¸ë˜í”„ ìƒíƒœ:")
    print(f"  â€¢ ë…¸ë“œ ìˆ˜: {len(graph.nodes)}")
    print(f"  â€¢ ì—£ì§€ ìˆ˜: {len(graph.edges)}")
    
    # 1. ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸
    similarity_engine = SemanticSimilarityEngine()
    
    print(f"\nğŸ” ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸:")
    test_pairs = [
        ("node1", "node2"),
        ("node1", "node3"),
        ("node2", "node3"),
        ("node4", "node5")
    ]
    
    for node1_id, node2_id in test_pairs:
        node1 = graph.nodes[node1_id]
        node2 = graph.nodes[node2_id]
        similarity = await similarity_engine.calculate_semantic_similarity(node1.content, node2.content)
        print(f"  â€¢ {node1_id} â†” {node2_id}: {similarity:.3f}")
    
    # 2. ì˜ë¯¸ ê¸°ë°˜ ì—°ê²° ìƒì„± í…ŒìŠ¤íŠ¸
    connection_generator = SemanticConnectionGenerator()
    
    print(f"\nğŸ”— ì˜ë¯¸ ê¸°ë°˜ ì—°ê²° ìƒì„± í…ŒìŠ¤íŠ¸:")
    for node_id, node in graph.nodes.items():
        connections = await connection_generator.generate_semantic_connections(graph, node, 3)
        
        if connections:
            print(f"  â€¢ {node_id} ì—°ê²° ìƒì„±: {len(connections)}ê°œ")
            for connection in connections:
                graph.edges[connection.edge_id] = connection
                print(f"    - {connection.source_node} â†’ {connection.target_node} ({connection.edge_type.value}, ê°•ë„: {connection.strength:.2f})")
    
    # 3. ì—°ê²° ìµœì í™” í…ŒìŠ¤íŠ¸
    optimizer = ConnectionOptimizer()
    
    print(f"\nâš™ï¸ ì—°ê²° ìµœì í™” í…ŒìŠ¤íŠ¸:")
    optimization_results = await optimizer.optimize_connections(graph)
    
    print(f"  â€¢ ìµœì í™”ëœ ì—£ì§€ ìˆ˜: {len(optimization_results['optimized_edges'])}")
    print(f"  â€¢ ìµœì í™” ë©”íŠ¸ë¦­:")
    for metric, value in optimization_results['optimization_metrics'].items():
        print(f"    - {metric}: {value:.3f}")
    
    # 4. ì—°ê²° ê²€ì¦ í…ŒìŠ¤íŠ¸
    validator = ConnectionValidator()
    
    print(f"\nğŸ“Š ì—°ê²° ê²€ì¦ í…ŒìŠ¤íŠ¸:")
    validation_results = await validator.validate_connections(graph)
    
    print(f"  â€¢ ì¢…í•© ê²€ì¦ ì ìˆ˜: {validation_results['overall_score']:.3f}")
    print(f"  â€¢ ì—°ê²° í’ˆì§ˆ: {validation_results['connection_quality']:.3f}")
    print(f"  â€¢ ì˜ë¯¸ì  ì¼ê´€ì„±: {validation_results['semantic_coherence']:.3f}")
    print(f"  â€¢ êµ¬ì¡°ì  ê· í˜•: {validation_results['structural_balance']:.3f}")
    print(f"  â€¢ ì—°ê²° íš¨ìœ¨ì„±: {validation_results['connection_efficiency']:.3f}")
    
    # 5. ìµœì¢… ê·¸ë˜í”„ ìƒíƒœ
    print(f"\nğŸ“Š ìµœì¢… ê·¸ë˜í”„ ìƒíƒœ:")
    print(f"  â€¢ ë…¸ë“œ ìˆ˜: {len(graph.nodes)}")
    print(f"  â€¢ ì—£ì§€ ìˆ˜: {len(graph.edges)} (ì¦ê°€: {len(graph.edges) - 0})")
    
    # ì—£ì§€ ìœ í˜•ë³„ ë¶„í¬
    edge_types = defaultdict(int)
    for edge in graph.edges.values():
        edge_types[edge.edge_type.value] += 1
    
    print(f"  â€¢ ì—£ì§€ ìœ í˜•ë³„ ë¶„í¬:")
    for edge_type, count in edge_types.items():
        print(f"    - {edge_type}: {count}ê°œ")
    
    # í‰ê·  ê°•ë„ì™€ ìœ ì‚¬ë„
    if graph.edges:
        avg_strength = sum(edge.strength for edge in graph.edges.values()) / len(graph.edges)
        avg_similarity = sum(edge.semantic_similarity for edge in graph.edges.values()) / len(graph.edges)
        print(f"  â€¢ í‰ê·  ê°•ë„: {avg_strength:.3f}")
        print(f"  â€¢ í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.3f}")
    
    print(f"\n{'='*70}")
    print("=== ì˜ë¯¸ ê¸°ë°˜ ë…¸ë“œ ì—°ê²° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ (Phase 1-3 Week 3 Day 5) ===")
    print("âœ… Day 5 ëª©í‘œ ë‹¬ì„±: ì˜ë¯¸ ê¸°ë°˜ ë…¸ë“œ ì—°ê²° ì‹œìŠ¤í…œ êµ¬í˜„")
    print("âœ… ê³ ê¸‰ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° êµ¬í˜„")
    print("âœ… ì˜ë¯¸ ê¸°ë°˜ ì—°ê²° ìƒì„± êµ¬í˜„")
    print("âœ… ì—°ê²° ìµœì í™” êµ¬í˜„")
    print("âœ… ì—°ê²° ê²€ì¦ êµ¬í˜„")

if __name__ == "__main__":
    asyncio.run(test_semantic_connection_system()) 