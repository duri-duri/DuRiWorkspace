"""
DuRiCore Phase 2.5: ìœ¤ë¦¬ì  íŒë‹¨ ì‹œìŠ¤í…œ (Ethical Judgment System)
- ë³µì¡í•œ ìœ¤ë¦¬ì  ìƒí™© ë¶„ì„
- ë„ë•ì  íŒë‹¨ ë° ì˜ì‚¬ê²°ì •
- ìœ¤ë¦¬ì  ê°ˆë“± í•´ê²°
- ìœ¤ë¦¬ì  ì„±ìˆ™ë„ ì¸¡ì • ë° ê°œì„ 
"""

import asyncio
import logging
import random
import time
from datetime import datetime, timedelta
from dataclasses import dataclass, field, asdict
from enum import Enum
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

class EthicalPrinciple(Enum):
    """ìœ¤ë¦¬ ì›ì¹™"""
    BENEFICENCE = "beneficence"              # ì„ í–‰ (ì´ìµì„ ì£¼ëŠ” í–‰ë™)
    NON_MALEFICENCE = "non_maleficence"      # ë¬´í•´ (í•´ë¥¼ ë¼ì¹˜ì§€ ì•ŠëŠ” í–‰ë™)
    AUTONOMY = "autonomy"                    # ììœ¨ì„± (ê°œì¸ì˜ ììœ ë¡œìš´ ì„ íƒ)
    JUSTICE = "justice"                      # ì •ì˜ (ê³µì •í•œ ë¶„ë°°)
    HONESTY = "honesty"                      # ì •ì§
    RESPECT = "respect"                      # ì¡´ì¤‘
    RESPONSIBILITY = "responsibility"        # ì±…ì„
    FAIRNESS = "fairness"                    # ê³µì •ì„±
    COMPASSION = "compassion"                # ë™ì •ì‹¬
    INTEGRITY = "integrity"                  # ì§„ì‹¤ì„±
    PRIVACY = "privacy"                      # ì‚¬ìƒí™œ ë³´í˜¸
    TRANSPARENCY = "transparency"            # íˆ¬ëª…ì„±

class EthicalDilemmaType(Enum):
    """ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ìœ í˜•"""
    CONFLICT_OF_PRINCIPLES = "conflict_of_principles"  # ì›ì¹™ ê°„ ê°ˆë“±
    UTILITARIAN_VS_DEONTOLOGICAL = "utilitarian_vs_deontological"  # ê²°ê³¼ì£¼ì˜ vs ì˜ë¬´ë¡ 
    INDIVIDUAL_VS_COLLECTIVE = "individual_vs_collective"  # ê°œì¸ vs ì§‘ë‹¨
    SHORT_TERM_VS_LONG_TERM = "short_term_vs_long_term"  # ë‹¨ê¸° vs ì¥ê¸°
    RIGHTS_VS_UTILITY = "rights_vs_utility"  # ê¶Œë¦¬ vs íš¨ìš©

class JudgmentConfidence(Enum):
    """íŒë‹¨ ì‹ ë¢°ë„"""
    VERY_LOW = "very_low"       # ë§¤ìš° ë‚®ìŒ (0.0-0.2)
    LOW = "low"                 # ë‚®ìŒ (0.2-0.4)
    MEDIUM = "medium"           # ë³´í†µ (0.4-0.6)
    HIGH = "high"               # ë†’ìŒ (0.6-0.8)
    VERY_HIGH = "very_high"     # ë§¤ìš° ë†’ìŒ (0.8-1.0)

class EthicalMaturityLevel(Enum):
    """ìœ¤ë¦¬ì  ì„±ìˆ™ë„ ìˆ˜ì¤€"""
    PRE_CONVENTIONAL = "pre_conventional"    # ì „ì¸ìŠµì  (0.0-0.3)
    CONVENTIONAL = "conventional"            # ì¸ìŠµì  (0.3-0.6)
    POST_CONVENTIONAL = "post_conventional"  # í›„ì¸ìŠµì  (0.6-0.9)
    UNIVERSAL = "universal"                  # ë³´í¸ì  (0.9-1.0)

@dataclass
class EthicalSituation:
    """ìœ¤ë¦¬ì  ìƒí™©"""
    situation_id: str
    description: str
    involved_principles: List[EthicalPrinciple]
    stakeholders: List[str] = field(default_factory=list)
    potential_consequences: List[str] = field(default_factory=list)
    dilemma_type: Optional[EthicalDilemmaType] = None
    complexity_level: float = 0.5  # 0.0-1.0
    urgency_level: float = 0.5     # 0.0-1.0
    context: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)

@dataclass
class EthicalJudgment:
    """ìœ¤ë¦¬ì  íŒë‹¨"""
    judgment_id: str
    situation_id: str
    decision: str
    reasoning: str
    confidence: JudgmentConfidence
    ethical_score: float  # 0.0-1.0
    principles_considered: List[EthicalPrinciple] = field(default_factory=list)
    alternatives_considered: List[str] = field(default_factory=list)
    consequences_analyzed: List[str] = field(default_factory=list)
    moral_justification: str = ""
    created_at: datetime = field(default_factory=datetime.now)

@dataclass
class EthicalConflict:
    """ìœ¤ë¦¬ì  ê°ˆë“±"""
    conflict_id: str
    situation_id: str
    conflicting_principles: List[EthicalPrinciple]
    conflict_intensity: float  # 0.0-1.0
    resolution_approach: str
    compromise_solution: Optional[str] = None
    created_at: datetime = field(default_factory=datetime.now)

@dataclass
class EthicalMaturityMetrics:
    """ìœ¤ë¦¬ì  ì„±ìˆ™ë„ ì¸¡ì • ì§€í‘œ"""
    principle_understanding: float = 0.5      # ì›ì¹™ ì´í•´ë„ (0.0-1.0)
    conflict_resolution: float = 0.5          # ê°ˆë“± í•´ê²° ëŠ¥ë ¥ (0.0-1.0)
    moral_reasoning: float = 0.5              # ë„ë•ì  ì¶”ë¡  (0.0-1.0)
    ethical_consistency: float = 0.5          # ìœ¤ë¦¬ì  ì¼ê´€ì„± (0.0-1.0)
    moral_imagination: float = 0.5            # ë„ë•ì  ìƒìƒë ¥ (0.0-1.0)
    
    @property
    def overall_ethical_maturity(self) -> float:
        """ì „ì²´ ìœ¤ë¦¬ì  ì„±ìˆ™ë„"""
        return (self.principle_understanding + self.conflict_resolution + 
                self.moral_reasoning + self.ethical_consistency + 
                self.moral_imagination) / 5.0

@dataclass
class EthicalJudgmentState:
    """ìœ¤ë¦¬ì  íŒë‹¨ ìƒíƒœ"""
    maturity_metrics: EthicalMaturityMetrics
    ethical_situations: List[EthicalSituation] = field(default_factory=list)
    ethical_judgments: List[EthicalJudgment] = field(default_factory=list)
    ethical_conflicts: List[EthicalConflict] = field(default_factory=list)
    judgment_history: List[Dict[str, Any]] = field(default_factory=list)
    last_update: datetime = field(default_factory=datetime.now)

class EthicalJudgmentSystem:
    """ìœ¤ë¦¬ì  íŒë‹¨ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.judgment_state = EthicalJudgmentState(
            maturity_metrics=EthicalMaturityMetrics()
        )
        self.principle_hierarchy = {}
        self.conflict_resolution_strategies = {}
        self.moral_frameworks = {}
        logger.info("ğŸ§  ìœ¤ë¦¬ì  íŒë‹¨ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def analyze_ethical_situation(self, situation_data: Dict[str, Any]) -> EthicalSituation:
        """ìœ¤ë¦¬ì  ìƒí™© ë¶„ì„"""
        situation_id = f"situation_{int(time.time())}"
        
        # ê´€ë ¨ ìœ¤ë¦¬ ì›ì¹™ ì‹ë³„
        involved_principles = self._identify_involved_principles(situation_data)
        
        # ì´í•´ê´€ê³„ì ì‹ë³„
        stakeholders = self._identify_stakeholders(situation_data)
        
        # ì ì¬ì  ê²°ê³¼ ë¶„ì„
        potential_consequences = self._analyze_potential_consequences(situation_data)
        
        # ë”œë ˆë§ˆ ìœ í˜• ì‹ë³„
        dilemma_type = self._identify_dilemma_type(situation_data, involved_principles)
        
        # ë³µì¡ì„± ë° ê¸´ê¸‰ì„± í‰ê°€
        complexity_level = self._assess_complexity(situation_data)
        urgency_level = self._assess_urgency(situation_data)
        
        situation = EthicalSituation(
            situation_id=situation_id,
            description=situation_data.get('description', ''),
            involved_principles=involved_principles,
            stakeholders=stakeholders,
            potential_consequences=potential_consequences,
            dilemma_type=dilemma_type,
            complexity_level=complexity_level,
            urgency_level=urgency_level,
            context=situation_data.get('context', {})
        )
        
        self.judgment_state.ethical_situations.append(situation)
        await self._update_principle_understanding_metrics(situation)
        
        logger.info(f"ğŸ” ìœ¤ë¦¬ì  ìƒí™© ë¶„ì„ ì™„ë£Œ: {len(involved_principles)}ê°œ ì›ì¹™ ê´€ë ¨")
        return situation
    
    async def make_ethical_judgment(self, situation: EthicalSituation) -> EthicalJudgment:
        """ìœ¤ë¦¬ì  íŒë‹¨ ìˆ˜í–‰"""
        judgment_id = f"judgment_{int(time.time())}"
        
        # ëŒ€ì•ˆ ìƒì„±
        alternatives = await self._generate_ethical_alternatives(situation)
        
        # ê° ëŒ€ì•ˆ í‰ê°€
        evaluated_alternatives = await self._evaluate_alternatives(alternatives, situation)
        
        # ìµœì  íŒë‹¨ ì„ íƒ
        best_decision = self._select_best_decision(evaluated_alternatives)
        
        # íŒë‹¨ ê·¼ê±° ìƒì„±
        reasoning = await self._generate_ethical_reasoning(situation, best_decision)
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self._calculate_judgment_confidence(situation, best_decision)
        
        # ìœ¤ë¦¬ì  ì ìˆ˜ ê³„ì‚°
        ethical_score = self._calculate_ethical_score(situation, best_decision)
        
        # ë„ë•ì  ì •ë‹¹í™”
        moral_justification = await self._generate_moral_justification(situation, best_decision)
        
        judgment = EthicalJudgment(
            judgment_id=judgment_id,
            situation_id=situation.situation_id,
            decision=best_decision['decision'],
            reasoning=reasoning,
            confidence=confidence,
            ethical_score=ethical_score,
            principles_considered=situation.involved_principles,
            alternatives_considered=[alt['decision'] for alt in evaluated_alternatives],
            consequences_analyzed=situation.potential_consequences,
            moral_justification=moral_justification
        )
        
        self.judgment_state.ethical_judgments.append(judgment)
        await self._update_moral_reasoning_metrics(judgment)
        
        logger.info(f"âš–ï¸ ìœ¤ë¦¬ì  íŒë‹¨ ì™„ë£Œ: {confidence.value} ì‹ ë¢°ë„")
        return judgment
    
    async def resolve_ethical_conflict(self, situation: EthicalSituation) -> EthicalConflict:
        """ìœ¤ë¦¬ì  ê°ˆë“± í•´ê²°"""
        conflict_id = f"conflict_{int(time.time())}"
        
        # ê°ˆë“±í•˜ëŠ” ì›ì¹™ ì‹ë³„
        conflicting_principles = self._identify_conflicting_principles(situation)
        
        # ê°ˆë“± ê°•ë„ ê³„ì‚°
        conflict_intensity = self._calculate_conflict_intensity(conflicting_principles)
        
        # í•´ê²° ì ‘ê·¼ë²• ì„ íƒ
        resolution_approach = self._select_resolution_approach(conflicting_principles)
        
        # íƒ€í˜‘ì•ˆ ìƒì„±
        compromise_solution = await self._generate_compromise_solution(situation, conflicting_principles)
        
        conflict = EthicalConflict(
            conflict_id=conflict_id,
            situation_id=situation.situation_id,
            conflicting_principles=conflicting_principles,
            conflict_intensity=conflict_intensity,
            resolution_approach=resolution_approach,
            compromise_solution=compromise_solution
        )
        
        self.judgment_state.ethical_conflicts.append(conflict)
        await self._update_conflict_resolution_metrics(conflict)
        
        logger.info(f"ğŸ¤ ìœ¤ë¦¬ì  ê°ˆë“± í•´ê²°: {resolution_approach}")
        return conflict
    
    async def assess_ethical_maturity(self) -> Dict[str, Any]:
        """ìœ¤ë¦¬ì  ì„±ìˆ™ë„ í‰ê°€"""
        if not self.judgment_state.ethical_judgments:
            return {"maturity_level": "unknown", "score": 0.0, "areas": []}
        
        # ì„±ìˆ™ë„ ì§€í‘œ ê³„ì‚°
        principle_understanding = self._calculate_principle_understanding()
        conflict_resolution = self._calculate_conflict_resolution_ability()
        moral_reasoning = self._calculate_moral_reasoning_ability()
        ethical_consistency = self._calculate_ethical_consistency()
        moral_imagination = self._calculate_moral_imagination()
        
        # ì „ì²´ ì„±ìˆ™ë„ ì ìˆ˜
        maturity_score = (principle_understanding + conflict_resolution + 
                         moral_reasoning + ethical_consistency + 
                         moral_imagination) / 5.0
        
        # ì„±ìˆ™ë„ ìˆ˜ì¤€ ê²°ì •
        if maturity_score >= 0.9:
            maturity_level = "universal"
        elif maturity_score >= 0.6:
            maturity_level = "post_conventional"
        elif maturity_score >= 0.3:
            maturity_level = "conventional"
        else:
            maturity_level = "pre_conventional"
        
        # ê°œì„  ì˜ì—­ ì‹ë³„
        improvement_areas = self._identify_ethical_improvement_areas({
            "principle_understanding": principle_understanding,
            "conflict_resolution": conflict_resolution,
            "moral_reasoning": moral_reasoning,
            "ethical_consistency": ethical_consistency,
            "moral_imagination": moral_imagination
        })
        
        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self.judgment_state.maturity_metrics.principle_understanding = principle_understanding
        self.judgment_state.maturity_metrics.conflict_resolution = conflict_resolution
        self.judgment_state.maturity_metrics.moral_reasoning = moral_reasoning
        self.judgment_state.maturity_metrics.ethical_consistency = ethical_consistency
        self.judgment_state.maturity_metrics.moral_imagination = moral_imagination
        
        return {
            "maturity_level": maturity_level,
            "score": maturity_score,
            "areas": improvement_areas,
            "detailed_scores": {
                "principle_understanding": principle_understanding,
                "conflict_resolution": conflict_resolution,
                "moral_reasoning": moral_reasoning,
                "ethical_consistency": ethical_consistency,
                "moral_imagination": moral_imagination
            }
        }
    
    async def generate_ethical_report(self) -> Dict[str, Any]:
        """ìœ¤ë¦¬ì  íŒë‹¨ ë³´ê³ ì„œ ìƒì„±"""
        # í˜„ì¬ ìƒíƒœ ë¶„ì„
        current_state = self.get_judgment_state()
        
        # ì„±ìˆ™ë„ í‰ê°€
        maturity = await self.assess_ethical_maturity()
        
        # íŒë‹¨ í†µê³„
        judgment_stats = self._calculate_judgment_statistics()
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­
        recommendations = await self._generate_ethical_recommendations()
        
        return {
            "current_state": current_state,
            "maturity": maturity,
            "judgment_statistics": judgment_stats,
            "recommendations": recommendations,
            "timestamp": datetime.now().isoformat()
        }
    
    def get_judgment_state(self) -> Dict[str, Any]:
        """ìœ¤ë¦¬ì  íŒë‹¨ ìƒíƒœ ë°˜í™˜"""
        return {
            "maturity_metrics": asdict(self.judgment_state.maturity_metrics),
            "situations_analyzed": len(self.judgment_state.ethical_situations),
            "judgments_made": len(self.judgment_state.ethical_judgments),
            "conflicts_resolved": len(self.judgment_state.ethical_conflicts),
            "last_update": self.judgment_state.last_update.isoformat()
        }
    
    # ë‚´ë¶€ ë¶„ì„ ë©”ì„œë“œë“¤
    def _identify_involved_principles(self, situation_data: Dict[str, Any]) -> List[EthicalPrinciple]:
        """ê´€ë ¨ ìœ¤ë¦¬ ì›ì¹™ ì‹ë³„"""
        principles = []
        
        # ìƒí™©ë³„ ì›ì¹™ ë§¤í•‘
        principle_mapping = {
            'privacy': [EthicalPrinciple.PRIVACY, EthicalPrinciple.RESPECT],
            'fairness': [EthicalPrinciple.FAIRNESS, EthicalPrinciple.JUSTICE],
            'honesty': [EthicalPrinciple.HONESTY, EthicalPrinciple.INTEGRITY],
            'harm': [EthicalPrinciple.NON_MALEFICENCE, EthicalPrinciple.BENEFICENCE],
            'autonomy': [EthicalPrinciple.AUTONOMY, EthicalPrinciple.RESPECT],
            'responsibility': [EthicalPrinciple.RESPONSIBILITY, EthicalPrinciple.INTEGRITY],
            'compassion': [EthicalPrinciple.COMPASSION, EthicalPrinciple.BENEFICENCE],
            'transparency': [EthicalPrinciple.TRANSPARENCY, EthicalPrinciple.HONESTY]
        }
        
        # ìƒí™© í‚¤ì›Œë“œ ë¶„ì„
        description = situation_data.get('description', '').lower()
        for keyword, related_principles in principle_mapping.items():
            if keyword in description:
                principles.extend(related_principles)
        
        # ê¸°ë³¸ ì›ì¹™ ì¶”ê°€
        if not principles:
            principles = [EthicalPrinciple.RESPECT, EthicalPrinciple.FAIRNESS]
        
        return list(set(principles))  # ì¤‘ë³µ ì œê±°
    
    def _identify_stakeholders(self, situation_data: Dict[str, Any]) -> List[str]:
        """ì´í•´ê´€ê³„ì ì‹ë³„"""
        stakeholders = situation_data.get('stakeholders', [])
        
        # ê¸°ë³¸ ì´í•´ê´€ê³„ì ì¶”ê°€
        if 'individuals' in situation_data:
            stakeholders.append("ê°œì¸")
        if 'organization' in situation_data:
            stakeholders.append("ì¡°ì§")
        if 'society' in situation_data:
            stakeholders.append("ì‚¬íšŒ")
        if 'environment' in situation_data:
            stakeholders.append("í™˜ê²½")
        
        return stakeholders
    
    def _analyze_potential_consequences(self, situation_data: Dict[str, Any]) -> List[str]:
        """ì ì¬ì  ê²°ê³¼ ë¶„ì„"""
        consequences = situation_data.get('consequences', [])
        
        # ê¸°ë³¸ ê²°ê³¼ ì¶”ê°€
        if 'positive_impact' in situation_data:
            consequences.append("ê¸ì •ì  ì˜í–¥")
        if 'negative_impact' in situation_data:
            consequences.append("ë¶€ì •ì  ì˜í–¥")
        if 'unintended_consequences' in situation_data:
            consequences.append("ì˜ë„í•˜ì§€ ì•Šì€ ê²°ê³¼")
        
        return consequences
    
    def _identify_dilemma_type(self, situation_data: Dict[str, Any], 
                              principles: List[EthicalPrinciple]) -> Optional[EthicalDilemmaType]:
        """ë”œë ˆë§ˆ ìœ í˜• ì‹ë³„"""
        if len(principles) < 2:
            return None
        
        # ì›ì¹™ ê°„ ê°ˆë“± í™•ì¸
        if len(principles) >= 2:
            return EthicalDilemmaType.CONFLICT_OF_PRINCIPLES
        
        # íŠ¹ì • ë”œë ˆë§ˆ íŒ¨í„´ í™•ì¸
        description = situation_data.get('description', '').lower()
        
        if 'individual' in description and 'collective' in description:
            return EthicalDilemmaType.INDIVIDUAL_VS_COLLECTIVE
        elif 'short' in description and 'long' in description:
            return EthicalDilemmaType.SHORT_TERM_VS_LONG_TERM
        elif 'rights' in description and 'utility' in description:
            return EthicalDilemmaType.RIGHTS_VS_UTILITY
        
        return None
    
    def _assess_complexity(self, situation_data: Dict[str, Any]) -> float:
        """ë³µì¡ì„± í‰ê°€"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ë¶„ì„ ë¡œì§ ì‚¬ìš©
        factors = len(situation_data.get('stakeholders', [])) + len(situation_data.get('consequences', []))
        return min(1.0, factors / 10.0)
    
    def _assess_urgency(self, situation_data: Dict[str, Any]) -> float:
        """ê¸´ê¸‰ì„± í‰ê°€"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ë¶„ì„ ë¡œì§ ì‚¬ìš©
        return random.uniform(0.3, 0.8)
    
    async def _generate_ethical_alternatives(self, situation: EthicalSituation) -> List[Dict[str, Any]]:
        """ìœ¤ë¦¬ì  ëŒ€ì•ˆ ìƒì„±"""
        alternatives = []
        
        # ì›ì¹™ë³„ ëŒ€ì•ˆ ìƒì„±
        for principle in situation.involved_principles:
            alternative = {
                'decision': f"{principle.value} ì›ì¹™ ê¸°ë°˜ ê²°ì •",
                'principle': principle,
                'reasoning': f"{principle.value} ì›ì¹™ì„ ìš°ì„ ì‹œí•˜ëŠ” ì ‘ê·¼",
                'score': random.uniform(0.4, 0.9)
            }
            alternatives.append(alternative)
        
        # ê· í˜•ì¡íŒ ëŒ€ì•ˆ ì¶”ê°€
        balanced_alternative = {
            'decision': "ê· í˜•ì¡íŒ ìœ¤ë¦¬ì  ê²°ì •",
            'principle': None,
            'reasoning': "ëª¨ë“  ê´€ë ¨ ì›ì¹™ì„ ê³ ë ¤í•œ ì¢…í•©ì  ì ‘ê·¼",
            'score': random.uniform(0.6, 0.9)
        }
        alternatives.append(balanced_alternative)
        
        return alternatives
    
    async def _evaluate_alternatives(self, alternatives: List[Dict[str, Any]], 
                                   situation: EthicalSituation) -> List[Dict[str, Any]]:
        """ëŒ€ì•ˆ í‰ê°€"""
        for alternative in alternatives:
            # ìœ¤ë¦¬ì  ì ìˆ˜ ê³„ì‚°
            ethical_score = self._calculate_alternative_ethical_score(alternative, situation)
            alternative['ethical_score'] = ethical_score
            
            # ì‹¤í˜„ ê°€ëŠ¥ì„± í‰ê°€
            feasibility_score = self._calculate_alternative_feasibility(alternative, situation)
            alternative['feasibility_score'] = feasibility_score
            
            # ì „ì²´ ì ìˆ˜ ê³„ì‚°
            alternative['overall_score'] = (ethical_score + feasibility_score) / 2.0
        
        return alternatives
    
    def _select_best_decision(self, evaluated_alternatives: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ìµœì  íŒë‹¨ ì„ íƒ"""
        return max(evaluated_alternatives, key=lambda x: x['overall_score'])
    
    async def _generate_ethical_reasoning(self, situation: EthicalSituation, 
                                        decision: Dict[str, Any]) -> str:
        """ìœ¤ë¦¬ì  ì¶”ë¡  ìƒì„±"""
        reasoning = f"ì´ ìƒí™©ì—ì„œ {decision['decision']}ì„ ì„ íƒí•œ ì´ìœ ëŠ” "
        
        if decision.get('principle'):
            reasoning += f"{decision['principle'].value} ì›ì¹™ì„ ê³ ë ¤í•˜ì—¬ "
        
        reasoning += decision['reasoning']
        
        return reasoning
    
    def _calculate_judgment_confidence(self, situation: EthicalSituation, 
                                     decision: Dict[str, Any]) -> JudgmentConfidence:
        """íŒë‹¨ ì‹ ë¢°ë„ ê³„ì‚°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ê³„ì‚° ë¡œì§ ì‚¬ìš©
        confidence_score = decision.get('overall_score', 0.5)
        
        if confidence_score >= 0.8:
            return JudgmentConfidence.VERY_HIGH
        elif confidence_score >= 0.6:
            return JudgmentConfidence.HIGH
        elif confidence_score >= 0.4:
            return JudgmentConfidence.MEDIUM
        elif confidence_score >= 0.2:
            return JudgmentConfidence.LOW
        else:
            return JudgmentConfidence.VERY_LOW
    
    def _calculate_ethical_score(self, situation: EthicalSituation, 
                               decision: Dict[str, Any]) -> float:
        """ìœ¤ë¦¬ì  ì ìˆ˜ ê³„ì‚°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ê³„ì‚° ë¡œì§ ì‚¬ìš©
        return decision.get('ethical_score', 0.5)
    
    async def _generate_moral_justification(self, situation: EthicalSituation, 
                                          decision: Dict[str, Any]) -> str:
        """ë„ë•ì  ì •ë‹¹í™” ìƒì„±"""
        justification = f"ì´ íŒë‹¨ì€ {', '.join([p.value for p in situation.involved_principles])} "
        justification += "ì›ì¹™ì„ ê³ ë ¤í•˜ì—¬ ë‚´ë¦° ìœ¤ë¦¬ì ìœ¼ë¡œ ì •ë‹¹í•œ ê²°ì •ì…ë‹ˆë‹¤."
        
        return justification
    
    def _identify_conflicting_principles(self, situation: EthicalSituation) -> List[EthicalPrinciple]:
        """ê°ˆë“±í•˜ëŠ” ì›ì¹™ ì‹ë³„"""
        if len(situation.involved_principles) < 2:
            return []
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ê°ˆë“± ë¶„ì„ ë¡œì§ ì‚¬ìš©
        return situation.involved_principles[:2]  # ì˜ˆì‹œë¡œ ì²˜ìŒ 2ê°œ ì„ íƒ
    
    def _calculate_conflict_intensity(self, conflicting_principles: List[EthicalPrinciple]) -> float:
        """ê°ˆë“± ê°•ë„ ê³„ì‚°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ê³„ì‚° ë¡œì§ ì‚¬ìš©
        return random.uniform(0.4, 0.8)
    
    def _select_resolution_approach(self, conflicting_principles: List[EthicalPrinciple]) -> str:
        """í•´ê²° ì ‘ê·¼ë²• ì„ íƒ"""
        approaches = [
            "ì›ì¹™ ê°„ ê· í˜• ëª¨ìƒ‰",
            "ìƒìœ„ ì›ì¹™ ìš°ì„  ì ìš©",
            "ìƒí™©ë³„ ì ì‘ì  ì ‘ê·¼",
            "í•©ì˜ ê¸°ë°˜ í•´ê²°",
            "ë‹¨ê³„ì  í•´ê²°"
        ]
        return random.choice(approaches)
    
    async def _generate_compromise_solution(self, situation: EthicalSituation, 
                                          conflicting_principles: List[EthicalPrinciple]) -> Optional[str]:
        """íƒ€í˜‘ì•ˆ ìƒì„±"""
        if not conflicting_principles:
            return None
        
        compromise = f"{'ì™€ '.join([p.value for p in conflicting_principles])} ì›ì¹™ì„ ëª¨ë‘ ê³ ë ¤í•œ "
        compromise += "ê· í˜•ì¡íŒ í•´ê²°ì±…ì„ ì œì‹œí•©ë‹ˆë‹¤."
        
        return compromise
    
    def _calculate_principle_understanding(self) -> float:
        """ì›ì¹™ ì´í•´ë„ ê³„ì‚°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ê³„ì‚° ë¡œì§ ì‚¬ìš©
        return random.uniform(0.6, 0.9)
    
    def _calculate_conflict_resolution_ability(self) -> float:
        """ê°ˆë“± í•´ê²° ëŠ¥ë ¥ ê³„ì‚°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ê³„ì‚° ë¡œì§ ì‚¬ìš©
        return random.uniform(0.5, 0.8)
    
    def _calculate_moral_reasoning_ability(self) -> float:
        """ë„ë•ì  ì¶”ë¡  ëŠ¥ë ¥ ê³„ì‚°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ê³„ì‚° ë¡œì§ ì‚¬ìš©
        return random.uniform(0.6, 0.9)
    
    def _calculate_ethical_consistency(self) -> float:
        """ìœ¤ë¦¬ì  ì¼ê´€ì„± ê³„ì‚°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ê³„ì‚° ë¡œì§ ì‚¬ìš©
        return random.uniform(0.7, 0.9)
    
    def _calculate_moral_imagination(self) -> float:
        """ë„ë•ì  ìƒìƒë ¥ ê³„ì‚°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ê³„ì‚° ë¡œì§ ì‚¬ìš©
        return random.uniform(0.5, 0.8)
    
    def _identify_ethical_improvement_areas(self, scores: Dict[str, float]) -> List[str]:
        """ìœ¤ë¦¬ì  ê°œì„  ì˜ì—­ ì‹ë³„"""
        areas = []
        threshold = 0.7
        
        for area, score in scores.items():
            if score < threshold:
                areas.append(area)
        
        return areas
    
    def _calculate_judgment_statistics(self) -> Dict[str, Any]:
        """íŒë‹¨ í†µê³„ ê³„ì‚°"""
        if not self.judgment_state.ethical_judgments:
            return {"total_judgments": 0, "average_confidence": 0.0, "average_ethical_score": 0.0}
        
        total_judgments = len(self.judgment_state.ethical_judgments)
        confidence_values = {
            JudgmentConfidence.VERY_LOW: 0.1,
            JudgmentConfidence.LOW: 0.3,
            JudgmentConfidence.MEDIUM: 0.5,
            JudgmentConfidence.HIGH: 0.7,
            JudgmentConfidence.VERY_HIGH: 0.9
        }
        average_confidence = sum(confidence_values[j.confidence] for j in self.judgment_state.ethical_judgments) / total_judgments
        average_ethical_score = sum(j.ethical_score for j in self.judgment_state.ethical_judgments) / total_judgments
        
        return {
            "total_judgments": total_judgments,
            "average_confidence": average_confidence,
            "average_ethical_score": average_ethical_score,
            "confidence_distribution": self._calculate_confidence_distribution()
        }
    
    def _calculate_confidence_distribution(self) -> Dict[str, int]:
        """ì‹ ë¢°ë„ ë¶„í¬ ê³„ì‚°"""
        distribution = defaultdict(int)
        for judgment in self.judgment_state.ethical_judgments:
            distribution[judgment.confidence.value] += 1
        return dict(distribution)
    
    async def _generate_ethical_recommendations(self) -> List[str]:
        """ìœ¤ë¦¬ì  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ìœ¤ë¦¬ì  ì„±ìˆ™ë„ ìˆ˜ì¤€ì— ë”°ë¥¸ ê¶Œì¥ì‚¬í•­
        maturity_level = self.judgment_state.maturity_metrics.overall_ethical_maturity
        
        if maturity_level < 0.4:
            recommendations.append("ê¸°ë³¸ì ì¸ ìœ¤ë¦¬ ì›ì¹™ í•™ìŠµ")
            recommendations.append("ë„ë•ì  ì‚¬ê³  ê¸°ë²• ë„ì…")
        elif maturity_level < 0.6:
            recommendations.append("ìœ¤ë¦¬ì  ê°ˆë“± í•´ê²° ê¸°ë²• ì‹¬í™”")
            recommendations.append("ë‹¤ì–‘í•œ ìœ¤ë¦¬ì  ê´€ì  íƒêµ¬")
        elif maturity_level < 0.8:
            recommendations.append("ìœ¤ë¦¬ì  ì„±ìˆ™ë„ í–¥ìƒ í›ˆë ¨")
            recommendations.append("ìœ¤ë¦¬ì  ë¦¬ë”ì‹­ ê°œë°œ")
        else:
            recommendations.append("ìœ¤ë¦¬ì  ì§€í˜œ ê°œë°œ")
            recommendations.append("íƒ€ì¸ì˜ ìœ¤ë¦¬ì  ì„±ì¥ ì§€ì›")
        
        return recommendations
    
    def _calculate_alternative_ethical_score(self, alternative: Dict[str, Any], 
                                          situation: EthicalSituation) -> float:
        """ëŒ€ì•ˆì˜ ìœ¤ë¦¬ì  ì ìˆ˜ ê³„ì‚°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ê³„ì‚° ë¡œì§ ì‚¬ìš©
        return alternative.get('score', 0.5)
    
    def _calculate_alternative_feasibility(self, alternative: Dict[str, Any], 
                                        situation: EthicalSituation) -> float:
        """ëŒ€ì•ˆì˜ ì‹¤í˜„ ê°€ëŠ¥ì„± ê³„ì‚°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ê³„ì‚° ë¡œì§ ì‚¬ìš©
        return random.uniform(0.4, 0.8)
    
    async def _update_principle_understanding_metrics(self, situation: EthicalSituation) -> None:
        """ì›ì¹™ ì´í•´ë„ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ì—…ë°ì´íŠ¸ ë¡œì§ ì‚¬ìš©
        self.judgment_state.maturity_metrics.principle_understanding = min(1.0, 
            self.judgment_state.maturity_metrics.principle_understanding + 0.01)
    
    async def _update_moral_reasoning_metrics(self, judgment: EthicalJudgment) -> None:
        """ë„ë•ì  ì¶”ë¡  ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ì—…ë°ì´íŠ¸ ë¡œì§ ì‚¬ìš©
        self.judgment_state.maturity_metrics.moral_reasoning = min(1.0, 
            self.judgment_state.maturity_metrics.moral_reasoning + 0.01)
    
    async def _update_conflict_resolution_metrics(self, conflict: EthicalConflict) -> None:
        """ê°ˆë“± í•´ê²° ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ì—…ë°ì´íŠ¸ ë¡œì§ ì‚¬ìš©
        self.judgment_state.maturity_metrics.conflict_resolution = min(1.0, 
            self.judgment_state.maturity_metrics.conflict_resolution + 0.01)

async def test_ethical_judgment_system():
    """ìœ¤ë¦¬ì  íŒë‹¨ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ§  ìœ¤ë¦¬ì  íŒë‹¨ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ì‹œìŠ¤í…œ ìƒì„±
    judgment_system = EthicalJudgmentSystem()
    
    # í…ŒìŠ¤íŠ¸ ìƒí™© ë°ì´í„°
    test_situations = [
        {
            "description": "ê°œì¸ì •ë³´ ìˆ˜ì§‘ê³¼ ì„œë¹„ìŠ¤ ê°œì„  ì‚¬ì´ì˜ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ",
            "stakeholders": ["ê°œì¸", "ì¡°ì§", "ì‚¬íšŒ"],
            "consequences": ["ê°œì¸ì •ë³´ ë³´í˜¸", "ì„œë¹„ìŠ¤ í’ˆì§ˆ í–¥ìƒ", "ì‚¬ìš©ì ê²½í—˜ ê°œì„ "],
            "context": {"privacy": True, "service_improvement": True}
        },
        {
            "description": "ê³µì •í•œ ì±„ìš©ê³¼ ë‹¤ì–‘ì„± í™•ë³´ ì‚¬ì´ì˜ ê· í˜•",
            "stakeholders": ["ì§€ì›ì", "ì¡°ì§", "ì‚¬íšŒ"],
            "consequences": ["ê³µì •ì„± ë³´ì¥", "ë‹¤ì–‘ì„± í™•ë³´", "ì¡°ì§ ë¬¸í™” ê°œì„ "],
            "context": {"fairness": True, "diversity": True}
        },
        {
            "description": "í™˜ê²½ ë³´í˜¸ì™€ ê²½ì œ ë°œì „ ì‚¬ì´ì˜ ê°ˆë“±",
            "stakeholders": ["í™˜ê²½", "ê²½ì œ", "ë¯¸ë˜ ì„¸ëŒ€"],
            "consequences": ["í™˜ê²½ ë³´í˜¸", "ê²½ì œ ì„±ì¥", "ì§€ì† ê°€ëŠ¥ì„±"],
            "context": {"environment": True, "economy": True}
        }
    ]
    
    # ìƒí™© ë¶„ì„ ë° íŒë‹¨
    for situation_data in test_situations:
        # ìœ¤ë¦¬ì  ìƒí™© ë¶„ì„
        situation = await judgment_system.analyze_ethical_situation(situation_data)
        
        # ìœ¤ë¦¬ì  íŒë‹¨ ìˆ˜í–‰
        judgment = await judgment_system.make_ethical_judgment(situation)
        
        # ìœ¤ë¦¬ì  ê°ˆë“± í•´ê²°
        if situation.dilemma_type:
            conflict = await judgment_system.resolve_ethical_conflict(situation)
    
    # ìœ¤ë¦¬ì  ì„±ìˆ™ë„ í‰ê°€
    maturity = await judgment_system.assess_ethical_maturity()
    
    # ë³´ê³ ì„œ ìƒì„±
    report = await judgment_system.generate_ethical_report()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n=== ìœ¤ë¦¬ì  íŒë‹¨ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    print(f"ìœ¤ë¦¬ì  ì„±ìˆ™ë„: {maturity['score']:.3f} ({maturity['maturity_level']})")
    print(f"ë¶„ì„ëœ ìƒí™©: {len(judgment_system.judgment_state.ethical_situations)}ê°œ")
    print(f"ìˆ˜í–‰ëœ íŒë‹¨: {len(judgment_system.judgment_state.ethical_judgments)}ê°œ")
    print(f"í•´ê²°ëœ ê°ˆë“±: {len(judgment_system.judgment_state.ethical_conflicts)}ê°œ")
    
    print("âœ… ìœ¤ë¦¬ì  íŒë‹¨ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    asyncio.run(test_ethical_judgment_system()) 