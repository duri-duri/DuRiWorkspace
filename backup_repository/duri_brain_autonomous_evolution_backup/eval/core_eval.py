"""
DuRiì˜ Core_Eval ì‹œìŠ¤í…œ

Dream ì „ëµì„ í‰ê°€í•˜ê³  ì±„íƒí• ì§€ íŒë‹¨í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
ìœ ë ˆì¹´ ê°ì§€ ë¡œì§ì„ í¬í•¨í•˜ì—¬ ì˜ˆìƒ ì™¸ì˜ ê³ ì„±ëŠ¥ ì „ëµì„ ì¦‰ì‹œ ìŠ¹ê²©í•©ë‹ˆë‹¤.
"""

import logging
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
import math
import uuid

# í‰ê°€ ë¡œê·¸ ë° ë™ì‹œì„± ì œì–´ ì‹œìŠ¤í…œ import
from duri_core.utils.evaluation_logger import get_evaluation_logger
from duri_core.utils.evaluation_lock import get_evaluation_lock, get_evaluation_state_manager

logger = logging.getLogger(__name__)

class EvaluationType(Enum):
    """í‰ê°€ ìœ í˜•"""
    STANDARD = "standard"          # í‘œì¤€ í‰ê°€
    EUREKA = "eureka"              # ìœ ë ˆì¹´ ê°ì§€
    COMPARATIVE = "comparative"     # ë¹„êµ í‰ê°€
    PREDICTIVE = "predictive"       # ì˜ˆì¸¡ í‰ê°€

class EvaluationResult(Enum):
    """í‰ê°€ ê²°ê³¼"""
    ADOPT = "adopt"                # ì±„íƒ
    REJECT = "reject"              # ê±°ë¶€
    FURTHER_TEST = "further_test"  # ì¶”ê°€ í…ŒìŠ¤íŠ¸
    EUREKA_PROMOTE = "eureka_promote"  # ìœ ë ˆì¹´ ìŠ¹ê²©

"""
ğŸ“Œ Evaluation ê¸°ì¤€ì€ core_eval ì „ìš© EvaluationCriteriaë¥¼ ë”°ë¥´ë©°,
SurvivalCriteriaì™€ëŠ” ëª©ì /ì ìš© ë²”ìœ„ê°€ ë‹¤ë¥´ë¯€ë¡œ í†µí•©í•˜ì§€ ì•ŠìŒ.
ë‹¨, í–¥í›„ ê¸°ì¤€ í†µì¼ì´ í•„ìš”í•  ê²½ìš°, duri_core/philosophy/unified_criteria.pyë¡œ ì´ì „ ê°€ëŠ¥.
"""

@dataclass
class DreamEvaluationCriteria:
    """Dream ì „ëµ í‰ê°€ ê¸°ì¤€"""
    performance_weight: float = 0.4
    novelty_weight: float = 0.3
    stability_weight: float = 0.2
    efficiency_weight: float = 0.1
    eureka_threshold: float = 0.85
    adoption_threshold: float = 0.7
    rejection_threshold: float = 0.3
    ttl_hours: int = 24  # 24ì‹œê°„ í›„ ìë™ íê¸°
    max_dream_candidates: int = 100  # ìµœëŒ€ Dream í›„ë³´ ìˆ˜

@dataclass
class EvaluationDecision:
    """í‰ê°€ ê²°ì •"""
    dream_id: str
    evaluation_type: EvaluationType
    result: EvaluationResult
    confidence: float
    reasoning: List[str]
    performance_score: float
    novelty_score: float
    stability_score: float
    efficiency_score: float
    combined_score: float
    evaluation_time: datetime
    eureka_detected: bool

class CoreEval:
    """
    DuRiì˜ Core_Eval ì‹œìŠ¤í…œ
    
    Dream ì „ëµì„ í‰ê°€í•˜ê³  ì±„íƒí• ì§€ íŒë‹¨í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """CoreEval ì´ˆê¸°í™”"""
        self.evaluation_criteria = DreamEvaluationCriteria()
        self.evaluation_history: List[EvaluationDecision] = []
        self.eureka_detections: List[str] = []
        self.adoption_history: List[str] = []
        
        # í‰ê°€ í†µê³„
        self.evaluation_stats = {
            'total_evaluations': 0,
            'adoptions': 0,
            'rejections': 0,
            'eureka_detections': 0,
            'further_tests': 0,
            'ttl_expired': 0  # TTL ë§Œë£Œëœ í‰ê°€ ìˆ˜
        }
        
        # TTL ê´€ë¦¬
        self.last_ttl_cleanup = datetime.now()
        self.ttl_cleanup_interval = timedelta(hours=1)  # 1ì‹œê°„ë§ˆë‹¤ ì •ë¦¬
        
        # í‰ê°€ ë¡œê·¸ ë° ë™ì‹œì„± ì œì–´ ì‹œìŠ¤í…œ
        self.evaluation_logger = get_evaluation_logger()
        self.evaluation_lock = get_evaluation_lock()
        self.state_manager = get_evaluation_state_manager()
        
        logger.info("CoreEval ì´ˆê¸°í™” ì™„ë£Œ")
    
    def evaluate_dream_strategy(self, dream_data: Dict[str, Any], 
                              current_performance: float = 0.0,
                              context: Optional[Dict[str, Any]] = None) -> EvaluationDecision:
        """
        Dream ì „ëµì„ í‰ê°€í•©ë‹ˆë‹¤.
        
        Args:
            dream_data: Dream ì „ëµ ë°ì´í„°
            current_performance: í˜„ì¬ ì„±ê³¼
            context: í‰ê°€ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            EvaluationDecision: í‰ê°€ ê²°ì •
        """
        dream_id = dream_data.get('dream_id', f"dream_{uuid.uuid4().hex[:8]}")
        evaluation_id = f"dream_eval_{uuid.uuid4().hex[:8]}"
        
        try:
            # ë™ì‹œì„± ì œì–´ ë° ìƒíƒœ ê´€ë¦¬
            with self.evaluation_lock.evaluation_context(evaluation_id):
                self.state_manager.start_evaluation("dream", evaluation_id)
                
                # TTL ì •ë¦¬
                self.cleanup_expired_evaluations()
                self.enforce_max_candidates()
                
                # 1. ê¸°ë³¸ ì ìˆ˜ ê³„ì‚°
                performance_score = self._calculate_performance_score(dream_data, current_performance)
                novelty_score = self._calculate_novelty_score(dream_data)
                stability_score = self._calculate_stability_score(dream_data)
                efficiency_score = self._calculate_efficiency_score(dream_data)
                
                # 2. ì¢…í•© ì ìˆ˜ ê³„ì‚°
                combined_score = self._calculate_combined_score(
                    performance_score, novelty_score, stability_score, efficiency_score
                )
                
                # 3. ìœ ë ˆì¹´ ê°ì§€
                eureka_detected = self._detect_eureka(combined_score, novelty_score, performance_score)
                
                # 4. í‰ê°€ ìœ í˜• ê²°ì •
                evaluation_type = self._determine_evaluation_type(dream_data, eureka_detected)
                
                # 5. í‰ê°€ ê²°ê³¼ ê²°ì •
                result, reasoning = self._determine_evaluation_result(
                    combined_score, eureka_detected, context
                )
                
                # 6. ì‹ ë¢°ë„ ê³„ì‚°
                confidence = self._calculate_evaluation_confidence(
                    performance_score, novelty_score, stability_score, efficiency_score
                )
                
                # 7. í‰ê°€ ê²°ì • ìƒì„±
                decision = EvaluationDecision(
                    dream_id=dream_id,
                    evaluation_type=evaluation_type,
                    result=result,
                    confidence=confidence,
                    reasoning=reasoning,
                    performance_score=performance_score,
                    novelty_score=novelty_score,
                    stability_score=stability_score,
                    efficiency_score=efficiency_score,
                    combined_score=combined_score,
                    evaluation_time=datetime.now(),
                    eureka_detected=eureka_detected
                )
                
                # 8. í†µê³„ ì—…ë°ì´íŠ¸
                self._update_evaluation_stats(decision)
                self.evaluation_history.append(decision)
                
                # 9. í‰ê°€ ë¡œê·¸ ì €ì¥
                self.evaluation_logger.log_dream_evaluation(
                    dream_id=dream_id,
                    performance_score=performance_score,
                    novelty_score=novelty_score,
                    stability_score=stability_score,
                    efficiency_score=efficiency_score,
                    combined_score=combined_score,
                    result=result.value,
                    confidence=confidence,
                    eureka_detected=eureka_detected
                )
                
                logger.info(f"Dream ì „ëµ í‰ê°€ ì™„ë£Œ: {dream_id}, ê²°ê³¼: {result.value}, ì ìˆ˜: {combined_score:.3f}")
                return decision
                
        except Exception as e:
            logger.error(f"Dream ì „ëµ í‰ê°€ ì‹¤íŒ¨: {e}")
            self.state_manager.record_error("dream")
            
            # Fallback í‰ê°€ ì‹œë„
            try:
                return self.evaluate_dream_strategy_fallback(dream_data, current_performance, context)
            except Exception as fallback_error:
                logger.error(f"Fallback í‰ê°€ë„ ì‹¤íŒ¨: {fallback_error}")
                return self._create_error_decision(dream_id)
        finally:
            self.state_manager.end_evaluation("dream")
    
    def _calculate_performance_score(self, dream_data: Dict[str, Any], 
                                   current_performance: float) -> float:
        """ì„±ê³¼ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        # ê¸°ë³¸ ì„±ê³¼ ì ìˆ˜
        base_performance = dream_data.get('confidence_score', 0.5)
        
        # í˜„ì¬ ì„±ê³¼ì™€ ë¹„êµ
        if current_performance > 0:
            performance_improvement = (base_performance - current_performance) / current_performance
            performance_score = base_performance + (performance_improvement * 0.3)
        else:
            performance_score = base_performance
        
        # ì „ëµ ìœ í˜•ë³„ ê°€ì¤‘ì¹˜
        strategy_type = dream_data.get('type', 'unknown')
        type_weights = {
            'random_combination': 0.8,
            'pattern_mutation': 0.9,
            'concept_fusion': 0.85,
            'intuition_exploration': 0.7
        }
        
        weight = type_weights.get(strategy_type, 0.8)
        performance_score *= weight
        
        return min(performance_score, 1.0)
    
    def _calculate_novelty_score(self, dream_data: Dict[str, Any]) -> float:
        """ìƒˆë¡œì›€ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        # ê¸°ë³¸ ìƒˆë¡œì›€ ì ìˆ˜
        novelty_score = dream_data.get('novelty_score', 0.5)
        
        # ì „ëµ ìœ í˜•ë³„ ìƒˆë¡œì›€
        strategy_type = dream_data.get('type', 'unknown')
        type_novelty = {
            'random_combination': 0.6,
            'pattern_mutation': 0.7,
            'concept_fusion': 0.8,
            'intuition_exploration': 0.9
        }
        
        type_score = type_novelty.get(strategy_type, 0.6)
        novelty_score = (novelty_score + type_score) / 2
        
        # ë³µì¡ì„± ë°˜ì˜
        complexity = dream_data.get('complexity_score', 0.5)
        novelty_score *= (1 + complexity * 0.2)
        
        return min(novelty_score, 1.0)
    
    def _calculate_stability_score(self, dream_data: Dict[str, Any]) -> float:
        """ì•ˆì •ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        # ê¸°ë³¸ ì•ˆì •ì„±
        stability_score = 0.5
        
        # ì „ëµ ìœ í˜•ë³„ ì•ˆì •ì„±
        strategy_type = dream_data.get('type', 'unknown')
        type_stability = {
            'random_combination': 0.6,
            'pattern_mutation': 0.7,
            'concept_fusion': 0.8,
            'intuition_exploration': 0.5
        }
        
        stability_score = type_stability.get(strategy_type, 0.6)
        
        # ì‹ ë¢°ë„ ë°˜ì˜
        confidence = dream_data.get('confidence_score', 0.5)
        stability_score = (stability_score + confidence) / 2
        
        return min(stability_score, 1.0)
    
    def _calculate_efficiency_score(self, dream_data: Dict[str, Any]) -> float:
        """íš¨ìœ¨ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        # ê¸°ë³¸ íš¨ìœ¨ì„±
        efficiency_score = 0.5
        
        # ì „ëµ ìœ í˜•ë³„ íš¨ìœ¨ì„±
        strategy_type = dream_data.get('type', 'unknown')
        type_efficiency = {
            'random_combination': 0.7,
            'pattern_mutation': 0.8,
            'concept_fusion': 0.75,
            'intuition_exploration': 0.6
        }
        
        efficiency_score = type_efficiency.get(strategy_type, 0.7)
        
        # ìš°ì„ ìˆœìœ„ ë°˜ì˜
        priority = dream_data.get('priority', 0.5)
        efficiency_score *= priority
        
        return min(efficiency_score, 1.0)
    
    def _calculate_combined_score(self, performance_score: float, novelty_score: float,
                                stability_score: float, efficiency_score: float) -> float:
        """ì¢…í•© ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        criteria = self.evaluation_criteria
        
        combined_score = (
            performance_score * criteria.performance_weight +
            novelty_score * criteria.novelty_weight +
            stability_score * criteria.stability_weight +
            efficiency_score * criteria.efficiency_weight
        )
        
        return min(combined_score, 1.0)
    
    def _detect_eureka(self, combined_score: float, novelty_score: float, 
                      performance_score: float) -> bool:
        """ìœ ë ˆì¹´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤."""
        # ìœ ë ˆì¹´ ì¡°ê±´: ë†’ì€ ì¢…í•© ì ìˆ˜ + ë†’ì€ ìƒˆë¡œì›€ + ë†’ì€ ì„±ê³¼
        eureka_threshold = self.evaluation_criteria.eureka_threshold
        
        if (combined_score >= eureka_threshold and 
            novelty_score >= 0.8 and 
            performance_score >= 0.8):
            return True
        
        # ì˜ˆìƒ ì™¸ì˜ ê³ ì„±ëŠ¥ ì¡°í•©
        if (combined_score >= 0.9 and 
            novelty_score >= 0.9):
            return True
        
        return False
    
    def _determine_evaluation_type(self, dream_data: Dict[str, Any], 
                                 eureka_detected: bool) -> EvaluationType:
        """í‰ê°€ ìœ í˜•ì„ ê²°ì •í•©ë‹ˆë‹¤."""
        if eureka_detected:
            return EvaluationType.EUREKA
        
        # íŠ¹ë³„í•œ ì¡°ê±´ë“¤
        strategy_type = dream_data.get('type', 'unknown')
        
        if strategy_type == 'intuition_exploration':
            return EvaluationType.EUREKA
        elif strategy_type == 'concept_fusion':
            return EvaluationType.COMPARATIVE
        else:
            return EvaluationType.STANDARD
    
    def _determine_evaluation_result(self, combined_score: float, eureka_detected: bool,
                                   context: Optional[Dict[str, Any]]) -> Tuple[EvaluationResult, List[str]]:
        """í‰ê°€ ê²°ê³¼ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
        reasoning = []
        criteria = self.evaluation_criteria
        
        # ìœ ë ˆì¹´ ê°ì§€ëœ ê²½ìš°
        if eureka_detected:
            reasoning.append("ìœ ë ˆì¹´ ê°ì§€: ì˜ˆìƒ ì™¸ì˜ ê³ ì„±ëŠ¥ ì „ëµ")
            return EvaluationResult.EUREKA_PROMOTE, reasoning
        
        # ì±„íƒ ì„ê³„ê°’ í™•ì¸
        if combined_score >= criteria.adoption_threshold:
            reasoning.append(f"ì¢…í•© ì ìˆ˜ {combined_score:.2f}ë¡œ ì±„íƒ ì„ê³„ê°’ {criteria.adoption_threshold} ì´ˆê³¼")
            return EvaluationResult.ADOPT, reasoning
        
        # ê±°ë¶€ ì„ê³„ê°’ í™•ì¸
        if combined_score <= criteria.rejection_threshold:
            reasoning.append(f"ì¢…í•© ì ìˆ˜ {combined_score:.2f}ë¡œ ê±°ë¶€ ì„ê³„ê°’ {criteria.rejection_threshold} ë¯¸ë§Œ")
            return EvaluationResult.REJECT, reasoning
        
        # ì¶”ê°€ í…ŒìŠ¤íŠ¸
        reasoning.append(f"ì¢…í•© ì ìˆ˜ {combined_score:.2f}ë¡œ ì¶”ê°€ í…ŒìŠ¤íŠ¸ í•„ìš”")
        return EvaluationResult.FURTHER_TEST, reasoning
    
    def _calculate_evaluation_confidence(self, performance_score: float, novelty_score: float,
                                       stability_score: float, efficiency_score: float) -> float:
        """í‰ê°€ ì‹ ë¢°ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        # ì ìˆ˜ë“¤ì˜ ì¼ê´€ì„±
        scores = [performance_score, novelty_score, stability_score, efficiency_score]
        variance = sum((score - sum(scores) / len(scores)) ** 2 for score in scores) / len(scores)
        
        # ì¼ê´€ì„± ì ìˆ˜ (ë¶„ì‚°ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì‹ ë¢°ë„)
        consistency_score = 1.0 - (variance * 2)
        
        # í‰ê·  ì ìˆ˜
        average_score = sum(scores) / len(scores)
        
        # ìµœì¢… ì‹ ë¢°ë„
        confidence = (consistency_score + average_score) / 2
        
        return max(confidence, 0.0)
    
    def _update_evaluation_stats(self, decision: EvaluationDecision):
        """í‰ê°€ í†µê³„ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        self.evaluation_stats['total_evaluations'] += 1
        
        if decision.result == EvaluationResult.ADOPT:
            self.evaluation_stats['adoptions'] += 1
            self.adoption_history.append(decision.dream_id)
        elif decision.result == EvaluationResult.REJECT:
            self.evaluation_stats['rejections'] += 1
        elif decision.result == EvaluationResult.EUREKA_PROMOTE:
            self.evaluation_stats['eureka_detections'] += 1
            self.eureka_detections.append(decision.dream_id)
        elif decision.result == EvaluationResult.FURTHER_TEST:
            self.evaluation_stats['further_tests'] += 1
    
    def evaluate_dream_strategy_fallback(self, dream_data: Dict[str, Any], 
                                       current_performance: float = 0.0,
                                       context: Optional[Dict[str, Any]] = None) -> EvaluationDecision:
        """
        Fallback ìƒí™©ì—ì„œì˜ ìµœì†Œ í‰ê°€ ê¸°ëŠ¥
        
        ì‹œìŠ¤í…œ ë¬´ë ¥í™”ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì œí•œëœ ê¸°ì¤€ìœ¼ë¡œ ê¸°ë³¸ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        dream_id = dream_data.get('dream_id', f"dream_{uuid.uuid4().hex[:8]}")
        
        try:
            logger.warning(f"Core_Eval Fallback ëª¨ë“œ í™œì„±í™”: {dream_id}")
            
            # ì œí•œëœ ë°ì´í„°ë¡œ ê¸°ë³¸ ì ìˆ˜ ê³„ì‚°
            performance_score = self._calculate_fallback_performance_score(dream_data, current_performance)
            novelty_score = self._calculate_fallback_novelty_score(dream_data)
            stability_score = 0.5  # ê¸°ë³¸ ì•ˆì •ì„± ì ìˆ˜
            efficiency_score = 0.5  # ê¸°ë³¸ íš¨ìœ¨ì„± ì ìˆ˜
            
            # ë‹¨ìˆœí™”ëœ ì¢…í•© ì ìˆ˜ ê³„ì‚°
            combined_score = (performance_score * 0.6 + novelty_score * 0.4)
            
            # ê¸°ë³¸ í‰ê°€ ê²°ê³¼ ê²°ì •
            if combined_score > 0.7:
                result = EvaluationResult.ADOPT
                reasoning = ["Fallback ëª¨ë“œ: ë†’ì€ ê¸°ë³¸ ì ìˆ˜ë¡œ ì±„íƒ"]
            elif combined_score > 0.5:
                result = EvaluationResult.FURTHER_TEST
                reasoning = ["Fallback ëª¨ë“œ: ì¶”ê°€ í…ŒìŠ¤íŠ¸ í•„ìš”"]
            else:
                result = EvaluationResult.REJECT
                reasoning = ["Fallback ëª¨ë“œ: ë‚®ì€ ì ìˆ˜ë¡œ ê±°ë¶€"]
            
            # ë‚®ì€ ì‹ ë¢°ë„ (Fallback ëª¨ë“œ)
            confidence = 0.3
            
            decision = EvaluationDecision(
                dream_id=dream_id,
                evaluation_type=EvaluationType.STANDARD,
                result=result,
                confidence=confidence,
                reasoning=reasoning,
                performance_score=performance_score,
                novelty_score=novelty_score,
                stability_score=stability_score,
                efficiency_score=efficiency_score,
                combined_score=combined_score,
                evaluation_time=datetime.now(),
                eureka_detected=False
            )
            
            logger.info(f"Fallback í‰ê°€ ì™„ë£Œ: {dream_id}, ê²°ê³¼: {result.value}, ì ìˆ˜: {combined_score:.3f}")
            return decision
            
        except Exception as e:
            logger.error(f"Fallback í‰ê°€ ì‹¤íŒ¨: {e}")
            return self._create_error_decision(dream_id)
    
    def _calculate_fallback_performance_score(self, dream_data: Dict[str, Any], 
                                            current_performance: float) -> float:
        """Fallback ëª¨ë“œì—ì„œì˜ ì„±ê³¼ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê¸°ë³¸ ì„±ê³¼ ì§€í‘œ ì¶”ì¶œ
            performance_indicators = dream_data.get('performance_indicators', {})
            
            if not performance_indicators:
                return 0.5  # ê¸°ë³¸ê°’
            
            # ë‹¨ìˆœí™”ëœ ì„±ê³¼ ê³„ì‚°
            score = 0.0
            count = 0
            
            for indicator, value in performance_indicators.items():
                if isinstance(value, (int, float)):
                    score += min(value, 1.0)
                    count += 1
            
            if count > 0:
                return score / count
            else:
                return 0.5
                
        except Exception as e:
            logger.error(f"Fallback ì„±ê³¼ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_fallback_novelty_score(self, dream_data: Dict[str, Any]) -> float:
        """Fallback ëª¨ë“œì—ì„œì˜ ìƒˆë¡œì›€ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê¸°ë³¸ ìƒˆë¡œì›€ ì§€í‘œ ì¶”ì¶œ
            novelty_indicators = dream_data.get('novelty_indicators', {})
            
            if not novelty_indicators:
                return 0.5  # ê¸°ë³¸ê°’
            
            # ë‹¨ìˆœí™”ëœ ìƒˆë¡œì›€ ê³„ì‚°
            score = 0.0
            count = 0
            
            for indicator, value in novelty_indicators.items():
                if isinstance(value, (int, float)):
                    score += min(value, 1.0)
                    count += 1
            
            if count > 0:
                return score / count
            else:
                return 0.5
                
        except Exception as e:
            logger.error(f"Fallback ìƒˆë¡œì›€ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _create_error_decision(self, dream_id: str) -> EvaluationDecision:
        """ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ê²°ì •ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        return EvaluationDecision(
            dream_id=dream_id,
            evaluation_type=EvaluationType.STANDARD,
            result=EvaluationResult.REJECT,
            confidence=0.0,
            reasoning=["í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"],
            performance_score=0.0,
            novelty_score=0.0,
            stability_score=0.0,
            efficiency_score=0.0,
            combined_score=0.0,
            evaluation_time=datetime.now(),
            eureka_detected=False
        )
    
    def get_evaluation_statistics(self) -> Dict[str, Any]:
        """í‰ê°€ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        total_evaluations = self.evaluation_stats['total_evaluations']
        
        if total_evaluations == 0:
            return {
                "total_evaluations": 0,
                "adoption_rate": 0.0,
                "rejection_rate": 0.0,
                "eureka_rate": 0.0,
                "further_test_rate": 0.0,
                "average_confidence": 0.0,
                "average_combined_score": 0.0
            }
        
        adoption_rate = self.evaluation_stats['adoptions'] / total_evaluations
        rejection_rate = self.evaluation_stats['rejections'] / total_evaluations
        eureka_rate = self.evaluation_stats['eureka_detections'] / total_evaluations
        further_test_rate = self.evaluation_stats['further_tests'] / total_evaluations
        
        # í‰ê·  ì‹ ë¢°ë„
        avg_confidence = sum(d.confidence for d in self.evaluation_history) / len(self.evaluation_history) if self.evaluation_history else 0
        
        # í‰ê·  ì¢…í•© ì ìˆ˜
        avg_combined_score = sum(d.combined_score for d in self.evaluation_history) / len(self.evaluation_history) if self.evaluation_history else 0
        
        return {
            "total_evaluations": total_evaluations,
            "adoption_rate": adoption_rate,
            "rejection_rate": rejection_rate,
            "eureka_rate": eureka_rate,
            "further_test_rate": further_test_rate,
            "average_confidence": avg_confidence,
            "average_combined_score": avg_combined_score,
            "eureka_detections": self.eureka_detections,
            "adoption_history": self.adoption_history
        }
    
    def update_evaluation_criteria(self, new_criteria: DreamEvaluationCriteria):
        """í‰ê°€ ê¸°ì¤€ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        self.evaluation_criteria = new_criteria
        logger.info("í‰ê°€ ê¸°ì¤€ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    
    def get_recent_evaluations(self, limit: int = 10) -> List[EvaluationDecision]:
        """ìµœê·¼ í‰ê°€ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.evaluation_history[-limit:] if self.evaluation_history else []
    
    def cleanup_expired_evaluations(self):
        """TTLì´ ë§Œë£Œëœ í‰ê°€ ê²°ê³¼ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
        current_time = datetime.now()
        
        # TTL ì •ë¦¬ ì£¼ê¸° í™•ì¸
        if current_time - self.last_ttl_cleanup < self.ttl_cleanup_interval:
            return
        
        ttl_threshold = current_time - timedelta(hours=self.evaluation_criteria.ttl_hours)
        expired_count = 0
        
        # ë§Œë£Œëœ í‰ê°€ ê²°ê³¼ ì œê±°
        original_count = len(self.evaluation_history)
        self.evaluation_history = [
            eval_result for eval_result in self.evaluation_history
            if eval_result.evaluation_time > ttl_threshold
        ]
        expired_count = original_count - len(self.evaluation_history)
        
        # ë§Œë£Œëœ ìœ ë ˆì¹´ ê°ì§€ ì œê±°
        self.eureka_detections = [
            dream_id for dream_id in self.eureka_detections
            if any(eval_result.dream_id == dream_id and 
                   eval_result.evaluation_time > ttl_threshold
                   for eval_result in self.evaluation_history)
        ]
        
        # ë§Œë£Œëœ ì±„íƒ íˆìŠ¤í† ë¦¬ ì œê±°
        self.adoption_history = [
            dream_id for dream_id in self.adoption_history
            if any(eval_result.dream_id == dream_id and 
                   eval_result.evaluation_time > ttl_threshold
                   for eval_result in self.evaluation_history)
        ]
        
        self.evaluation_stats['ttl_expired'] += expired_count
        self.last_ttl_cleanup = current_time
        
        if expired_count > 0:
            logger.info(f"TTL ë§Œë£Œ í‰ê°€ ì •ë¦¬ ì™„ë£Œ: {expired_count}ê°œ ì œê±°")
    
    def enforce_max_candidates(self):
        """ìµœëŒ€ Dream í›„ë³´ ìˆ˜ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ì˜¤ë˜ëœ ê²ƒë¶€í„° ì œê±°í•©ë‹ˆë‹¤."""
        if len(self.evaluation_history) > self.evaluation_criteria.max_dream_candidates:
            # ì˜¤ë˜ëœ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ ì´ˆê³¼ë¶„ ì œê±°
            sorted_evaluations = sorted(self.evaluation_history, 
                                      key=lambda x: x.evaluation_time)
            excess_count = len(self.evaluation_history) - self.evaluation_criteria.max_dream_candidates
            
            self.evaluation_history = sorted_evaluations[excess_count:]
            logger.info(f"ìµœëŒ€ í›„ë³´ ìˆ˜ ì´ˆê³¼ë¡œ {excess_count}ê°œ ì œê±°")

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_core_eval = None

def get_core_eval() -> CoreEval:
    """CoreEval ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _core_eval
    if _core_eval is None:
        _core_eval = CoreEval()
    return _core_eval 