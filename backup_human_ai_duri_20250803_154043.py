#!/usr/bin/env python3
"""
ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ì´ í†µí•©ëœ DuRi ìê°€ì§„í™” AI ì‹œìŠ¤í…œ (ì¸ê°„í˜• AI ëª¨ë“ˆ ì¶”ê°€)
"""

import functools
import os
import sys
import time
from datetime import datetime
from typing import Any, Dict, Optional

import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware

# ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), "duri_modules"))

# ëª¨ë“ˆí™”ëœ ì‹œìŠ¤í…œ import
from duri_modules import (
    chatgpt_evaluator,
    context_analyzer,
    conversation_store,
    dashboard_generator,
    duri_chatgpt_discussion,
    duri_self_reflector,
    emotion_analyzer,
    intuitive_judgment,
    meta_loop_system,
    performance_tracker,
)

app = FastAPI(
    title="DuRi Monitored Self-Evolving AI System",
    description="ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ì´ í†µí•©ëœ ìê°€ì§„í™” AI ì‹œìŠ¤í…œ (ì¸ê°„í˜• AI ëª¨ë“ˆ í¬í•¨)",
    version="4.0.0",
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def track_performance(endpoint_name: str):
    """ì„±ëŠ¥ ì¶”ì  ë°ì½”ë ˆì´í„°"""

    def decorator(func):
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            success = True
            error_message = None

            try:
                result = await func(*args, **kwargs)
                return result
            except Exception as e:
                success = False
                error_message = str(e)
                raise
            finally:
                response_time = time.time() - start_time
                performance_tracker.track_request(
                    endpoint_name, response_time, success, error_message
                )

        return wrapper

    return decorator


@app.get("/health")
@track_performance("health_check")
async def health_check():
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ (ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í¬í•¨)"""
    health_data = performance_tracker.get_health_check()

    return {
        "status": "healthy",
        "service": "duri-monitored-system",
        "version": "4.0.0",
        "timestamp": datetime.now().isoformat(),
        "modules": {
            "evaluation": "âœ… loaded",
            "reflection": "âœ… loaded",
            "discussion": "âœ… loaded",
            "data_store": "âœ… loaded",
            "performance_tracker": "âœ… loaded",
            "context_analyzer": "âœ… loaded",
            "intuitive_judgment": "âœ… loaded",
            "emotion_analyzer": "âœ… loaded",
        },
        "performance": health_data,
    }


@app.post("/context-analyze")
@track_performance("context_analyze")
async def analyze_context(context_request: Dict[str, Any]):
    """ë§¥ë½ ë¶„ì„ (ì¸ê°„í˜• AI ëª¨ë“ˆ)"""
    try:
        conversation_history = context_request.get("conversation_history", [])

        # ë§¥ë½ ë¶„ì„ ìˆ˜í–‰
        context_result = context_analyzer.analyze_conversation_context(
            conversation_history
        )

        return {
            "status": "success",
            "context_analysis": context_result,
            "message": "ë§¥ë½ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        print(f"âŒ ë§¥ë½ ë¶„ì„ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/intuitive-judgment")
@track_performance("intuitive_judgment")
async def trigger_intuitive_judgment(intuitive_request: Dict[str, Any]):
    """ì§ê´€ì  íŒë‹¨ íŠ¸ë¦¬ê±° (ì¸ê°„í˜• AI ëª¨ë“ˆ)"""
    try:
        user_input = intuitive_request.get("user_input", "")
        context = intuitive_request.get("context", {})

        if not user_input:
            raise HTTPException(status_code=400, detail="user_inputì´ í•„ìš”í•©ë‹ˆë‹¤")

        # ì§ê´€ì  íŒë‹¨ íŠ¸ë¦¬ê±°
        intuitive_result = intuitive_judgment.trigger_intuitive_response(
            user_input, context
        )

        return {
            "status": "success",
            "intuitive_judgment": intuitive_result,
            "should_trigger": intuitive_judgment.should_trigger_intuition(
                user_input, context
            ),
            "message": "ì§ê´€ì  íŒë‹¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        print(f"âŒ ì§ê´€ì  íŒë‹¨ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/emotion-analyze")
@track_performance("emotion_analyze")
async def analyze_emotion(emotion_request: Dict[str, Any]):
    """ê°ì • ë¶„ì„ (ì¸ê°„í˜• AI ëª¨ë“ˆ)"""
    try:
        text = emotion_request.get("text", "")
        context = emotion_request.get("context", None)

        if not text:
            raise HTTPException(status_code=400, detail="textê°€ í•„ìš”í•©ë‹ˆë‹¤")

        # ê°ì • ë¶„ì„ ìˆ˜í–‰
        emotion_result = emotion_analyzer.analyze_user_emotion(text, context)

        # ê°ì •ì— ì ì‘í•œ ì‘ë‹µ ìƒì„±
        adaptive_response = emotion_analyzer.generate_emotion_adaptive_response(
            emotion_result["primary_emotion"], context
        )

        return {
            "status": "success",
            "emotion_analysis": emotion_result,
            "adaptive_response": adaptive_response,
            "message": "ê°ì • ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        print(f"âŒ ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/human-ai-response")
@track_performance("human_ai_response")
async def generate_human_ai_response(human_request: Dict[str, Any]):
    """ì¸ê°„í˜• AI ì‘ë‹µ ìƒì„± (í†µí•© ëª¨ë“ˆ)"""
    try:
        user_input = human_request.get("user_input", "")
        conversation_history = human_request.get("conversation_history", [])

        if not user_input:
            raise HTTPException(status_code=400, detail="user_inputì´ í•„ìš”í•©ë‹ˆë‹¤")

        # 1. ë§¥ë½ ë¶„ì„
        context_result = context_analyzer.analyze_conversation_context(
            conversation_history
        )

        # 2. ê°ì • ë¶„ì„
        emotion_result = emotion_analyzer.analyze_user_emotion(
            user_input, context_result
        )

        # 3. ì§ê´€ì  íŒë‹¨ íŠ¸ë¦¬ê±°
        intuitive_result = intuitive_judgment.trigger_intuitive_response(
            user_input, context_result
        )

        # 4. ê°ì •ì— ì ì‘í•œ ì‘ë‹µ ìƒì„±
        adaptive_response = emotion_analyzer.generate_emotion_adaptive_response(
            emotion_result["primary_emotion"], context_result
        )

        # 5. í†µí•© ì‘ë‹µ ìƒì„±
        integrated_response = _generate_integrated_response(
            user_input,
            context_result,
            emotion_result,
            intuitive_result,
            adaptive_response,
        )

        return {
            "status": "success",
            "integrated_response": integrated_response,
            "context_analysis": context_result,
            "emotion_analysis": emotion_result,
            "intuitive_judgment": intuitive_result,
            "adaptive_response": adaptive_response,
            "message": "ì¸ê°„í˜• AI ì‘ë‹µì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤",
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        print(f"âŒ ì¸ê°„í˜• AI ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


def _generate_integrated_response(
    user_input: str,
    context: Dict,
    emotion: Dict,
    intuitive: Optional[Dict],
    adaptive: Dict,
) -> Dict[str, Any]:
    """í†µí•© ì‘ë‹µ ìƒì„±"""

    # ê¸°ë³¸ ì‘ë‹µ í…œí”Œë¦¿
    base_response = (
        adaptive["suggested_phrases"][0]
        if adaptive["suggested_phrases"]
        else "ì•Œê² ìŠµë‹ˆë‹¤. ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤."
    )

    # ì§ê´€ì  íŒë‹¨ì´ ìˆì„ ê²½ìš° í†µí•©
    if intuitive and intuitive["confidence"] > 0.7:
        integrated_text = f"{intuitive['response']} {base_response}"
    else:
        integrated_text = base_response

    # ë§¥ë½ ì •ë³´ ì¶”ê°€
    context_summary = context.get("context_summary", "")

    return {
        "response_text": integrated_text,
        "tone": adaptive["tone"],
        "style": adaptive["style"],
        "context_summary": context_summary,
        "emotion": emotion["primary_emotion"],
        "intensity": emotion["intensity"],
        "confidence": {
            "context": context["confidence"],
            "emotion": emotion["confidence"],
            "intuitive": intuitive["confidence"] if intuitive else 0.0,
        },
        "human_like_indicators": {
            "context_aware": context["confidence"] > 0.7,
            "emotion_adaptive": emotion["confidence"] > 0.6,
            "intuitive_triggered": intuitive is not None,
            "natural_flow": context["conversation_flow"]["flow_type"] != "single",
        },
    }


@app.post("/chatgpt-evaluate")
@track_performance("chatgpt_evaluate")
async def chatgpt_evaluate_response(evaluation_request: Dict[str, Any]):
    """ChatGPT 6ì°¨ì› í‰ê°€ (ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í¬í•¨)"""
    try:
        duri_response = evaluation_request.get("duri_response", "")
        user_question = evaluation_request.get("user_question", "")

        if not duri_response or not user_question:
            raise HTTPException(
                status_code=400, detail="duri_responseì™€ user_questionì´ í•„ìš”í•©ë‹ˆë‹¤"
            )

        # ëª¨ë“ˆí™”ëœ í‰ê°€ ì‹œìŠ¤í…œ ì‚¬ìš©
        evaluation_result = chatgpt_evaluator.evaluate_response(
            duri_response, user_question
        )

        # í•™ìŠµ ë©”íŠ¸ë¦­ ì¶”ì 
        performance_tracker.track_learning_metric(
            "evaluation_score",
            evaluation_result.get("total_score", 0.0),
            {"user_question": user_question[:50]},
        )

        return {
            "status": "success",
            "evaluation": evaluation_result,
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        print(f"âŒ ChatGPT í‰ê°€ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/duri-self-reflect")
@track_performance("duri_self_reflect")
async def duri_self_reflect_endpoint(reflection_request: Dict[str, Any]):
    """DuRi ìê¸°ì„±ì°° (ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í¬í•¨)"""
    try:
        chatgpt_evaluation = reflection_request.get("chatgpt_evaluation", {})
        original_response = reflection_request.get("original_response", "")
        user_question = reflection_request.get("user_question", "")

        if not chatgpt_evaluation or not original_response:
            raise HTTPException(
                status_code=400,
                detail="chatgpt_evaluationê³¼ original_responseê°€ í•„ìš”í•©ë‹ˆë‹¤",
            )

        # ëª¨ë“ˆí™”ëœ ìê¸°ì„±ì°° ì‹œìŠ¤í…œ ì‚¬ìš©
        reflection_result = duri_self_reflector.reflect_on_chatgpt_feedback(
            chatgpt_evaluation, original_response, user_question
        )

        # í•™ìŠµ ë©”íŠ¸ë¦­ ì¶”ì 
        improvement_count = len(
            reflection_result.get("improvement_proposal", {}).get(
                "specific_improvements", []
            )
        )
        performance_tracker.track_learning_metric(
            "improvement_suggestions",
            improvement_count,
            {"reflection_depth": len(reflection_result.get("accepted_criticisms", []))},
        )

        return {
            "status": "success",
            "reflection": reflection_result,
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        print(f"âŒ DuRi ìê¸°ì„±ì°° ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/capture-conversation")
@track_performance("capture_conversation")
async def capture_conversation_endpoint(conversation_data: Dict[str, Any]):
    """ì‹¤ì œ ëŒ€í™” ë°ì´í„° ìˆ˜ì§‘ (ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í¬í•¨)"""
    try:
        user_input = conversation_data.get("user_input", "")
        duri_response = conversation_data.get("duri_response", "")
        metadata = conversation_data.get("metadata", {})

        if not user_input or not duri_response:
            raise HTTPException(
                status_code=400, detail="user_inputê³¼ duri_responseê°€ í•„ìš”í•©ë‹ˆë‹¤"
            )

        # ëŒ€í™” ë°ì´í„° ì €ì¥
        conversation_id = conversation_store.store_conversation(
            user_input, duri_response, metadata
        )

        # í•™ìŠµ ë©”íŠ¸ë¦­ ì¶”ì 
        learning_value = conversation_store._calculate_learning_value(
            user_input, duri_response
        )
        performance_tracker.track_learning_metric(
            "conversation_learning_value",
            learning_value,
            {"conversation_id": conversation_id},
        )

        # ìë™ í•™ìŠµ ë£¨í”„ ì‹œì‘ (ê¸°ë³¸ í™œì„±í™”)
        auto_learn = conversation_data.get("auto_learn", True)  # ê¸°ë³¸ê°’ì„ Trueë¡œ ë³€ê²½

        if auto_learn:
            print(f"ğŸ”„ ìë™ í•™ìŠµ ë£¨í”„ ì‹œì‘: {conversation_id}")

            # 1ë‹¨ê³„: ChatGPT í‰ê°€
            evaluation_result = chatgpt_evaluator.evaluate_response(
                duri_response, user_input
            )
            print(
                f"   ğŸ“Š ChatGPT í‰ê°€ ì™„ë£Œ: ì´ì  {evaluation_result.get('total_score', 0):.3f}"
            )

            # 2ë‹¨ê³„: DuRi ìê¸°ì„±ì°°
            reflection_result = duri_self_reflector.reflect_on_chatgpt_feedback(
                evaluation_result, duri_response, user_input
            )
            print(
                f"   ğŸ¤” DuRi ìê¸°ì„±ì°° ì™„ë£Œ: {len(reflection_result.get('improvement_proposal', {}).get('specific_improvements', []))}ê°œ ê°œì„ ì•ˆ"
            )

            # 3ë‹¨ê³„: DuRi-ChatGPT ë…¼ì˜ (ì„ íƒì )
            discussion_result = None
            if evaluation_result.get("total_score", 0) < 0.5:  # ë‚®ì€ ì ìˆ˜ì¼ ë•Œë§Œ ë…¼ì˜
                discussion_result = duri_chatgpt_discussion.initiate_discussion(
                    reflection_result.get("improvement_proposal", {}), evaluation_result
                )
                print(
                    f"   ğŸ“¥ DuRi-ChatGPT ë…¼ì˜ ì™„ë£Œ: í•©ì˜ ìˆ˜ì¤€ {discussion_result.get('agreement_level', 0):.2f}"
                )

            # 4ë‹¨ê³„: í•™ìŠµ ê²°ê³¼ ì €ì¥ ë° ë©”íƒ€ ë£¨í”„ ì¤€ë¹„
            learning_data = {
                "conversation_id": conversation_id,
                "evaluation": evaluation_result,
                "reflection": reflection_result,
                "discussion": discussion_result,
                "auto_learn_enabled": True,
                "learning_cycle_completed": True,
                "timestamp": datetime.now().isoformat(),
            }

            # ë©”íƒ€ í•™ìŠµ ë©”íŠ¸ë¦­ ì¶”ì 
            performance_tracker.track_learning_metric(
                "learning_cycle_completion",
                1.0,
                {
                    "conversation_id": conversation_id,
                    "evaluation_score": evaluation_result.get("total_score", 0),
                    "improvement_count": len(
                        reflection_result.get("improvement_proposal", {}).get(
                            "specific_improvements", []
                        )
                    ),
                },
            )

            return {
                "status": "success",
                "conversation_id": conversation_id,
                "message": "ëŒ€í™” ì €ì¥ ë° ìë™ í•™ìŠµ ë£¨í”„ ì™„ë£Œ",
                "learning_data": learning_data,
                "learning_summary": {
                    "evaluation_score": evaluation_result.get("total_score", 0),
                    "improvement_suggestions": len(
                        reflection_result.get("improvement_proposal", {}).get(
                            "specific_improvements", []
                        )
                    ),
                    "discussion_held": discussion_result is not None,
                    "cycle_completed": True,
                },
                "timestamp": datetime.now().isoformat(),
            }

        return {
            "status": "success",
            "conversation_id": conversation_id,
            "message": "ëŒ€í™” ì €ì¥ ì™„ë£Œ",
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        print(f"âŒ ëŒ€í™” ì €ì¥ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/performance-summary")
async def get_performance_summary():
    """ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ"""
    try:
        summary = performance_tracker.get_performance_summary()
        return {
            "status": "success",
            "summary": summary,
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        print(f"âŒ ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/system-health")
async def get_system_health():
    """ì‹œìŠ¤í…œ ê±´ê°•ë„ í™•ì¸"""
    try:
        health_data = performance_tracker.get_health_check()
        return {
            "status": "success",
            "health": health_data,
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ê±´ê°•ë„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/learning-statistics")
async def get_learning_statistics():
    """í•™ìŠµ í†µê³„ ì¡°íšŒ"""
    try:
        stats = conversation_store.get_learning_statistics()
        patterns = conversation_store.extract_learning_patterns()

        return {
            "status": "success",
            "statistics": stats,
            "learning_patterns": patterns,
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        print(f"âŒ í•™ìŠµ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/meta-evaluate-improvement")
@track_performance("meta_evaluate_improvement")
async def meta_evaluate_improvement_endpoint(meta_request: Dict[str, Any]):
    """ë©”íƒ€ ë£¨í”„: ê°œì„  íš¨ê³¼ í‰ê°€"""
    try:
        original_response = meta_request.get("original_response", "")
        improved_response = meta_request.get("improved_response", "")
        user_question = meta_request.get("user_question", "")
        original_evaluation = meta_request.get("original_evaluation", {})

        if not original_response or not improved_response or not user_question:
            raise HTTPException(
                status_code=400,
                detail="original_response, improved_response, user_questionì´ í•„ìš”í•©ë‹ˆë‹¤",
            )

        # ë©”íƒ€ ë£¨í”„ ì‹œìŠ¤í…œ ì‚¬ìš©
        meta_result = meta_loop_system.evaluate_improvement_effect(
            original_response, improved_response, user_question, original_evaluation
        )

        # ê°œì„  í”¼ë“œë°± ìƒì„±
        feedback = meta_loop_system.generate_improvement_feedback(meta_result)

        # í•™ìŠµ ë©”íŠ¸ë¦­ ì¶”ì 
        improvement_score = meta_result.get("meta_score", 0)
        performance_tracker.track_learning_metric(
            "meta_improvement_score",
            improvement_score,
            {
                "improvement_success": feedback.get("improvement_success", False),
                "overall_improvement": feedback.get("overall_improvement", 0),
            },
        )

        return {
            "status": "success",
            "meta_evaluation": meta_result,
            "feedback": feedback,
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        print(f"âŒ ë©”íƒ€ í‰ê°€ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/meta-learning-statistics")
async def get_meta_learning_statistics():
    """ë©”íƒ€ í•™ìŠµ í†µê³„ ì¡°íšŒ"""
    try:
        meta_stats = meta_loop_system.get_meta_learning_statistics()

        return {
            "status": "success",
            "meta_learning_statistics": meta_stats,
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        print(f"âŒ ë©”íƒ€ í•™ìŠµ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/dashboard")
async def get_dashboard():
    """ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
    try:
        # í•„ìš”í•œ ë°ì´í„° ìˆ˜ì§‘
        performance_summary = performance_tracker.get_performance_summary()
        learning_stats = conversation_store.get_learning_statistics()
        meta_stats = meta_loop_system.get_meta_learning_statistics()

        # ëŒ€ì‹œë³´ë“œ ìƒì„±
        dashboard_path = dashboard_generator.generate_dashboard(
            performance_summary, learning_stats, meta_stats
        )

        return {
            "status": "success",
            "dashboard_path": dashboard_path,
            "dashboard_url": f"file://{dashboard_path}",
            "message": "ëŒ€ì‹œë³´ë“œê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤",
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        print(f"âŒ ëŒ€ì‹œë³´ë“œ ìƒì„± ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8088)
