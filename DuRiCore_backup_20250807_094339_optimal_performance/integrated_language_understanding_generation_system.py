#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DuRi Phase 1-3 Week 3 Day 12 - í†µí•© ì–¸ì–´ ì´í•´ ë° ìƒì„± ì‹œìŠ¤í…œ

ê¸°ì¡´ ì–¸ì–´ ê´€ë ¨ ì‹œìŠ¤í…œë“¤ì„ í†µí•©í•˜ê³  ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì—¬ ì™„ì „í•œ ì–¸ì–´ ì´í•´ ë° ìƒì„± ì‹œìŠ¤í…œ êµ¬í˜„
- ì‹¬ì¸µ ì–¸ì–´ ì´í•´: ë§¥ë½ ê¸°ë°˜ ëŒ€í™” ë° ê°ì •ì  ì–¸ì–´ í‘œí˜„
- ìì—°ì–´ ì²˜ë¦¬ ê³ ë„í™”: ê³ ê¸‰ ìì—°ì–´ ì²˜ë¦¬ ëŠ¥ë ¥
- ê°ì •ì  ì–¸ì–´ í‘œí˜„: ê°ì •ì„ ë‹´ì€ ìì—°ìŠ¤ëŸ¬ìš´ ì–¸ì–´ ìƒì„±
- ë‹¤êµ­ì–´ ì²˜ë¦¬ ëŠ¥ë ¥: ë‹¤ì–‘í•œ ì–¸ì–´ ì²˜ë¦¬ ë° ìƒì„±
"""

import asyncio
import hashlib
import json
import logging
import re
import time
import unicodedata
from collections import Counter, defaultdict
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
from integrated_social_intelligence_system import IntegratedSocialIntelligenceSystem

# ê¸°ì¡´ ì‹œìŠ¤í…œë“¤ import
from natural_language_processing_system import NaturalLanguageProcessingSystem

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LanguageUnderstandingType(Enum):
    """ì–¸ì–´ ì´í•´ ìœ í˜•"""

    CONVERSATION_ANALYSIS = "conversation_analysis"  # ëŒ€í™” ë¶„ì„
    INTENT_RECOGNITION = "intent_recognition"  # ì˜ë„ ì¸ì‹
    CONTEXT_UNDERSTANDING = "context_understanding"  # ë§¥ë½ ì´í•´
    SEMANTIC_ANALYSIS = "semantic_analysis"  # ì˜ë¯¸ ë¶„ì„
    EMOTION_DETECTION = "emotion_detection"  # ê°ì • ê°ì§€
    MULTILINGUAL_PROCESSING = "multilingual_processing"  # ë‹¤êµ­ì–´ ì²˜ë¦¬


class LanguageGenerationType(Enum):
    """ì–¸ì–´ ìƒì„± ìœ í˜•"""

    CONVERSATIONAL_RESPONSE = "conversational_response"  # ëŒ€í™” ì‘ë‹µ
    EMOTIONAL_EXPRESSION = "emotional_expression"  # ê°ì •ì  í‘œí˜„
    CONTEXTUAL_GENERATION = "contextual_generation"  # ë§¥ë½ ê¸°ë°˜ ìƒì„±
    MULTILINGUAL_GENERATION = "multilingual_generation"  # ë‹¤êµ­ì–´ ìƒì„±
    CREATIVE_WRITING = "creative_writing"  # ì°½ì˜ì  ê¸€ì“°ê¸°


@dataclass
class LanguageUnderstandingResult:
    """ì–¸ì–´ ì´í•´ ê²°ê³¼ ë°ì´í„° êµ¬ì¡°"""

    understanding_id: str
    source_text: str
    understanding_type: LanguageUnderstandingType
    intent: str
    key_concepts: List[str]
    emotional_tone: str
    context_meaning: str
    learning_insights: List[str]
    confidence_score: float
    multilingual_analysis: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)


@dataclass
class LanguageGenerationResult:
    """ì–¸ì–´ ìƒì„± ê²°ê³¼ ë°ì´í„° êµ¬ì¡°"""

    generation_id: str
    source_context: Dict[str, Any]
    generation_type: LanguageGenerationType
    generated_text: str
    emotional_expression: str
    contextual_relevance: float
    confidence_score: float
    multilingual_support: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)


@dataclass
class IntegratedLanguageResult:
    """í†µí•© ì–¸ì–´ ì²˜ë¦¬ ê²°ê³¼ ë°ì´í„° êµ¬ì¡°"""

    result_id: str
    understanding_result: LanguageUnderstandingResult
    generation_result: LanguageGenerationResult
    integration_score: float
    system_performance: Dict[str, Any]
    created_at: datetime = field(default_factory=datetime.now)


class DeepLanguageUnderstandingEngine:
    """ì‹¬ì¸µ ì–¸ì–´ ì´í•´ ì—”ì§„"""

    def __init__(self):
        self.understanding_cache = {}
        self.context_analyzer = ContextAnalyzer()
        self.emotion_analyzer = EmotionAnalyzer()
        self.intent_recognizer = IntentRecognizer()
        self.semantic_analyzer = SemanticAnalyzer()
        self.multilingual_processor = MultilingualProcessor()

    async def understand_language(
        self, text: str, context: Dict[str, Any] = None
    ) -> LanguageUnderstandingResult:
        """ì‹¬ì¸µ ì–¸ì–´ ì´í•´"""
        understanding_id = f"understanding_{int(time.time())}"

        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(
            f"{text}_{json.dumps(context, sort_keys=True)}".encode()
        ).hexdigest()
        if cache_key in self.understanding_cache:
            return self.understanding_cache[cache_key]

        # 1. ë§¥ë½ ë¶„ì„
        context_analysis = await self.context_analyzer.analyze_context(text, context)

        # 2. ê°ì • ë¶„ì„
        emotion_analysis = await self.emotion_analyzer.analyze_emotion(text, context)

        # 3. ì˜ë„ ì¸ì‹
        intent_analysis = await self.intent_recognizer.recognize_intent(text, context)

        # 4. ì˜ë¯¸ ë¶„ì„
        semantic_analysis = await self.semantic_analyzer.analyze_semantics(
            text, context
        )

        # 5. ë‹¤êµ­ì–´ ì²˜ë¦¬
        multilingual_analysis = await self.multilingual_processor.process_multilingual(
            text, context
        )

        # 6. í†µí•© ë¶„ì„
        understanding_result = LanguageUnderstandingResult(
            understanding_id=understanding_id,
            source_text=text,
            understanding_type=LanguageUnderstandingType.CONVERSATION_ANALYSIS,
            intent=intent_analysis.get("primary_intent", ""),
            key_concepts=semantic_analysis.get("key_concepts", []),
            emotional_tone=emotion_analysis.get("primary_emotion", ""),
            context_meaning=context_analysis.get("context_meaning", ""),
            learning_insights=semantic_analysis.get("learning_insights", []),
            confidence_score=self._calculate_understanding_confidence(
                context_analysis, emotion_analysis, intent_analysis, semantic_analysis
            ),
            multilingual_analysis=multilingual_analysis,
        )

        # ìºì‹œ ì €ì¥
        self.understanding_cache[cache_key] = understanding_result

        return understanding_result

    def _calculate_understanding_confidence(
        self,
        context_analysis: Dict,
        emotion_analysis: Dict,
        intent_analysis: Dict,
        semantic_analysis: Dict,
    ) -> float:
        """ì´í•´ ì‹ ë¢°ë„ ê³„ì‚°"""
        scores = [
            context_analysis.get("confidence", 0.0),
            emotion_analysis.get("confidence", 0.0),
            intent_analysis.get("confidence", 0.0),
            semantic_analysis.get("confidence", 0.0),
        ]
        return np.mean(scores)


class AdvancedLanguageGenerationEngine:
    """ê³ ê¸‰ ì–¸ì–´ ìƒì„± ì—”ì§„"""

    def __init__(self):
        self.generation_cache = {}
        self.conversational_generator = ConversationalGenerator()
        self.emotional_generator = EmotionalGenerator()
        self.contextual_generator = ContextualGenerator()
        self.multilingual_generator = MultilingualGenerator()
        self.creative_generator = CreativeGenerator()

    async def generate_language(
        self, context: Dict[str, Any], generation_type: LanguageGenerationType
    ) -> LanguageGenerationResult:
        """ê³ ê¸‰ ì–¸ì–´ ìƒì„± (ì˜ë¯¸ ë¶„ì„ ê²°ê³¼ ë°˜ì˜ ê°•í™”)"""
        generation_id = f"generation_{int(time.time())}"

        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(
            f"{json.dumps(context, sort_keys=True)}_{generation_type.value}".encode()
        ).hexdigest()
        if cache_key in self.generation_cache:
            return self.generation_cache[cache_key]

        # ì˜ë¯¸ ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ (ìƒˆë¡œ ì¶”ê°€)
        semantic_analysis = context.get("semantic_analysis", {})
        learning_insights = context.get("learning_insights", [])
        key_concepts = context.get("keywords", [])
        confidence_score = context.get("confidence_score", 0.5)

        # ì˜ë¯¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ë°˜ì˜
        enhanced_context = context.copy()
        if semantic_analysis:
            enhanced_context.update(semantic_analysis)
        if learning_insights:
            enhanced_context["learning_insights"] = learning_insights
        if key_concepts:
            enhanced_context["key_concepts"] = key_concepts
        enhanced_context["semantic_confidence"] = confidence_score

        # ìƒì„± ìœ í˜•ì— ë”°ë¥¸ ì²˜ë¦¬
        if generation_type == LanguageGenerationType.CONVERSATIONAL_RESPONSE:
            generated_text = await self.conversational_generator.generate_response(
                enhanced_context
            )
        elif generation_type == LanguageGenerationType.EMOTIONAL_EXPRESSION:
            generated_text = (
                await self.emotional_generator.generate_emotional_expression(
                    enhanced_context
                )
            )
        elif generation_type == LanguageGenerationType.CONTEXTUAL_GENERATION:
            generated_text = await self.contextual_generator.generate_contextual_text(
                enhanced_context
            )
        elif generation_type == LanguageGenerationType.MULTILINGUAL_GENERATION:
            generated_text = (
                await self.multilingual_generator.generate_multilingual_text(
                    enhanced_context
                )
            )
        elif generation_type == LanguageGenerationType.CREATIVE_WRITING:
            generated_text = await self.creative_generator.generate_creative_text(
                enhanced_context
            )
        else:
            generated_text = await self.conversational_generator.generate_response(
                enhanced_context
            )

        # ê°ì •ì  í‘œí˜„ ë¶„ì„
        emotional_expression = (
            await self.emotional_generator.analyze_emotional_expression(generated_text)
        )

        # ë§¥ë½ ê´€ë ¨ì„± í‰ê°€ (ì˜ë¯¸ ë¶„ì„ ê²°ê³¼ ë°˜ì˜)
        contextual_relevance = (
            await self.contextual_generator.evaluate_contextual_relevance(
                generated_text, enhanced_context
            )
        )

        # ë‹¤êµ­ì–´ ì§€ì›
        multilingual_support = (
            await self.multilingual_generator.get_multilingual_support(
                generated_text, enhanced_context
            )
        )

        # ì‹ ë¢°ë„ ê³„ì‚° (ì˜ë¯¸ ë¶„ì„ ê²°ê³¼ ë°˜ì˜)
        confidence_score = self._calculate_generation_confidence(
            generated_text, emotional_expression, contextual_relevance, enhanced_context
        )

        generation_result = LanguageGenerationResult(
            generation_id=generation_id,
            source_context=enhanced_context,
            generation_type=generation_type,
            generated_text=generated_text,
            emotional_expression=emotional_expression,
            contextual_relevance=contextual_relevance,
            multilingual_support=multilingual_support,
            confidence_score=confidence_score,
        )

        # ìºì‹œ ì €ì¥
        self.generation_cache[cache_key] = generation_result

        return generation_result

    def _calculate_generation_confidence(
        self,
        generated_text: str,
        emotional_expression: str,
        contextual_relevance: float,
        context: Dict[str, Any] = None,
    ) -> float:
        """ìƒì„± ì‹ ë¢°ë„ ê³„ì‚° (ì˜ë¯¸ ë¶„ì„ ê²°ê³¼ ë°˜ì˜)"""
        try:
            # í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€
            text_quality = min(
                1.0, len(generated_text.strip()) / 100.0
            )  # ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜

            # ê°ì •ì  í‘œí˜„ í‰ê°€
            emotion_quality = 1.0 if emotional_expression else 0.5

            # ë§¥ë½ ê´€ë ¨ì„± í‰ê°€
            context_quality = max(0.0, min(1.0, contextual_relevance))

            # ì˜ë¯¸ ë¶„ì„ ê²°ê³¼ ë°˜ì˜ (ìƒˆë¡œ ì¶”ê°€)
            semantic_quality = 0.5  # ê¸°ë³¸ê°’
            if context:
                # í‚¤ ì»¨ì…‰ ë°˜ì˜
                key_concepts = context.get("key_concepts", [])
                if key_concepts:
                    semantic_quality = min(
                        1.0, semantic_quality + len(key_concepts) * 0.1
                    )

                # í•™ìŠµ í†µì°° ë°˜ì˜
                learning_insights = context.get("learning_insights", [])
                if learning_insights:
                    semantic_quality = min(
                        1.0, semantic_quality + len(learning_insights) * 0.1
                    )

                # ì˜ë¯¸ ë¶„ì„ ì‹ ë¢°ë„ ë°˜ì˜
                semantic_confidence = context.get("semantic_confidence", 0.5)
                semantic_quality = min(
                    1.0, semantic_quality + semantic_confidence * 0.2
                )

            # í†µí•© ì‹ ë¢°ë„ (ì˜ë¯¸ ë¶„ì„ ê°€ì¤‘ì¹˜ ì¶”ê°€)
            confidence = (
                text_quality * 0.3  # í…ìŠ¤íŠ¸ í’ˆì§ˆ (30%)
                + emotion_quality * 0.2  # ê°ì •ì  í‘œí˜„ (20%)
                + context_quality * 0.3  # ë§¥ë½ ê´€ë ¨ì„± (30%)
                + semantic_quality * 0.2  # ì˜ë¯¸ ë¶„ì„ (20%)
            )

            return max(0.0, min(1.0, confidence))

        except Exception as e:
            logger.error(f"ìƒì„± ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5  # ê¸°ë³¸ê°’ ë°˜í™˜


class ContextAnalyzer:
    """ë§¥ë½ ë¶„ì„ê¸°"""

    async def analyze_context(
        self, text: str, context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ë§¥ë½ ë¶„ì„"""
        # ì‹œê°„ì  ë§¥ë½
        temporal_context = self._extract_temporal_context(text)

        # ê³µê°„ì  ë§¥ë½
        spatial_context = self._extract_spatial_context(text)

        # ì‚¬íšŒì  ë§¥ë½
        social_context = self._extract_social_context(text)

        # ì£¼ì œì  ë§¥ë½
        topical_context = self._extract_topical_context(text)

        # ê°ì •ì  ë§¥ë½
        emotional_context = self._extract_emotional_context(text)

        # í†µí•© ë§¥ë½ ì˜ë¯¸
        context_meaning = self._integrate_context_meaning(
            temporal_context,
            spatial_context,
            social_context,
            topical_context,
            emotional_context,
        )

        return {
            "temporal_context": temporal_context,
            "spatial_context": spatial_context,
            "social_context": social_context,
            "topical_context": topical_context,
            "emotional_context": emotional_context,
            "context_meaning": context_meaning,
            "confidence": 0.85,
        }

    def _extract_temporal_context(self, text: str) -> Dict[str, Any]:
        """ì‹œê°„ì  ë§¥ë½ ì¶”ì¶œ"""
        # ì‹œê°„ ê´€ë ¨ í‚¤ì›Œë“œ íŒ¨í„´
        time_patterns = [
            r"ì˜¤ëŠ˜|ì–´ì œ|ë‚´ì¼|ì´ë²ˆ|ë‹¤ìŒ|ì§€ë‚œ|ì´ì „",
            r"ë…„|ì›”|ì¼|ì‹œ|ë¶„|ì´ˆ",
            r"ì•„ì¹¨|ì ì‹¬|ì €ë…|ë°¤|ìƒˆë²½",
        ]

        temporal_keywords = []
        for pattern in time_patterns:
            matches = re.findall(pattern, text)
            temporal_keywords.extend(matches)

        return {
            "temporal_keywords": temporal_keywords,
            "temporal_relevance": len(temporal_keywords) / len(text.split()),
        }

    def _extract_spatial_context(self, text: str) -> Dict[str, Any]:
        """ê³µê°„ì  ë§¥ë½ ì¶”ì¶œ"""
        # ê³µê°„ ê´€ë ¨ í‚¤ì›Œë“œ íŒ¨í„´
        spatial_patterns = [
            r"ì§‘|í•™êµ|íšŒì‚¬|ë³‘ì›|ìƒì |ê³µì›|ì—­|ê³µí•­",
            r"ìœ„|ì•„ë˜|ì•|ë’¤|ì™¼ìª½|ì˜¤ë¥¸ìª½|ì•ˆ|ë°–",
            r"ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°",
        ]

        spatial_keywords = []
        for pattern in spatial_patterns:
            matches = re.findall(pattern, text)
            spatial_keywords.extend(matches)

        return {
            "spatial_keywords": spatial_keywords,
            "spatial_relevance": len(spatial_keywords) / len(text.split()),
        }

    def _extract_social_context(self, text: str) -> Dict[str, Any]:
        """ì‚¬íšŒì  ë§¥ë½ ì¶”ì¶œ"""
        # ì‚¬íšŒì  ê´€ê³„ í‚¤ì›Œë“œ
        social_keywords = re.findall(
            r"ê°€ì¡±|ì¹œêµ¬|ë™ë£Œ|ì„ ìƒë‹˜|í•™ìƒ|ë¶€ëª¨|ìì‹|í˜•ì œ|ìë§¤", text
        )

        return {
            "social_keywords": social_keywords,
            "social_relevance": len(social_keywords) / len(text.split()),
        }

    def _extract_topical_context(self, text: str) -> Dict[str, Any]:
        """ì£¼ì œì  ë§¥ë½ ì¶”ì¶œ"""
        # ì£¼ì œ ê´€ë ¨ í‚¤ì›Œë“œ
        topic_keywords = re.findall(
            r"í•™ìŠµ|êµìœ¡|ì¼|ì·¨ë¯¸|ìš´ë™|ìŒì‹|ì—¬í–‰|ì˜í™”|ìŒì•…|ì±…", text
        )

        return {
            "topic_keywords": topic_keywords,
            "topic_relevance": len(topic_keywords) / len(text.split()),
        }

    def _extract_emotional_context(self, text: str) -> Dict[str, Any]:
        """ê°ì •ì  ë§¥ë½ ì¶”ì¶œ"""
        # ê°ì • ê´€ë ¨ í‚¤ì›Œë“œ
        emotion_keywords = re.findall(
            r"ê¸°ì¨|ìŠ¬í””|í™”ë‚¨|ë†€ëŒ|ë‘ë ¤ì›€|ì‚¬ë‘|ë¯¸ì›€|í¬ë§|ì ˆë§|ê°ì‚¬", text
        )

        return {
            "emotion_keywords": emotion_keywords,
            "emotion_relevance": len(emotion_keywords) / len(text.split()),
        }

    def _integrate_context_meaning(
        self,
        temporal_context: Dict,
        spatial_context: Dict,
        social_context: Dict,
        topical_context: Dict,
        emotional_context: Dict,
    ) -> str:
        """ë§¥ë½ ì˜ë¯¸ í†µí•©"""
        context_elements = []

        if temporal_context["temporal_relevance"] > 0.1:
            context_elements.append("ì‹œê°„ì  ë§¥ë½ì´ ì¤‘ìš”í•œ ëŒ€í™”")

        if spatial_context["spatial_relevance"] > 0.1:
            context_elements.append("ê³µê°„ì  ë§¥ë½ì´ ì¤‘ìš”í•œ ëŒ€í™”")

        if social_context["social_relevance"] > 0.1:
            context_elements.append("ì‚¬íšŒì  ê´€ê³„ê°€ ì¤‘ìš”í•œ ëŒ€í™”")

        if topical_context["topic_relevance"] > 0.1:
            context_elements.append("íŠ¹ì • ì£¼ì œì— ê´€í•œ ëŒ€í™”")

        if emotional_context["emotion_relevance"] > 0.1:
            context_elements.append("ê°ì •ì  í‘œí˜„ì´ ì¤‘ìš”í•œ ëŒ€í™”")

        if not context_elements:
            return "ì¼ë°˜ì ì¸ ëŒ€í™”"

        return " + ".join(context_elements)


class EmotionAnalyzer:
    """ê°ì • ë¶„ì„ê¸°"""

    def __init__(self):
        self.emotion_keywords = {
            "ê¸°ì¨": ["ê¸°ì˜ë‹¤", "í–‰ë³µí•˜ë‹¤", "ì¦ê²ë‹¤", "ì‹ ë‚˜ë‹¤", "ì¢‹ë‹¤", "ë§Œì¡±í•˜ë‹¤"],
            "ìŠ¬í””": ["ìŠ¬í”„ë‹¤", "ìš°ìš¸í•˜ë‹¤", "ì†ìƒí•˜ë‹¤", "ì•„í”„ë‹¤", "í˜ë“¤ë‹¤", "ì§€ì¹˜ë‹¤"],
            "í™”ë‚¨": ["í™”ë‚˜ë‹¤", "ì§œì¦ë‚˜ë‹¤", "ë¶„í•˜ë‹¤", "ì—´ë°›ë‹¤", "í™”ê°€ë‚˜ë‹¤", "ë‹µë‹µí•˜ë‹¤"],
            "ë†€ëŒ": ["ë†€ëë‹¤", "ê¹œì§", "ì–´ì´ì—†ë‹¤", "í—", "ì™€", "ëŒ€ë°•"],
            "ë‘ë ¤ì›€": ["ë¬´ì„­ë‹¤", "ê²ë‚˜ë‹¤", "ë¶ˆì•ˆí•˜ë‹¤", "ê±±ì •ë˜ë‹¤", "ë¬´ì„œì›Œí•˜ë‹¤"],
            "ì‚¬ë‘": ["ì‚¬ë‘í•˜ë‹¤", "ì¢‹ì•„í•˜ë‹¤", "ê·¸ë¦½ë‹¤", "ë³´ê³ ì‹¶ë‹¤", "ì•„ë¼ë‹¤"],
            "ë¯¸ì›€": ["ì‹«ë‹¤", "ë¯¸ì›Œí•˜ë‹¤", "ì§œì¦ë‚˜ë‹¤", "ë‹µë‹µí•˜ë‹¤", "í™”ë‚˜ë‹¤"],
            "í¬ë§": ["í¬ë§ì ì´ë‹¤", "ê¸°ëŒ€í•˜ë‹¤", "ê¿ˆê¾¸ë‹¤", "ë°”ë¼ë‹¤", "ì›í•˜ë‹¤"],
            "ì ˆë§": ["ì ˆë§ì ì´ë‹¤", "í¬ê¸°í•˜ë‹¤", "ì‹¤ë§í•˜ë‹¤", "í—ˆíƒˆí•˜ë‹¤"],
            "ê°ì‚¬": ["ê°ì‚¬í•˜ë‹¤", "ê³ ë§™ë‹¤", "ì€í˜œë¡­ë‹¤", "ì¶•ë³µë°›ë‹¤"],
        }

    async def analyze_emotion(
        self, text: str, context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ê°ì • ë¶„ì„"""
        emotion_scores = {}

        # ê° ê°ì •ë³„ ì ìˆ˜ ê³„ì‚°
        for emotion, keywords in self.emotion_keywords.items():
            score = 0
            for keyword in keywords:
                if keyword in text:
                    score += 1
            emotion_scores[emotion] = score

        # ì£¼ìš” ê°ì • ê²°ì •
        primary_emotion = (
            max(emotion_scores.items(), key=lambda x: x[1])[0]
            if emotion_scores
            else "ì¤‘ë¦½"
        )

        # ê°ì • ê°•ë„ ê³„ì‚°
        total_emotion_words = sum(emotion_scores.values())
        emotion_intensity = min(total_emotion_words / len(text.split()), 1.0)

        return {
            "primary_emotion": primary_emotion,
            "emotion_scores": emotion_scores,
            "emotion_intensity": emotion_intensity,
            "confidence": 0.8 if emotion_scores else 0.5,
        }


class IntentRecognizer:
    """ì˜ë„ ì¸ì‹ê¸°"""

    def __init__(self):
        self.intent_patterns = {
            "ì§ˆë¬¸": [r"\?$", r"ë¬´ì—‡|ì–´ë–¤|ì–´ë””|ì–¸ì œ|ëˆ„ê°€|ì™œ|ì–´ë–»ê²Œ"],
            "ìš”ì²­": [r"í•´ì£¼ì„¸ìš”", r"ë¶€íƒ", r"ë„ì™€", r"ì¢€", r"í•´ë‹¬ë¼"],
            "ëª…ë ¹": [r"í•´ë¼", r"í•˜ë¼", r"í•´ì•¼", r"í•„ìš”", r"í•´ì•¼ì§€"],
            "ê°ì •í‘œí˜„": [r"ê¸°ì˜ë‹¤", r"ìŠ¬í”„ë‹¤", r"í™”ë‚˜ë‹¤", r"ì¢‹ë‹¤", r"ì‹«ë‹¤"],
            "ì •ë³´ì œê³µ": [r"~ì…ë‹ˆë‹¤", r"~ì´ì—ìš”", r"~ì•¼", r"~ë‹¤"],
            "ë™ì˜": [r"ë§ë‹¤", r"ê·¸ë ‡ë‹¤", r"ì˜³ë‹¤", r"ì¢‹ë‹¤", r"ë„¤"],
            "ë°˜ëŒ€": [r"ì•„ë‹ˆë‹¤", r"í‹€ë ¸ë‹¤", r"ì‹«ë‹¤", r"ì•ˆëœë‹¤", r"ì•„ë‹ˆìš”"],
        }

    async def recognize_intent(
        self, text: str, context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ì˜ë„ ì¸ì‹"""
        intent_scores = {}

        # ê° ì˜ë„ë³„ ì ìˆ˜ ê³„ì‚°
        for intent, patterns in self.intent_patterns.items():
            score = 0
            for pattern in patterns:
                matches = re.findall(pattern, text)
                score += len(matches)
            intent_scores[intent] = score

        # ì£¼ìš” ì˜ë„ ê²°ì •
        primary_intent = (
            max(intent_scores.items(), key=lambda x: x[1])[0]
            if intent_scores
            else "ì¼ë°˜"
        )

        return {
            "primary_intent": primary_intent,
            "intent_scores": intent_scores,
            "confidence": 0.8 if intent_scores else 0.5,
        }


class SemanticAnalyzer:
    """ì˜ë¯¸ ë¶„ì„ê¸°"""

    async def analyze_semantics(
        self, text: str, context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ì˜ë¯¸ ë¶„ì„"""
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_keywords(text)

        # í•µì‹¬ ê°œë… ì¶”ì¶œ
        key_concepts = self._extract_key_concepts(text)

        # í•™ìŠµ í†µì°° ì¶”ì¶œ
        learning_insights = self._extract_learning_insights(text, context)

        return {
            "keywords": keywords,
            "key_concepts": key_concepts,
            "learning_insights": learning_insights,
            "confidence": 0.85,
        }

    def _extract_keywords(self, text: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ê¸°ë²• ì‚¬ìš©)
        words = text.split()
        word_freq = Counter(words)
        keywords = [word for word, freq in word_freq.most_common(5) if len(word) > 1]
        return keywords

    def _extract_key_concepts(self, text: str) -> List[str]:
        """í•µì‹¬ ê°œë… ì¶”ì¶œ"""
        # í•µì‹¬ ê°œë… íŒ¨í„´
        concept_patterns = [
            r"í•™ìŠµ|êµìœ¡|ì§€ì‹|ê¸°ìˆ |ëŠ¥ë ¥|ê²½í—˜",
            r"ê°€ì¡±|ì¹œêµ¬|ê´€ê³„|ì†Œí†µ|ì´í•´|ì‚¬ë‘",
            r"ëª©í‘œ|ê³„íš|ì‹¤í–‰|ê²°ê³¼|ì„±ê³µ|ì‹¤íŒ¨",
            r"ë¬¸ì œ|í•´ê²°|ë„ì „|ì–´ë ¤ì›€|ê·¹ë³µ|ì„±ì¥",
        ]

        concepts = []
        for pattern in concept_patterns:
            matches = re.findall(pattern, text)
            concepts.extend(matches)

        return list(set(concepts))

    def _extract_learning_insights(
        self, text: str, context: Dict[str, Any] = None
    ) -> List[str]:
        """í•™ìŠµ í†µì°° ì¶”ì¶œ"""
        insights = []

        # í•™ìŠµ ê´€ë ¨ íŒ¨í„´
        learning_patterns = [
            r"ë°°ìš°ë‹¤|í•™ìŠµí•˜ë‹¤|ìµíˆë‹¤|í›ˆë ¨í•˜ë‹¤",
            r"ê²½í—˜í•˜ë‹¤|ì²´í—˜í•˜ë‹¤|ì‹¤ìŠµí•˜ë‹¤",
            r"ì´í•´í•˜ë‹¤|ê¹¨ë‹«ë‹¤|ì•Œë‹¤|ì•Œê²Œë˜ë‹¤",
        ]

        for pattern in learning_patterns:
            if re.search(pattern, text):
                insights.append("í•™ìŠµ ê²½í—˜ ê´€ë ¨")
                break

        # ì„±ì¥ ê´€ë ¨ íŒ¨í„´
        growth_patterns = [
            r"ì„±ì¥í•˜ë‹¤|ë°œì „í•˜ë‹¤|í–¥ìƒë˜ë‹¤|ê°œì„ ë˜ë‹¤",
            r"ë³€í™”í•˜ë‹¤|ë‹¬ë¼ì§€ë‹¤|ë°”ë€Œë‹¤",
        ]

        for pattern in growth_patterns:
            if re.search(pattern, text):
                insights.append("ì„±ì¥ ë° ë°œì „ ê´€ë ¨")
                break

        return insights


class MultilingualProcessor:
    """ë‹¤êµ­ì–´ ì²˜ë¦¬ê¸°"""

    def __init__(self):
        self.supported_languages = ["ko", "en", "ja", "zh", "es", "fr", "de"]
        self.language_detectors = {
            "ko": self._detect_korean,
            "en": self._detect_english,
            "ja": self._detect_japanese,
            "zh": self._detect_chinese,
        }

    async def process_multilingual(
        self, text: str, context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ë‹¤êµ­ì–´ ì²˜ë¦¬"""
        # ì–¸ì–´ ê°ì§€
        detected_language = self._detect_language(text)

        # ì–¸ì–´ë³„ ì²˜ë¦¬
        language_specific_analysis = await self._process_language_specific(
            text, detected_language
        )

        return {
            "detected_language": detected_language,
            "language_specific_analysis": language_specific_analysis,
            "multilingual_support": True,
        }

    def _detect_language(self, text: str) -> str:
        """ì–¸ì–´ ê°ì§€"""
        for lang_code, detector in self.language_detectors.items():
            if detector(text):
                return lang_code
        return "ko"  # ê¸°ë³¸ê°’

    def _detect_korean(self, text: str) -> bool:
        """í•œêµ­ì–´ ê°ì§€"""
        korean_pattern = re.compile(r"[ê°€-í£]")
        return bool(korean_pattern.search(text))

    def _detect_english(self, text: str) -> bool:
        """ì˜ì–´ ê°ì§€"""
        english_pattern = re.compile(r"[a-zA-Z]")
        return bool(english_pattern.search(text))

    def _detect_japanese(self, text: str) -> bool:
        """ì¼ë³¸ì–´ ê°ì§€"""
        japanese_pattern = re.compile(r"[ã‚-ã‚“ã‚¢-ãƒ³]")
        return bool(japanese_pattern.search(text))

    def _detect_chinese(self, text: str) -> bool:
        """ì¤‘êµ­ì–´ ê°ì§€"""
        chinese_pattern = re.compile(r"[\u4e00-\u9fff]")
        return bool(chinese_pattern.search(text))

    async def _process_language_specific(
        self, text: str, language: str
    ) -> Dict[str, Any]:
        """ì–¸ì–´ë³„ íŠ¹í™” ì²˜ë¦¬"""
        if language == "ko":
            return self._process_korean(text)
        elif language == "en":
            return self._process_english(text)
        else:
            return {"language": language, "processed": True}

    def _process_korean(self, text: str) -> Dict[str, Any]:
        """í•œêµ­ì–´ íŠ¹í™” ì²˜ë¦¬"""
        return {
            "language": "ko",
            "processed": True,
            "features": ["í•œê¸€ ì²˜ë¦¬", "ì¡°ì‚¬ ë¶„ì„", "ì–´ë¯¸ ë¶„ì„"],
        }

    def _process_english(self, text: str) -> Dict[str, Any]:
        """ì˜ì–´ íŠ¹í™” ì²˜ë¦¬"""
        return {
            "language": "en",
            "processed": True,
            "features": ["ì˜ì–´ ì²˜ë¦¬", "ì‹œì œ ë¶„ì„", "í’ˆì‚¬ ë¶„ì„"],
        }


class ConversationalGenerator:
    """ëŒ€í™” ìƒì„±ê¸°"""

    async def generate_response(self, context: Dict[str, Any]) -> str:
        """ëŒ€í™” ì‘ë‹µ ìƒì„±"""
        # ë§¥ë½ ë¶„ì„
        intent = context.get("intent", "ì¼ë°˜")
        emotion = context.get("emotion", "ì¤‘ë¦½")
        topic = context.get("topic", "ì¼ë°˜")

        # ì˜ë„ë³„ ì‘ë‹µ ìƒì„±
        if intent == "ì§ˆë¬¸":
            return self._generate_question_response(context)
        elif intent == "ìš”ì²­":
            return self._generate_request_response(context)
        elif intent == "ê°ì •í‘œí˜„":
            return self._generate_emotional_response(context)
        else:
            return self._generate_general_response(context)

    def _generate_question_response(self, context: Dict[str, Any]) -> str:
        """ì§ˆë¬¸ ì‘ë‹µ ìƒì„±"""
        topic = context.get("topic", "ì¼ë°˜")
        return f"{topic}ì— ëŒ€í•œ ë‹µë³€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

    def _generate_request_response(self, context: Dict[str, Any]) -> str:
        """ìš”ì²­ ì‘ë‹µ ìƒì„±"""
        return "ë„¤, ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ì§€ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì„¸ìš”."

    def _generate_emotional_response(self, context: Dict[str, Any]) -> str:
        """ê°ì • í‘œí˜„ ì‘ë‹µ ìƒì„±"""
        emotion = context.get("emotion", "ì¤‘ë¦½")
        if emotion == "ê¸°ì¨":
            return "ì •ë§ ê¸°ì˜ì‹œê² ë„¤ìš”! í•¨ê»˜ ê¸°ë»í•´ ë“œë¦¬ê³  ì‹¶ìŠµë‹ˆë‹¤."
        elif emotion == "ìŠ¬í””":
            return "ë§ˆìŒì´ ì•„í”„ì‹œê² ì–´ìš”. ì œê°€ ì˜†ì—ì„œ í•¨ê»˜ ìˆì–´ë“œë¦´ê²Œìš”."
        elif emotion == "í™”ë‚¨":
            return "í™”ê°€ ë‚˜ì‹œëŠ” ê²ƒ ê°™ì•„ìš”. ì°¨ë¶„íˆ ì´ì•¼ê¸°í•´ ë³´ì„¸ìš”."
        else:
            return "ê·¸ëŸ° ê°ì •ì„ ëŠë¼ê³  ê³„ì‹œëŠ”êµ°ìš”. ë” ìì„¸íˆ ì´ì•¼ê¸°í•´ ì£¼ì„¸ìš”."

    def _generate_general_response(self, context: Dict[str, Any]) -> str:
        """ì¼ë°˜ ì‘ë‹µ ìƒì„±"""
        return (
            "ì´í•´í–ˆìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì‹œë©´ ë” ë‚˜ì€ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )


class EmotionalGenerator:
    """ê°ì •ì  í‘œí˜„ ìƒì„±ê¸°"""

    async def generate_emotional_expression(self, context: Dict[str, Any]) -> str:
        """ê°ì •ì  í‘œí˜„ ìƒì„±"""
        emotion = context.get("emotion", "ì¤‘ë¦½")
        intensity = context.get("emotion_intensity", 0.5)

        if emotion == "ê¸°ì¨":
            if intensity > 0.7:
                return "ì •ë§ ì •ë§ ê¸°ë»ìš”! ë§ˆìŒì´ ê°€ë²¼ì›Œì§€ëŠ” ê²ƒ ê°™ì•„ìš”!"
            else:
                return "ê¸°ì˜ë„¤ìš”. ì¢‹ì€ ì¼ì´ ìˆìœ¼ì‹ ê°€ìš”?"
        elif emotion == "ìŠ¬í””":
            if intensity > 0.7:
                return "ì •ë§ ë§ˆìŒì´ ì•„í”„ì‹œê² ì–´ìš”. ì œê°€ í•¨ê»˜ ìˆì–´ë“œë¦´ê²Œìš”."
            else:
                return "ìŠ¬í”ˆ ì¼ì´ ìˆìœ¼ì‹ ê°€ìš”? ì´ì•¼ê¸°í•´ ì£¼ì„¸ìš”."
        elif emotion == "í™”ë‚¨":
            if intensity > 0.7:
                return "ì •ë§ í™”ê°€ ë‚˜ì‹œëŠ” ê²ƒ ê°™ì•„ìš”. ì°¨ë¶„íˆ ìƒê°í•´ ë³´ì„¸ìš”."
            else:
                return "í™”ê°€ ë‚˜ì‹œëŠ”êµ°ìš”. ì–´ë–¤ ì¼ì´ ìˆìœ¼ì…¨ë‚˜ìš”?"
        else:
            return "ê°ì •ì„ í‘œí˜„í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤."

    async def analyze_emotional_expression(self, text: str) -> str:
        """ê°ì •ì  í‘œí˜„ ë¶„ì„"""
        emotion_keywords = {
            "ê¸°ì¨": ["ê¸°ë»ìš”", "ì¢‹ì•„ìš”", "í–‰ë³µí•´ìš”", "ì¦ê±°ì›Œìš”"],
            "ìŠ¬í””": ["ì•„í”„ì‹œê² ì–´ìš”", "ìŠ¬í”„ì‹œê² ì–´ìš”", "í˜ë“œì‹œê² ì–´ìš”"],
            "í™”ë‚¨": ["í™”ê°€ ë‚˜ì‹œëŠ”", "ì§œì¦ë‚˜ì‹œëŠ”", "ë‹µë‹µí•˜ì‹œëŠ”"],
            "ì‚¬ë‘": ["ì‚¬ë‘", "ì¢‹ì•„", "ê·¸ë¦½", "ë³´ê³ ì‹¶"],
        }

        for emotion, keywords in emotion_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    return emotion

        return "ì¤‘ë¦½"


class ContextualGenerator:
    """ë§¥ë½ ê¸°ë°˜ ìƒì„±ê¸°"""

    async def generate_contextual_text(self, context: Dict[str, Any]) -> str:
        """ë§¥ë½ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„±"""
        context_type = context.get("context_type", "ì¼ë°˜")

        if context_type == "í•™ìŠµ":
            return "í•™ìŠµì— ëŒ€í•œ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ„ê³  ê³„ì‹œëŠ”êµ°ìš”. ì–´ë–¤ ë¶€ë¶„ì—ì„œ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?"
        elif context_type == "ê°€ì¡±":
            return "ê°€ì¡±ì— ëŒ€í•œ ì´ì•¼ê¸°ë„¤ìš”. ê°€ì¡± ê´€ê³„ëŠ” ì •ë§ ì¤‘ìš”í•œ ë¶€ë¶„ì´ì—ìš”."
        elif context_type == "ì¼":
            return "ì¼ì— ëŒ€í•œ ì´ì•¼ê¸°ë¥¼ í•˜ê³  ê³„ì‹œëŠ”êµ°ìš”. ì–´ë–¤ ì–´ë ¤ì›€ì´ ìˆìœ¼ì‹ ê°€ìš”?"
        else:
            return "ë§¥ë½ì„ ê³ ë ¤í•œ ì‘ë‹µì„ ë“œë¦¬ê³  ì‹¶ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì„¸ìš”."

    async def evaluate_contextual_relevance(
        self, text: str, context: Dict[str, Any]
    ) -> float:
        """ë§¥ë½ ê´€ë ¨ì„± í‰ê°€"""
        # ê°„ë‹¨í•œ ê´€ë ¨ì„± í‰ê°€ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë°©ë²• ì‚¬ìš©)
        context_keywords = context.get("keywords", [])
        text_words = text.split()

        if not context_keywords:
            return 0.5

        relevant_words = sum(1 for word in text_words if word in context_keywords)
        relevance = relevant_words / len(text_words) if text_words else 0.0

        return min(relevance, 1.0)


class MultilingualGenerator:
    """ë‹¤êµ­ì–´ ìƒì„±ê¸°"""

    async def generate_multilingual_text(self, context: Dict[str, Any]) -> str:
        """ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ìƒì„±"""
        target_language = context.get("target_language", "ko")

        if target_language == "en":
            return "I understand. Please tell me more specifically so I can help you better."
        elif target_language == "ja":
            return "ç†è§£ã—ã¾ã—ãŸã€‚ã‚ˆã‚Šå…·ä½“çš„ã«ãŠèã‹ã›ãã ã•ã„ã€‚"
        elif target_language == "zh":
            return "æˆ‘æ˜ç™½äº†ã€‚è¯·æ›´å…·ä½“åœ°å‘Šè¯‰æˆ‘ï¼Œè¿™æ ·æˆ‘å°±èƒ½æ›´å¥½åœ°å¸®åŠ©æ‚¨ã€‚"
        else:
            return "ì´í•´í–ˆìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì‹œë©´ ë” ë‚˜ì€ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

    async def get_multilingual_support(
        self, text: str, context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë‹¤êµ­ì–´ ì§€ì› ì •ë³´"""
        return {
            "supported_languages": ["ko", "en", "ja", "zh"],
            "current_language": "ko",
            "translation_available": True,
        }


class CreativeGenerator:
    """ì°½ì˜ì  ìƒì„±ê¸°"""

    async def generate_creative_text(self, context: Dict[str, Any]) -> str:
        """ì°½ì˜ì  í…ìŠ¤íŠ¸ ìƒì„±"""
        topic = context.get("topic", "ì¼ë°˜")

        creative_responses = {
            "í•™ìŠµ": "í•™ìŠµì€ ë§ˆì¹˜ ì •ì›ì„ ê°€ê¾¸ëŠ” ê²ƒê³¼ ê°™ì•„ìš”. ê¾¸ì¤€í•œ ê´€ì‹¬ê³¼ ì‚¬ë‘ìœ¼ë¡œ ì•„ë¦„ë‹¤ìš´ ê½ƒì„ í”¼ìš¸ ìˆ˜ ìˆë‹µë‹ˆë‹¤.",
            "ê°€ì¡±": "ê°€ì¡±ì€ ë§ˆì¹˜ ë‚˜ë¬´ì˜ ë¿Œë¦¬ì™€ ê°™ì•„ìš”. ê¹Šê³  íŠ¼íŠ¼í•œ ë¿Œë¦¬ê°€ ìˆì–´ì•¼ í‘¸ë¥¸ ìê³¼ ì•„ë¦„ë‹¤ìš´ ê½ƒì´ í”¼ì–´ë‚  ìˆ˜ ìˆì–´ìš”.",
            "ì„±ì¥": "ì„±ì¥ì€ ë§ˆì¹˜ ë‚˜ë¹„ê°€ ë˜ëŠ” ê³¼ì •ê³¼ ê°™ì•„ìš”. ë•Œë¡œëŠ” ì–´ë ¤ì›€ì„ ê²ªì§€ë§Œ, ê·¸ ê³¼ì •ì„ í†µí•´ ë” ì•„ë¦„ë‹¤ì›Œì§ˆ ìˆ˜ ìˆì–´ìš”.",
            "ì¼ë°˜": "ëª¨ë“  ê²½í—˜ì€ ìš°ë¦¬ë¥¼ ì„±ì¥ì‹œí‚¤ëŠ” ì†Œì¤‘í•œ ì„ ë¬¼ì´ì—ìš”. ì–´ë–¤ ì–´ë ¤ì›€ì´ ìˆë”ë¼ë„ í•¨ê»˜ ê·¹ë³µí•´ ë‚˜ê°ˆ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”.",
        }

        return creative_responses.get(topic, creative_responses["ì¼ë°˜"])


class IntegratedLanguageUnderstandingGenerationSystem:
    """í†µí•© ì–¸ì–´ ì´í•´ ë° ìƒì„± ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.system_name = "í†µí•© ì–¸ì–´ ì´í•´ ë° ìƒì„± ì‹œìŠ¤í…œ"
        self.version = "1.0.0"
        self.deep_understanding_engine = DeepLanguageUnderstandingEngine()
        self.advanced_generation_engine = AdvancedLanguageGenerationEngine()
        self.nlp_system = NaturalLanguageProcessingSystem()
        self.social_intelligence_system = IntegratedSocialIntelligenceSystem()

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.performance_metrics = defaultdict(float)
        self.system_status = "active"

        logger.info(f"ğŸš€ {self.system_name} v{self.version} ì´ˆê¸°í™” ì™„ë£Œ")

    async def process_language(
        self,
        text: str,
        context: Dict[str, Any] = None,
        generation_type: LanguageGenerationType = LanguageGenerationType.CONVERSATIONAL_RESPONSE,
    ) -> IntegratedLanguageResult:
        """í†µí•© ì–¸ì–´ ì²˜ë¦¬"""
        start_time = time.time()

        try:
            logger.info("=== í†µí•© ì–¸ì–´ ì´í•´ ë° ìƒì„± ì‹œìŠ¤í…œ ì‹œì‘ ===")

            # 1. ë¹ˆ ì…ë ¥ ì²˜ë¦¬ ì‹œ division by zero ì˜ˆì™¸ ë°©ì§€ ë¡œì§ ì¶”ê°€
            if not text or not text.strip():
                logger.warning("ë¹ˆ í…ìŠ¤íŠ¸ ì…ë ¥ ê°ì§€, ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬")
                text = "ì¼ë°˜ì ì¸ ëŒ€í™”"

            # 2. ì‹¬ì¸µ ì–¸ì–´ ì´í•´
            understanding_result = (
                await self.deep_understanding_engine.understand_language(text, context)
            )

            # 3. ê³ ê¸‰ ì–¸ì–´ ìƒì„± (ì˜ë¯¸ ë¶„ì„ ê²°ê³¼ê°€ ì–¸ì–´ ìƒì„± ê°€ì¤‘ì¹˜ì— ì œëŒ€ë¡œ ë°˜ì˜ë˜ë„ë¡ ì—°ê²° ë³´ê°•)
            generation_context = {
                "intent": understanding_result.intent,
                "emotion": understanding_result.emotional_tone,
                "topic": (
                    understanding_result.key_concepts[0]
                    if understanding_result.key_concepts
                    else "ì¼ë°˜"
                ),
                "context_type": understanding_result.context_meaning,
                "keywords": understanding_result.key_concepts,
                "learning_insights": understanding_result.learning_insights,  # ì˜ë¯¸ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
                "confidence_score": understanding_result.confidence_score,  # ì´í•´ ì‹ ë¢°ë„ ì¶”ê°€
                "semantic_analysis": {
                    "key_concepts": understanding_result.key_concepts,
                    "learning_insights": understanding_result.learning_insights,
                },
            }

            generation_result = await self.advanced_generation_engine.generate_language(
                generation_context, generation_type
            )

            # 4. í†µí•© ë¶„ì„ (integration_score ê³„ì‚°ì‹ ì¬ì¡°ì • ë° 0.0~1.0 ì •ê·œí™” ì ìš©)
            integration_score = self._calculate_integration_score(
                understanding_result, generation_result
            )

            # 5. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self._update_performance_metrics(processing_time, integration_score)

            # 6. ê²°ê³¼ ìƒì„±
            result = IntegratedLanguageResult(
                result_id=f"result_{int(time.time())}",
                understanding_result=understanding_result,
                generation_result=generation_result,
                integration_score=integration_score,
                system_performance={
                    "processing_time": processing_time,
                    "system_status": self.system_status,
                    "performance_metrics": dict(self.performance_metrics),
                },
            )

            logger.info(
                f"âœ… í†µí•© ì–¸ì–´ ì²˜ë¦¬ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ, í†µí•©ì ìˆ˜: {integration_score:.2f})"
            )

            return result

        except Exception as e:
            logger.error(f"í†µí•© ì–¸ì–´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise

    def _calculate_integration_score(
        self,
        understanding_result: LanguageUnderstandingResult,
        generation_result: LanguageGenerationResult,
    ) -> float:
        """í†µí•© ì ìˆ˜ ê³„ì‚° (ì¬ì¡°ì • ë° 0.0~1.0 ì •ê·œí™” ì ìš©)"""
        try:
            # ì´í•´ ì ìˆ˜ (0.0~1.0 ì •ê·œí™”)
            understanding_score = max(
                0.0, min(1.0, understanding_result.confidence_score)
            )

            # ìƒì„± ì ìˆ˜ (0.0~1.0 ì •ê·œí™”)
            generation_score = max(0.0, min(1.0, generation_result.confidence_score))

            # ë§¥ë½ ê´€ë ¨ì„± ì ìˆ˜ (0.0~1.0 ì •ê·œí™”)
            contextual_score = max(
                0.0, min(1.0, generation_result.contextual_relevance)
            )

            # ì˜ë¯¸ ë¶„ì„ ê²°ê³¼ ë°˜ì˜ (ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ ì¶”ê°€)
            semantic_score = 0.0
            if understanding_result.key_concepts:
                semantic_score = min(1.0, len(understanding_result.key_concepts) * 0.1)
            if understanding_result.learning_insights:
                semantic_score = min(
                    1.0,
                    semantic_score + len(understanding_result.learning_insights) * 0.1,
                )

            # í†µí•© ì ìˆ˜ (ê°€ì¤‘ í‰ê· ) - ì¬ì¡°ì •ëœ ê°€ì¤‘ì¹˜
            integration_score = (
                understanding_score * 0.35  # ì´í•´ ì ìˆ˜ (35%)
                + generation_score * 0.35  # ìƒì„± ì ìˆ˜ (35%)
                + contextual_score * 0.20  # ë§¥ë½ ê´€ë ¨ì„± (20%)
                + semantic_score * 0.10  # ì˜ë¯¸ ë¶„ì„ (10%)
            )

            # 0.0~1.0 ì •ê·œí™” ì ìš©
            normalized_score = max(0.0, min(1.0, integration_score))

            return normalized_score

        except Exception as e:
            logger.error(f"í†µí•© ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5  # ê¸°ë³¸ê°’ ë°˜í™˜

    def _update_performance_metrics(
        self, processing_time: float, integration_score: float
    ):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ (division by zero ì˜ˆì™¸ ë°©ì§€)"""
        try:
            self.performance_metrics["total_processing_time"] += processing_time

            # division by zero ì˜ˆì™¸ ë°©ì§€
            current_count = self.performance_metrics.get("request_count", 0)
            new_count = current_count + 1

            if new_count > 0:
                self.performance_metrics["average_processing_time"] = (
                    self.performance_metrics["total_processing_time"] / new_count
                )

                # í‰ê·  í†µí•© ì ìˆ˜ ê³„ì‚° (division by zero ì˜ˆì™¸ ë°©ì§€)
                current_avg = self.performance_metrics.get(
                    "average_integration_score", 0.0
                )
                self.performance_metrics["average_integration_score"] = (
                    current_avg * current_count + integration_score
                ) / new_count

            self.performance_metrics["request_count"] = new_count

        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ì„¤ì •
            self.performance_metrics["request_count"] = (
                self.performance_metrics.get("request_count", 0) + 1
            )
            self.performance_metrics["average_processing_time"] = processing_time
            self.performance_metrics["average_integration_score"] = integration_score

    async def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
        return {
            "system_name": self.system_name,
            "version": self.version,
            "status": self.system_status,
            "performance_metrics": dict(self.performance_metrics),
            "timestamp": datetime.now().isoformat(),
        }

    async def get_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¡°íšŒ"""
        return {
            "system_name": self.system_name,
            "version": self.version,
            "performance_metrics": dict(self.performance_metrics),
            "system_health": "healthy" if self.system_status == "active" else "warning",
            "timestamp": datetime.now().isoformat(),
        }


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_integrated_language_system():
    """í†µí•© ì–¸ì–´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    logger.info("=== í†µí•© ì–¸ì–´ ì´í•´ ë° ìƒì„± ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")

    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    system = IntegratedLanguageUnderstandingGenerationSystem()

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
    test_cases = [
        {
            "text": "ì˜¤ëŠ˜ ì •ë§ ê¸°ë»ìš”! ìƒˆë¡œìš´ ê²ƒì„ ë°°ì› ì–´ìš”.",
            "context": {"topic": "í•™ìŠµ", "emotion": "ê¸°ì¨"},
            "generation_type": LanguageGenerationType.EMOTIONAL_EXPRESSION,
        },
        {
            "text": "ê°€ì¡±ê³¼ í•¨ê»˜í•˜ëŠ” ì‹œê°„ì´ ê°€ì¥ ì†Œì¤‘í•´ìš”.",
            "context": {"topic": "ê°€ì¡±", "emotion": "ì‚¬ë‘"},
            "generation_type": LanguageGenerationType.CONVERSATIONAL_RESPONSE,
        },
        {
            "text": "ì–´ë ¤ìš´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
            "context": {"topic": "ë¬¸ì œí•´ê²°", "intent": "ì§ˆë¬¸"},
            "generation_type": LanguageGenerationType.CONTEXTUAL_GENERATION,
        },
    ]

    results = []

    for i, test_case in enumerate(test_cases, 1):
        logger.info(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}: {test_case['text']}")

        try:
            result = await system.process_language(
                text=test_case["text"],
                context=test_case["context"],
                generation_type=test_case["generation_type"],
            )

            results.append(
                {
                    "test_case": i,
                    "input_text": test_case["text"],
                    "understanding_score": result.understanding_result.confidence_score,
                    "generation_score": result.generation_result.confidence_score,
                    "integration_score": result.integration_score,
                    "generated_text": result.generation_result.generated_text,
                }
            )

            logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i} ì™„ë£Œ")
            logger.info(
                f"   ì´í•´ ì ìˆ˜: {result.understanding_result.confidence_score:.2f}"
            )
            logger.info(
                f"   ìƒì„± ì ìˆ˜: {result.generation_result.confidence_score:.2f}"
            )
            logger.info(f"   í†µí•© ì ìˆ˜: {result.integration_score:.2f}")
            logger.info(f"   ìƒì„±ëœ í…ìŠ¤íŠ¸: {result.generation_result.generated_text}")

        except Exception as e:
            logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i} ì‹¤íŒ¨: {e}")
            results.append({"test_case": i, "error": str(e)})

    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    successful_tests = [r for r in results if "error" not in r]
    if successful_tests:
        avg_understanding_score = np.mean(
            [r["understanding_score"] for r in successful_tests]
        )
        avg_generation_score = np.mean(
            [r["generation_score"] for r in successful_tests]
        )
        avg_integration_score = np.mean(
            [r["integration_score"] for r in successful_tests]
        )

        logger.info(f"\n=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ===")
        logger.info(f"ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {len(successful_tests)}/{len(test_cases)}")
        logger.info(f"í‰ê·  ì´í•´ ì ìˆ˜: {avg_understanding_score:.2f}")
        logger.info(f"í‰ê·  ìƒì„± ì ìˆ˜: {avg_generation_score:.2f}")
        logger.info(f"í‰ê·  í†µí•© ì ìˆ˜: {avg_integration_score:.2f}")

    # ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ
    system_status = await system.get_system_status()
    logger.info(f"\nì‹œìŠ¤í…œ ìƒíƒœ: {system_status['status']}")

    return results


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_integrated_language_system())
