#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DuRi Phase Z v2.0: í†µí•© í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ Phase Z v2.0ì˜ ì „ì²´ ì‹œìŠ¤í…œì„ í†µí•© í…ŒìŠ¤íŠ¸í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
DuRiThoughtFlow, ë‚´ë¶€ ëª¨ìˆœ íƒì§€, í‘œí˜„ ê³„ì¸µì„ ëª¨ë‘ í†µí•©í•˜ì—¬ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ì „ì²´ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
- ì„±ëŠ¥ ìµœì í™”
- ì•ˆì •ì„± ê²€ì¦
- ê²°ê³¼ ë¶„ì„ ë° ë¦¬í¬íŠ¸
"""

import asyncio
import logging
import time
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional

import numpy as np

# Phase Z v2.0 ëª¨ë“ˆë“¤ import
try:
    from duri_expression_layer import DuRiExpressionLayer, ExpressionResult  # noqa: F401
    from duri_thought_flow import DuRiThoughtFlow, ThoughtFlowResult  # noqa: F401
    from internal_conflict_detector import ConflictAnalysisResult, InternalConflictDetector  # noqa: F401
except ImportError as e:
    logging.warning(f"Phase Z v2.0 ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class TestType(Enum):
    """í…ŒìŠ¤íŠ¸ ìœ í˜• ì—´ê±°í˜•"""

    UNIT = "unit"
    INTEGRATION = "integration"
    PERFORMANCE = "performance"
    STABILITY = "stability"
    END_TO_END = "end_to_end"


class TestResult(Enum):
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì—´ê±°í˜•"""

    PASS = "pass"
    FAIL = "fail"
    WARNING = "warning"
    ERROR = "error"


@dataclass
class TestCase:
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë°ì´í„° í´ë˜ìŠ¤"""

    test_id: str
    test_type: TestType
    description: str
    input_data: Dict[str, Any]
    expected_result: Dict[str, Any]
    timeout: float = 30.0


@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""

    test_id: str
    test_type: TestType
    result: TestResult
    execution_time: float
    actual_result: Dict[str, Any]
    error_message: Optional[str] = None
    performance_metrics: Dict[str, Any] = field(default_factory=dict)


@dataclass
class IntegrationTestReport:
    """í†µí•© í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ë°ì´í„° í´ë˜ìŠ¤"""

    test_results: List[TestResult]
    total_tests: int
    passed_tests: int
    failed_tests: int
    warning_tests: int
    error_tests: int
    total_execution_time: float
    average_execution_time: float
    success_rate: float
    performance_summary: Dict[str, Any]
    recommendations: List[str]


class PhaseZIntegrationTest:
    """Phase Z v2.0 í†µí•© í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.thought_flow = None
        self.conflict_detector = None
        self.expression_layer = None
        self.test_cases = self._initialize_test_cases()
        self.test_results: List[TestResult] = []

    def _initialize_test_cases(self) -> List[TestCase]:
        """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì´ˆê¸°í™”"""
        test_cases = []

        # 1. ê¸°ë³¸ ì‚¬ê³  íë¦„ í…ŒìŠ¤íŠ¸
        test_cases.append(
            TestCase(
                test_id="TF_001",
                test_type=TestType.UNIT,
                description="ê¸°ë³¸ ì‚¬ê³  íë¦„ í…ŒìŠ¤íŠ¸",
                input_data={
                    "question": "DuRiëŠ” ì§„ì§œë¡œ ìƒê°í•  ìˆ˜ ìˆëŠ”ê°€?",
                    "context": "AIì˜ ì‚¬ê³  ëŠ¥ë ¥ì— ëŒ€í•œ ì² í•™ì  ì§ˆë¬¸",
                },
                expected_result={
                    "success": True,
                    "reflection_score": 0.7,
                    "thought_process_length": 5,
                },
            )
        )

        # 2. ë‚´ë¶€ ëª¨ìˆœ íƒì§€ í…ŒìŠ¤íŠ¸
        test_cases.append(
            TestCase(
                test_id="CD_001",
                test_type=TestType.UNIT,
                description="ë‚´ë¶€ ëª¨ìˆœ íƒì§€ í…ŒìŠ¤íŠ¸",
                input_data={
                    "goals": ["íš¨ìœ¨ì„± ê·¹ëŒ€í™”", "ìœ¤ë¦¬ì  ì›ì¹™ ì¤€ìˆ˜"],
                    "principles": ["ììœ¨ì„±", "ê³µì •ì„±"],
                    "arguments": [
                        "ëª¨ë“  ê²°ì •ì€ íš¨ìœ¨ì ì´ì–´ì•¼ í•œë‹¤",
                        "ë•Œë¡œëŠ” íš¨ìœ¨ì„±ì„ í¬ê¸°í•´ì•¼ í•  ìˆ˜ë„ ìˆë‹¤",
                    ],
                },
                expected_result={
                    "success": True,
                    "total_conflicts": 1,
                    "severity_distribution": {"medium": 1},
                },
            )
        )

        # 3. í‘œí˜„ ê³„ì¸µ í…ŒìŠ¤íŠ¸
        test_cases.append(
            TestCase(
                test_id="EL_001",
                test_type=TestType.UNIT,
                description="í‘œí˜„ ê³„ì¸µ í…ŒìŠ¤íŠ¸",
                input_data={
                    "internal_conflicts": [
                        {"type": "logical", "description": "ë…¼ë¦¬ì  ëª¨ìˆœ ë°œê²¬"},
                        {"type": "ethical", "description": "ìœ¤ë¦¬ì  ë”œë ˆë§ˆ"},
                    ],
                    "reflection_score": 0.8,
                    "thought_process": [
                        {"role": "observer", "content": "ìê¸° ê´€ì°°"},
                        {"role": "counter_arguer", "content": "ë‚´ì  ë°˜ë°•"},
                        {"role": "reframer", "content": "ë¬¸ì œ ì¬ì •ì˜"},
                    ],
                    "goal_alignment": 0.7,
                    "context": {"environment": "professional"},
                    "patterns": ["logical_analysis", "ethical_consideration"],
                    "abstraction_level": 0.6,
                    "creativity_score": 0.5,
                },
                expected_result={
                    "success": True,
                    "emotion_expression": True,
                    "art_expression": True,
                    "social_expression": True,
                },
            )
        )

        # 4. í†µí•© í…ŒìŠ¤íŠ¸
        test_cases.append(
            TestCase(
                test_id="INT_001",
                test_type=TestType.INTEGRATION,
                description="ì „ì²´ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸",
                input_data={
                    "question": "ë³µì¡í•œ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ìƒí™©ì—ì„œ DuRiëŠ” ì–´ë–»ê²Œ ì‚¬ê³ í•˜ëŠ”ê°€?",
                    "context": {
                        "environment": "professional",
                        "stakeholders": ["user", "system", "society"],
                        "constraints": ["ethical", "legal", "practical"],
                    },
                    "goals": [
                        "ìœ¤ë¦¬ì  ì›ì¹™ ì¤€ìˆ˜",
                        "ì‹¤ìš©ì  í•´ê²°ì±… ì œì‹œ",
                        "ì‚¬ìš©ì ë§Œì¡±ë„",
                    ],
                    "principles": ["ììœ¨ì„±", "ê³µì •ì„±", "íš¨ìœ¨ì„±"],
                },
                expected_result={
                    "success": True,
                    "thought_flow_success": True,
                    "conflict_detection_success": True,
                    "expression_success": True,
                    "overall_confidence": 0.7,
                },
            )
        )

        # 5. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        test_cases.append(
            TestCase(
                test_id="PERF_001",
                test_type=TestType.PERFORMANCE,
                description="ì„±ëŠ¥ í…ŒìŠ¤íŠ¸",
                input_data={
                    "question": "ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ DuRiì˜ ì‚¬ê³  ì„±ëŠ¥ì€ ì–´ë– í•œê°€?",
                    "context": {"data_size": "large", "complexity": "high"},
                    "iterations": 10,
                },
                expected_result={
                    "success": True,
                    "average_processing_time": 5.0,
                    "memory_usage": "stable",
                    "throughput": "acceptable",
                },
            )
        )

        return test_cases

    async def run_all_tests(self) -> IntegrationTestReport:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ Phase Z v2.0 í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        start_time = time.time()  # noqa: F841

        try:
            # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            await self._initialize_systems()

            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            for test_case in self.test_cases:
                test_result = await self._run_test_case(test_case)
                self.test_results.append(test_result)

            # ë¦¬í¬íŠ¸ ìƒì„±
            report = await self._generate_test_report()

            logger.info("âœ… Phase Z v2.0 í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            return report

        except Exception as e:
            logger.error(f"í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return await self._generate_error_report(str(e))

    async def _initialize_systems(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        logger.info("ğŸ”§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")

        try:
            # DuRiThoughtFlow ì´ˆê¸°í™”
            test_input = {"question": "test", "context": "test"}
            self.thought_flow = DuRiThoughtFlow(test_input, {"goal": "test"})

            # InternalConflictDetector ì´ˆê¸°í™”
            self.conflict_detector = InternalConflictDetector()

            # DuRiExpressionLayer ì´ˆê¸°í™”
            self.expression_layer = DuRiExpressionLayer()

            logger.info("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    async def _run_test_case(self, test_case: TestCase) -> TestResult:
        """ê°œë³„ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰"""
        logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰: {test_case.test_id} - {test_case.description}")
        start_time = time.time()

        try:
            if test_case.test_type == TestType.UNIT:
                actual_result = await self._run_unit_test(test_case)
            elif test_case.test_type == TestType.INTEGRATION:
                actual_result = await self._run_integration_test(test_case)
            elif test_case.test_type == TestType.PERFORMANCE:
                actual_result = await self._run_performance_test(test_case)
            else:
                actual_result = await self._run_general_test(test_case)

            execution_time = time.time() - start_time

            # ê²°ê³¼ ê²€ì¦
            test_result = await self._validate_test_result(test_case, actual_result, execution_time)

            logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {test_case.test_id} - {test_result.result.value}")
            return test_result

        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {test_case.test_id} - {e}")

            return TestResult(
                test_id=test_case.test_id,
                test_type=test_case.test_type,
                result=TestResult.ERROR,
                execution_time=execution_time,
                actual_result={},
                error_message=str(e),
            )

    async def _run_unit_test(self, test_case: TestCase) -> Dict[str, Any]:
        """ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        if test_case.test_id.startswith("TF_"):
            # DuRiThoughtFlow í…ŒìŠ¤íŠ¸
            thought_flow = DuRiThoughtFlow(test_case.input_data, test_case.input_data.get("context", {}))
            result = await thought_flow.process()
            return {
                "success": result.success,
                "reflection_score": result.reflection_result.score,
                "thought_process_length": len(result.thought_process),
            }

        elif test_case.test_id.startswith("CD_"):
            # InternalConflictDetector í…ŒìŠ¤íŠ¸
            detector = InternalConflictDetector()
            result = await detector.detect_conflicts(test_case.input_data)
            return {
                "success": result.success,
                "total_conflicts": result.total_conflicts,
                "severity_distribution": result.severity_distribution,
            }

        elif test_case.test_id.startswith("EL_"):
            # DuRiExpressionLayer í…ŒìŠ¤íŠ¸
            expression_layer = DuRiExpressionLayer()
            emotion_result = await expression_layer.express_emotion(test_case.input_data)
            art_result = await expression_layer.express_art(test_case.input_data)
            social_result = await expression_layer.express_sociality(test_case.input_data)

            return {
                "success": True,
                "emotion_expression": emotion_result.success,
                "art_expression": art_result.success,
                "social_expression": social_result.success,
            }

        else:
            return {"success": False, "error": "Unknown test type"}

    async def _run_integration_test(self, test_case: TestCase) -> Dict[str, Any]:
        """í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        # 1. DuRiThoughtFlow ì‹¤í–‰
        thought_flow = DuRiThoughtFlow(test_case.input_data, test_case.input_data.get("context", {}))
        thought_result = await thought_flow.process()

        # 2. ë‚´ë¶€ ëª¨ìˆœ íƒì§€
        detector = InternalConflictDetector()
        conflict_result = await detector.detect_conflicts(test_case.input_data)

        # 3. í‘œí˜„ ê³„ì¸µ
        expression_layer = DuRiExpressionLayer()
        expression_result = await expression_layer.express_integrated(thought_result.final_decision)

        return {
            "success": True,
            "thought_flow_success": thought_result.success,
            "conflict_detection_success": conflict_result.success,
            "expression_success": expression_result.success,
            "overall_confidence": thought_result.reflection_result.score,
        }

    async def _run_performance_test(self, test_case: TestCase) -> Dict[str, Any]:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        iterations = test_case.input_data.get("iterations", 10)
        processing_times = []

        for i in range(iterations):
            start_time = time.time()

            # í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            thought_flow = DuRiThoughtFlow(test_case.input_data, test_case.input_data.get("context", {}))
            thought_result = await thought_flow.process()

            detector = InternalConflictDetector()
            conflict_result = await detector.detect_conflicts(test_case.input_data)  # noqa: F841

            expression_layer = DuRiExpressionLayer()
            expression_result = await expression_layer.express_integrated(thought_result.final_decision)  # noqa: F841

            processing_time = time.time() - start_time
            processing_times.append(processing_time)

        average_time = np.mean(processing_times)
        max_time = np.max(processing_times)
        min_time = np.min(processing_times)

        return {
            "success": True,
            "average_processing_time": average_time,
            "max_processing_time": max_time,
            "min_processing_time": min_time,
            "memory_usage": "stable",
            "throughput": "acceptable" if average_time < 5.0 else "slow",
        }

    async def _run_general_test(self, test_case: TestCase) -> Dict[str, Any]:
        """ì¼ë°˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        return {"success": True, "message": "General test completed"}

    async def _validate_test_result(
        self, test_case: TestCase, actual_result: Dict[str, Any], execution_time: float
    ) -> TestResult:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê²€ì¦"""
        # ê¸°ë³¸ ì„±ê³µ ì—¬ë¶€ í™•ì¸
        if not actual_result.get("success", False):
            return TestResult(
                test_id=test_case.test_id,
                test_type=test_case.test_type,
                result=TestResult.FAIL,
                execution_time=execution_time,
                actual_result=actual_result,
                error_message="Test failed - success flag is False",
            )

        # ì˜ˆìƒ ê²°ê³¼ì™€ ì‹¤ì œ ê²°ê³¼ ë¹„êµ
        validation_score = await self._calculate_validation_score(test_case.expected_result, actual_result)

        if validation_score >= 0.8:
            result = TestResult.PASS
        elif validation_score >= 0.6:
            result = TestResult.WARNING
        else:
            result = TestResult.FAIL

        return TestResult(
            test_id=test_case.test_id,
            test_type=test_case.test_type,
            result=result,
            execution_time=execution_time,
            actual_result=actual_result,
            performance_metrics={"validation_score": validation_score},
        )

    async def _calculate_validation_score(self, expected: Dict[str, Any], actual: Dict[str, Any]) -> float:
        """ê²€ì¦ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        total_checks = 0

        for key, expected_value in expected.items():
            if key in actual:
                actual_value = actual[key]

                if isinstance(expected_value, (int, float)) and isinstance(actual_value, (int, float)):
                    # ìˆ˜ì¹˜ ë¹„êµ
                    if abs(expected_value - actual_value) < 0.1:
                        score += 1.0
                    elif abs(expected_value - actual_value) < 0.3:
                        score += 0.5
                elif isinstance(expected_value, bool) and isinstance(actual_value, bool):
                    # ë¶ˆë¦° ë¹„êµ
                    if expected_value == actual_value:
                        score += 1.0
                elif isinstance(expected_value, str) and isinstance(actual_value, str):
                    # ë¬¸ìì—´ ë¹„êµ
                    if expected_value.lower() in actual_value.lower():
                        score += 1.0
                elif isinstance(expected_value, dict) and isinstance(actual_value, dict):
                    # ë”•ì…”ë„ˆë¦¬ ë¹„êµ
                    sub_score = await self._calculate_validation_score(expected_value, actual_value)
                    score += sub_score
                else:
                    # ê¸°íƒ€ íƒ€ì… ë¹„êµ
                    if expected_value == actual_value:
                        score += 1.0

                total_checks += 1

        return score / total_checks if total_checks > 0 else 0.0

    async def _generate_test_report(self) -> IntegrationTestReport:
        """í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results if r.result == TestResult.PASS])
        failed_tests = len([r for r in self.test_results if r.result == TestResult.FAIL])
        warning_tests = len([r for r in self.test_results if r.result == TestResult.WARNING])
        error_tests = len([r for r in self.test_results if r.result == TestResult.ERROR])

        total_execution_time = sum(r.execution_time for r in self.test_results)
        average_execution_time = total_execution_time / total_tests if total_tests > 0 else 0.0
        success_rate = passed_tests / total_tests if total_tests > 0 else 0.0

        # ì„±ëŠ¥ ìš”ì•½
        performance_summary = {
            "average_execution_time": average_execution_time,
            "max_execution_time": (max(r.execution_time for r in self.test_results) if self.test_results else 0.0),
            "min_execution_time": (min(r.execution_time for r in self.test_results) if self.test_results else 0.0),
            "total_execution_time": total_execution_time,
        }

        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = await self._generate_recommendations()

        return IntegrationTestReport(
            test_results=self.test_results,
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            warning_tests=warning_tests,
            error_tests=error_tests,
            total_execution_time=total_execution_time,
            average_execution_time=average_execution_time,
            success_rate=success_rate,
            performance_summary=performance_summary,
            recommendations=recommendations,
        )

    async def _generate_error_report(self, error_message: str) -> IntegrationTestReport:
        """ì—ëŸ¬ ë¦¬í¬íŠ¸ ìƒì„±"""
        return IntegrationTestReport(
            test_results=[],
            total_tests=0,
            passed_tests=0,
            failed_tests=0,
            warning_tests=0,
            error_tests=1,
            total_execution_time=0.0,
            average_execution_time=0.0,
            success_rate=0.0,
            performance_summary={},
            recommendations=[f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {error_message}"],
        )

    async def _generate_recommendations(self) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []

        # ì„±ê³µë¥  ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        success_rate = (
            len([r for r in self.test_results if r.result == TestResult.PASS]) / len(self.test_results)
            if self.test_results
            else 0.0
        )

        if success_rate < 0.8:
            recommendations.append("í…ŒìŠ¤íŠ¸ ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ê°œì„ í•´ì•¼ í•©ë‹ˆë‹¤.")

        # ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        avg_time = (
            sum(r.execution_time for r in self.test_results) / len(self.test_results) if self.test_results else 0.0
        )

        if avg_time > 5.0:
            recommendations.append("í‰ê·  ì‹¤í–‰ ì‹œê°„ì´ ê¸¸ìŠµë‹ˆë‹¤. ì„±ëŠ¥ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        # ì—ëŸ¬ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        error_count = len([r for r in self.test_results if r.result == TestResult.ERROR])

        if error_count > 0:
            recommendations.append(
                f"{error_count}ê°œì˜ í…ŒìŠ¤íŠ¸ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ì ê²€í•´ì•¼ í•©ë‹ˆë‹¤."
            )

        return recommendations


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # Phase Z v2.0 í†µí•© í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    integration_test = PhaseZIntegrationTest()

    # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    report = await integration_test.run_all_tests()

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ§  Phase Z v2.0 í†µí•© í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸")
    print("=" * 80)

    print("\nğŸ“Š ê¸°ë³¸ ì •ë³´:")
    print(f"  - ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {report.total_tests}")
    print(f"  - ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {report.passed_tests}")
    print(f"  - ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸: {report.failed_tests}")
    print(f"  - ê²½ê³  í…ŒìŠ¤íŠ¸: {report.warning_tests}")
    print(f"  - ì—ëŸ¬ í…ŒìŠ¤íŠ¸: {report.error_tests}")
    print(f"  - ì„±ê³µë¥ : {report.success_rate:.2%}")

    print("\nâ±ï¸ ì„±ëŠ¥ ì •ë³´:")
    print(f"  - ì´ ì‹¤í–‰ ì‹œê°„: {report.total_execution_time:.2f}ì´ˆ")
    print(f"  - í‰ê·  ì‹¤í–‰ ì‹œê°„: {report.average_execution_time:.2f}ì´ˆ")
    print(f"  - ìµœëŒ€ ì‹¤í–‰ ì‹œê°„: {report.performance_summary.get('max_execution_time', 0):.2f}ì´ˆ")
    print(f"  - ìµœì†Œ ì‹¤í–‰ ì‹œê°„: {report.performance_summary.get('min_execution_time', 0):.2f}ì´ˆ")

    print("\nğŸ§ª ìƒì„¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    for result in report.test_results:
        status_emoji = {
            TestResult.PASS: "âœ…",
            TestResult.FAIL: "âŒ",
            TestResult.WARNING: "âš ï¸",
            TestResult.ERROR: "ğŸš¨",
        }.get(result.result, "â“")

        print(f"  {status_emoji} {result.test_id}: {result.result.value} ({result.execution_time:.2f}ì´ˆ)")
        if result.error_message:
            print(f"    - ì—ëŸ¬: {result.error_message}")

    if report.recommendations:
        print("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        for rec in report.recommendations:
            print(f"  - {rec}")

    return report


if __name__ == "__main__":
    asyncio.run(main())
