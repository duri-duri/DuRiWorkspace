#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DuRi Phase 1-3 Week 3 Day 9 - ìì—°ì–´ ì²˜ë¦¬ ì‹œìŠ¤í…œ

ìì—°ì–´ ì²˜ë¦¬ ë° ì´í•´ ëŠ¥ë ¥ ê°•í™”
- ê³ ê¸‰ í…ìŠ¤íŠ¸ ë¶„ì„
- ì˜ë¯¸ ì¶”ì¶œ ë° ì´í•´
- ë¬¸ë§¥ ì¸ì‹ ë° ì²˜ë¦¬
- ë‹¤êµ­ì–´ ì§€ì›
"""

import asyncio
from collections import Counter, defaultdict
from dataclasses import dataclass, field
from datetime import datetime
import hashlib
import json
import logging
import re
import time
from typing import Any, Dict, List, Optional, Tuple
import unicodedata

import numpy as np

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class TextAnalysis:
    """í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼ ë°ì´í„° êµ¬ì¡°"""

    text_id: str
    original_text: str
    processed_text: str
    analysis_type: str
    features: Dict[str, Any]
    sentiment_score: float
    confidence: float
    created_at: datetime = field(default_factory=datetime.now)


@dataclass
class SemanticExtraction:
    """ì˜ë¯¸ ì¶”ì¶œ ê²°ê³¼ ë°ì´í„° êµ¬ì¡°"""

    extraction_id: str
    source_text: str
    extracted_entities: List[Dict[str, Any]]
    extracted_concepts: List[Dict[str, Any]]
    relationships: List[Dict[str, Any]]
    semantic_graph: Dict[str, Any]
    confidence: float
    created_at: datetime = field(default_factory=datetime.now)


@dataclass
class ContextualAnalysis:
    """ë¬¸ë§¥ ë¶„ì„ ê²°ê³¼ ë°ì´í„° êµ¬ì¡°"""

    context_id: str
    text: str
    context_type: str
    contextual_features: Dict[str, Any]
    context_score: float
    related_contexts: List[str]
    created_at: datetime = field(default_factory=datetime.now)


@dataclass
class LanguageSupport:
    """ë‹¤êµ­ì–´ ì§€ì› ë°ì´í„° êµ¬ì¡°"""

    language_code: str
    language_name: str
    supported_features: List[str]
    processing_rules: Dict[str, Any]
    created_at: datetime = field(default_factory=datetime.now)


class AdvancedTextAnalyzer:
    """ê³ ê¸‰ í…ìŠ¤íŠ¸ ë¶„ì„ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.text_cache = {}
        self.analysis_models = {}
        self.feature_extractors = {}
        self.sentiment_analyzer = {}
        self.confidence_threshold = 0.6

    def analyze_text(
        self, text: str, analysis_type: str = "comprehensive"
    ) -> TextAnalysis:
        """í…ìŠ¤íŠ¸ ë¶„ì„"""
        text_id = f"text_{int(time.time())}"

        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(text.encode()).hexdigest()
        if cache_key in self.text_cache:
            return self.text_cache[cache_key]

        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_text = self.preprocess_text(text)

        # ë¶„ì„ ì‹¤í–‰
        if analysis_type == "comprehensive":
            features = self.comprehensive_analysis(processed_text)
        elif analysis_type == "sentiment":
            features = self.sentiment_analysis(processed_text)
        elif analysis_type == "structural":
            features = self.structural_analysis(processed_text)
        else:
            features = self.basic_analysis(processed_text)

        # ê°ì • ë¶„ì„
        sentiment_score = self.calculate_sentiment(processed_text)

        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self.calculate_confidence(features, sentiment_score)

        # ë¶„ì„ ê²°ê³¼ ìƒì„±
        analysis_result = TextAnalysis(
            text_id=text_id,
            original_text=text,
            processed_text=processed_text,
            analysis_type=analysis_type,
            features=features,
            sentiment_score=sentiment_score,
            confidence=confidence,
        )

        self.text_cache[cache_key] = analysis_result
        return analysis_result

    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # ê¸°ë³¸ ì •ê·œí™”
        text = text.strip()
        text = re.sub(r"\s+", " ", text)  # ì—°ì†ëœ ê³µë°± ì œê±°

        # íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬
        text = unicodedata.normalize("NFKC", text)

        # ê¸°ë³¸ ì •ì œ
        text = re.sub(r"[^\w\s\.\,\!\?\;\:\-\(\)]", "", text)

        return text

    def comprehensive_analysis(self, text: str) -> Dict[str, Any]:
        """ì¢…í•©ì  í…ìŠ¤íŠ¸ ë¶„ì„"""
        features = {}

        # ê¸°ë³¸ í†µê³„
        features["length"] = len(text)
        features["word_count"] = len(text.split())
        features["sentence_count"] = len(re.split(r"[.!?]+", text))
        features["paragraph_count"] = len(text.split("\n\n"))

        # ì–´íœ˜ ë¶„ì„
        words = text.lower().split()
        features["unique_words"] = len(set(words))
        features["vocabulary_diversity"] = len(set(words)) / len(words) if words else 0

        # ë¬¸ì¥ ê¸¸ì´ ë¶„ì„
        sentences = re.split(r"[.!?]+", text)
        sentence_lengths = [len(s.split()) for s in sentences if s.strip()]
        if sentence_lengths:
            features["avg_sentence_length"] = np.mean(sentence_lengths)
            features["sentence_length_std"] = np.std(sentence_lengths)

        # ì–´íœ˜ ë³µì¡ë„
        features["avg_word_length"] = (
            np.mean([len(word) for word in words]) if words else 0
        )

        # ë¬¸ì²´ ë¶„ì„
        features["formal_indicators"] = self.count_formal_indicators(text)
        features["informal_indicators"] = self.count_informal_indicators(text)

        return features

    def sentiment_analysis(self, text: str) -> Dict[str, Any]:
        """ê°ì • ë¶„ì„"""
        features = {}

        # ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ë¶„ì„
        positive_words = [
            "ì¢‹ë‹¤",
            "í›Œë¥­í•˜ë‹¤",
            "ë©‹ì§€ë‹¤",
            "í–‰ë³µí•˜ë‹¤",
            "ì„±ê³µí•˜ë‹¤",
            "ì¢‹ì€",
            "í›Œë¥­í•œ",
            "ë©‹ì§„",
        ]
        negative_words = [
            "ë‚˜ì˜ë‹¤",
            "ë”ì°í•˜ë‹¤",
            "ì‹¤íŒ¨í•˜ë‹¤",
            "ìŠ¬í”„ë‹¤",
            "í™”ë‚˜ë‹¤",
            "ë‚˜ìœ",
            "ë”ì°í•œ",
            "ì‹¤íŒ¨í•œ",
        ]

        words = text.lower().split()
        positive_count = sum(1 for word in words if word in positive_words)
        negative_count = sum(1 for word in words if word in negative_words)

        features["positive_word_count"] = positive_count
        features["negative_word_count"] = negative_count
        features["sentiment_ratio"] = (
            (positive_count - negative_count) / len(words) if words else 0
        )

        # ê°ì • ê°•ë„
        features["emotion_intensity"] = (
            (positive_count + negative_count) / len(words) if words else 0
        )

        return features

    def structural_analysis(self, text: str) -> Dict[str, Any]:
        """êµ¬ì¡°ì  ë¶„ì„"""
        features = {}

        # ë¬¸ì¥ êµ¬ì¡° ë¶„ì„
        sentences = re.split(r"[.!?]+", text)
        features["sentence_count"] = len([s for s in sentences if s.strip()])

        # ë‹¨ë½ êµ¬ì¡° ë¶„ì„
        paragraphs = text.split("\n\n")
        features["paragraph_count"] = len([p for p in paragraphs if p.strip()])

        # ë¬¸ì¥ ìœ í˜• ë¶„ì„
        question_count = len(re.findall(r"\?", text))
        exclamation_count = len(re.findall(r"\!", text))

        features["question_count"] = question_count
        features["exclamation_count"] = exclamation_count
        features["question_ratio"] = (
            question_count / features["sentence_count"]
            if features["sentence_count"] > 0
            else 0
        )

        return features

    def basic_analysis(self, text: str) -> Dict[str, Any]:
        """ê¸°ë³¸ ë¶„ì„"""
        features = {}
        features["length"] = len(text)
        features["word_count"] = len(text.split())
        features["character_count"] = len(text.replace(" ", ""))
        return features

    def calculate_sentiment(self, text: str) -> float:
        """ê°ì • ì ìˆ˜ ê³„ì‚°"""
        words = text.lower().split()
        if not words:
            return 0.0

        # ê°ì • ì‚¬ì „ ê¸°ë°˜ ë¶„ì„
        positive_words = [
            "ì¢‹ë‹¤",
            "í›Œë¥­í•˜ë‹¤",
            "ë©‹ì§€ë‹¤",
            "í–‰ë³µí•˜ë‹¤",
            "ì„±ê³µí•˜ë‹¤",
            "ì¢‹ì€",
            "í›Œë¥­í•œ",
            "ë©‹ì§„",
        ]
        negative_words = [
            "ë‚˜ì˜ë‹¤",
            "ë”ì°í•˜ë‹¤",
            "ì‹¤íŒ¨í•˜ë‹¤",
            "ìŠ¬í”„ë‹¤",
            "í™”ë‚˜ë‹¤",
            "ë‚˜ìœ",
            "ë”ì°í•œ",
            "ì‹¤íŒ¨í•œ",
        ]

        positive_count = sum(1 for word in words if word in positive_words)
        negative_count = sum(1 for word in words if word in negative_words)

        total_sentiment_words = positive_count + negative_count
        if total_sentiment_words == 0:
            return 0.0

        sentiment_score = (positive_count - negative_count) / total_sentiment_words
        return max(-1.0, min(1.0, sentiment_score))

    def calculate_confidence(
        self, features: Dict[str, Any], sentiment_score: float
    ) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 0.5  # ê¸°ë³¸ ì‹ ë¢°ë„

        # íŠ¹ì§• ê¸°ë°˜ ì‹ ë¢°ë„ ì¡°ì •
        if features.get("word_count", 0) > 10:
            confidence += 0.2

        if features.get("vocabulary_diversity", 0) > 0.5:
            confidence += 0.1

        if abs(sentiment_score) > 0.3:
            confidence += 0.1

        return min(1.0, confidence)

    def count_formal_indicators(self, text: str) -> int:
        """í˜•ì‹ì  ì§€í‘œ ì¹´ìš´íŠ¸"""
        formal_indicators = ["ì…ë‹ˆë‹¤", "ìŠµë‹ˆë‹¤", "ìŠµë‹ˆë‹¤", "ì…ë‹ˆë‹¤", "ì…ë‹ˆë‹¤"]
        return sum(text.count(indicator) for indicator in formal_indicators)

    def count_informal_indicators(self, text: str) -> int:
        """ë¹„í˜•ì‹ì  ì§€í‘œ ì¹´ìš´íŠ¸"""
        informal_indicators = ["ì•¼", "ì–´", "ì•„", "ë„¤", "ìš”"]
        return sum(text.count(indicator) for indicator in informal_indicators)


class SemanticExtractor:
    """ì˜ë¯¸ ì¶”ì¶œ ë° ì´í•´ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.entity_extractors = {}
        self.concept_extractors = {}
        self.relationship_extractors = {}
        self.semantic_graphs = {}
        self.extraction_cache = {}

    def extract_semantics(self, text: str) -> SemanticExtraction:
        """ì˜ë¯¸ ì¶”ì¶œ"""
        extraction_id = f"extraction_{int(time.time())}"

        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(text.encode()).hexdigest()
        if cache_key in self.extraction_cache:
            return self.extraction_cache[cache_key]

        # ì—”í‹°í‹° ì¶”ì¶œ
        entities = self.extract_entities(text)

        # ê°œë… ì¶”ì¶œ
        concepts = self.extract_concepts(text)

        # ê´€ê³„ ì¶”ì¶œ
        relationships = self.extract_relationships(text, entities, concepts)

        # ì˜ë¯¸ ê·¸ë˜í”„ ìƒì„±
        semantic_graph = self.build_semantic_graph(entities, concepts, relationships)

        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self.calculate_extraction_confidence(
            entities, concepts, relationships
        )

        # ì¶”ì¶œ ê²°ê³¼ ìƒì„±
        extraction_result = SemanticExtraction(
            extraction_id=extraction_id,
            source_text=text,
            extracted_entities=entities,
            extracted_concepts=concepts,
            relationships=relationships,
            semantic_graph=semantic_graph,
            confidence=confidence,
        )

        self.extraction_cache[cache_key] = extraction_result
        return extraction_result

    def extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """ì—”í‹°í‹° ì¶”ì¶œ"""
        entities = []

        # ëª…ì‚¬ ì¶”ì¶œ (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
        nouns = re.findall(r"\b[ê°€-í£]+[ì´|ê°€|ì„|ë¥¼|ì˜|ì—|ë¡œ|ì™€|ê³¼]\b", text)

        for noun in nouns:
            # ëª…ì‚¬ ì •ì œ
            clean_noun = re.sub(r"[ì´|ê°€|ì„|ë¥¼|ì˜|ì—|ë¡œ|ì™€|ê³¼]$", "", noun)
            if len(clean_noun) > 1:
                entities.append(
                    {
                        "text": clean_noun,
                        "type": "noun",
                        "confidence": 0.7,
                        "position": text.find(noun),
                    }
                )

        # ìˆ«ì ì¶”ì¶œ
        numbers = re.findall(r"\d+", text)
        for number in numbers:
            entities.append(
                {
                    "text": number,
                    "type": "number",
                    "confidence": 0.9,
                    "position": text.find(number),
                }
            )

        # ë‚ ì§œ ì¶”ì¶œ
        dates = re.findall(r"\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼", text)
        for date in dates:
            entities.append(
                {
                    "text": date,
                    "type": "date",
                    "confidence": 0.8,
                    "position": text.find(date),
                }
            )

        return entities

    def extract_concepts(self, text: str) -> List[Dict[str, Any]]:
        """ê°œë… ì¶”ì¶œ"""
        concepts = []

        # í‚¤ì›Œë“œ ì¶”ì¶œ
        words = text.split()
        word_freq = Counter(words)

        # ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        for word, freq in word_freq.most_common(10):
            if freq > 1 and len(word) > 1:
                concepts.append(
                    {
                        "text": word,
                        "type": "keyword",
                        "frequency": freq,
                        "confidence": min(0.9, freq / len(words)),
                    }
                )

        # ì£¼ì œ ì¶”ì¶œ (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
        topics = {
            "ê¸°ìˆ ": ["ê¸°ìˆ ", "ê°œë°œ", "í”„ë¡œê·¸ë˜ë°", "ì½”ë”©", "ì†Œí”„íŠ¸ì›¨ì–´"],
            "ë¹„ì¦ˆë‹ˆìŠ¤": ["ë¹„ì¦ˆë‹ˆìŠ¤", "ê²½ì˜", "ë§ˆì¼€íŒ…", "ì „ëµ", "ìˆ˜ìµ"],
            "êµìœ¡": ["êµìœ¡", "í•™ìŠµ", "í›ˆë ¨", "ê°•ì˜", "ìˆ˜ì—…"],
            "ê±´ê°•": ["ê±´ê°•", "ìš´ë™", "ì˜ë£Œ", "ì¹˜ë£Œ", "ì˜ˆë°©"],
        }

        for topic, keywords in topics.items():
            for keyword in keywords:
                if keyword in text:
                    concepts.append(
                        {
                            "text": topic,
                            "type": "topic",
                            "confidence": 0.8,
                            "keywords": [keyword],
                        }
                    )
                    break

        return concepts

    def extract_relationships(
        self, text: str, entities: List[Dict[str, Any]], concepts: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ê´€ê³„ ì¶”ì¶œ"""
        relationships = []

        # ì—”í‹°í‹° ê°„ ê´€ê³„ ì¶”ì¶œ
        for i, entity1 in enumerate(entities):
            for j, entity2 in enumerate(entities[i + 1 :], i + 1):
                # ê°„ë‹¨í•œ ê±°ë¦¬ ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œ
                distance = abs(entity1["position"] - entity2["position"])
                if distance < 50:  # 50ì ì´ë‚´
                    relationships.append(
                        {
                            "source": entity1["text"],
                            "target": entity2["text"],
                            "type": "proximity",
                            "confidence": max(0.3, 1.0 - distance / 50),
                            "distance": distance,
                        }
                    )

        # ê°œë…ê³¼ ì—”í‹°í‹° ê°„ ê´€ê³„
        for concept in concepts:
            for entity in entities:
                if (
                    concept["text"] in entity["text"]
                    or entity["text"] in concept["text"]
                ):
                    relationships.append(
                        {
                            "source": concept["text"],
                            "target": entity["text"],
                            "type": "contains",
                            "confidence": 0.8,
                        }
                    )

        return relationships

    def build_semantic_graph(
        self,
        entities: List[Dict[str, Any]],
        concepts: List[Dict[str, Any]],
        relationships: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """ì˜ë¯¸ ê·¸ë˜í”„ ìƒì„±"""
        graph = {
            "nodes": [],
            "edges": [],
            "metadata": {
                "node_count": len(entities) + len(concepts),
                "edge_count": len(relationships),
                "created_at": datetime.now().isoformat(),
            },
        }

        # ë…¸ë“œ ì¶”ê°€
        for entity in entities:
            graph["nodes"].append(
                {
                    "id": entity["text"],
                    "type": "entity",
                    "entity_type": entity["type"],
                    "confidence": entity["confidence"],
                }
            )

        for concept in concepts:
            graph["nodes"].append(
                {
                    "id": concept["text"],
                    "type": "concept",
                    "concept_type": concept["type"],
                    "confidence": concept["confidence"],
                }
            )

        # ì—£ì§€ ì¶”ê°€
        for relationship in relationships:
            graph["edges"].append(
                {
                    "source": relationship["source"],
                    "target": relationship["target"],
                    "type": relationship["type"],
                    "confidence": relationship["confidence"],
                }
            )

        return graph

    def calculate_extraction_confidence(
        self,
        entities: List[Dict[str, Any]],
        concepts: List[Dict[str, Any]],
        relationships: List[Dict[str, Any]],
    ) -> float:
        """ì¶”ì¶œ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not entities and not concepts:
            return 0.0

        # ì—”í‹°í‹° ì‹ ë¢°ë„
        entity_confidence = (
            np.mean([e["confidence"] for e in entities]) if entities else 0.0
        )

        # ê°œë… ì‹ ë¢°ë„
        concept_confidence = (
            np.mean([c["confidence"] for c in concepts]) if concepts else 0.0
        )

        # ê´€ê³„ ì‹ ë¢°ë„
        relationship_confidence = (
            np.mean([r["confidence"] for r in relationships]) if relationships else 0.0
        )

        # ì¢…í•© ì‹ ë¢°ë„
        total_confidence = (
            entity_confidence + concept_confidence + relationship_confidence
        ) / 3
        return min(1.0, total_confidence)


class ContextualProcessor:
    """ë¬¸ë§¥ ì¸ì‹ ë° ì²˜ë¦¬ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.context_models = {}
        self.context_cache = {}
        self.contextual_rules = {}
        self.context_analyzer = {}

    def analyze_context(
        self, text: str, context_type: str = "general"
    ) -> ContextualAnalysis:
        """ë¬¸ë§¥ ë¶„ì„"""
        context_id = f"context_{int(time.time())}"

        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5((text + context_type).encode()).hexdigest()
        if cache_key in self.context_cache:
            return self.context_cache[cache_key]

        # ë¬¸ë§¥ íŠ¹ì§• ì¶”ì¶œ
        contextual_features = self.extract_contextual_features(text, context_type)

        # ë¬¸ë§¥ ì ìˆ˜ ê³„ì‚°
        context_score = self.calculate_context_score(contextual_features)

        # ê´€ë ¨ ë¬¸ë§¥ ì°¾ê¸°
        related_contexts = self.find_related_contexts(text, context_type)

        # ë¬¸ë§¥ ë¶„ì„ ê²°ê³¼ ìƒì„±
        context_analysis = ContextualAnalysis(
            context_id=context_id,
            text=text,
            context_type=context_type,
            contextual_features=contextual_features,
            context_score=context_score,
            related_contexts=related_contexts,
        )

        self.context_cache[cache_key] = context_analysis
        return context_analysis

    def extract_contextual_features(
        self, text: str, context_type: str
    ) -> Dict[str, Any]:
        """ë¬¸ë§¥ íŠ¹ì§• ì¶”ì¶œ"""
        features = {}

        # ì‹œê°„ì  ë¬¸ë§¥
        features["temporal_context"] = self.extract_temporal_context(text)

        # ê³µê°„ì  ë¬¸ë§¥
        features["spatial_context"] = self.extract_spatial_context(text)

        # ì‚¬íšŒì  ë¬¸ë§¥
        features["social_context"] = self.extract_social_context(text)

        # ì£¼ì œì  ë¬¸ë§¥
        features["topical_context"] = self.extract_topical_context(text)

        # ê°ì •ì  ë¬¸ë§¥
        features["emotional_context"] = self.extract_emotional_context(text)

        return features

    def extract_temporal_context(self, text: str) -> Dict[str, Any]:
        """ì‹œê°„ì  ë¬¸ë§¥ ì¶”ì¶œ"""
        temporal_features = {}

        # ì‹œê°„ í‘œí˜„ ì¶”ì¶œ
        time_patterns = [
            r"\d{4}ë…„",
            r"\d{1,2}ì›”",
            r"\d{1,2}ì¼",
            r"ì˜¤ëŠ˜",
            r"ì–´ì œ",
            r"ë‚´ì¼",
            r"ì´ë²ˆ ì£¼",
            r"ë‹¤ìŒ ì£¼",
        ]

        temporal_features["time_expressions"] = []
        for pattern in time_patterns:
            matches = re.findall(pattern, text)
            temporal_features["time_expressions"].extend(matches)

        temporal_features["temporal_density"] = (
            len(temporal_features["time_expressions"]) / len(text.split())
            if text.split()
            else 0
        )

        return temporal_features

    def extract_spatial_context(self, text: str) -> Dict[str, Any]:
        """ê³µê°„ì  ë¬¸ë§¥ ì¶”ì¶œ"""
        spatial_features = {}

        # ì¥ì†Œ í‘œí˜„ ì¶”ì¶œ
        location_patterns = [
            r"[ê°€-í£]+ì‹œ",
            r"[ê°€-í£]+êµ¬",
            r"[ê°€-í£]+ë™",
            r"[ê°€-í£]+êµ­",
            r"[ê°€-í£]+íšŒì‚¬",
            r"[ê°€-í£]+í•™êµ",
        ]

        spatial_features["location_expressions"] = []
        for pattern in location_patterns:
            matches = re.findall(pattern, text)
            spatial_features["location_expressions"].extend(matches)

        spatial_features["spatial_density"] = (
            len(spatial_features["location_expressions"]) / len(text.split())
            if text.split()
            else 0
        )

        return spatial_features

    def extract_social_context(self, text: str) -> Dict[str, Any]:
        """ì‚¬íšŒì  ë¬¸ë§¥ ì¶”ì¶œ"""
        social_features = {}

        # ì‚¬íšŒì  ê´€ê³„ í‘œí˜„
        social_indicators = ["ì¹œêµ¬", "ê°€ì¡±", "ë™ë£Œ", "ìƒì‚¬", "ë¶€í•˜", "ê³ ê°", "íŒŒíŠ¸ë„ˆ"]

        social_features["social_indicators"] = []
        for indicator in social_indicators:
            if indicator in text:
                social_features["social_indicators"].append(indicator)

        social_features["social_density"] = (
            len(social_features["social_indicators"]) / len(text.split())
            if text.split()
            else 0
        )

        return social_features

    def extract_topical_context(self, text: str) -> Dict[str, Any]:
        """ì£¼ì œì  ë¬¸ë§¥ ì¶”ì¶œ"""
        topical_features = {}

        # ì£¼ì œ í‚¤ì›Œë“œ
        topics = {
            "ê¸°ìˆ ": ["ê¸°ìˆ ", "ê°œë°œ", "í”„ë¡œê·¸ë˜ë°", "ì½”ë”©"],
            "ë¹„ì¦ˆë‹ˆìŠ¤": ["ë¹„ì¦ˆë‹ˆìŠ¤", "ê²½ì˜", "ë§ˆì¼€íŒ…", "ì „ëµ"],
            "êµìœ¡": ["êµìœ¡", "í•™ìŠµ", "í›ˆë ¨", "ê°•ì˜"],
            "ê±´ê°•": ["ê±´ê°•", "ìš´ë™", "ì˜ë£Œ", "ì¹˜ë£Œ"],
        }

        topical_features["detected_topics"] = []
        for topic, keywords in topics.items():
            for keyword in keywords:
                if keyword in text:
                    topical_features["detected_topics"].append(topic)
                    break

        return topical_features

    def extract_emotional_context(self, text: str) -> Dict[str, Any]:
        """ê°ì •ì  ë¬¸ë§¥ ì¶”ì¶œ"""
        emotional_features = {}

        # ê°ì • í‘œí˜„
        emotions = {
            "ê¸°ì¨": ["ê¸°ì˜ë‹¤", "í–‰ë³µí•˜ë‹¤", "ì¦ê²ë‹¤", "ì‹ ë‚˜ë‹¤"],
            "ìŠ¬í””": ["ìŠ¬í”„ë‹¤", "ìš°ìš¸í•˜ë‹¤", "ì†ìƒí•˜ë‹¤"],
            "í™”ë‚¨": ["í™”ë‚˜ë‹¤", "ì§œì¦ë‚˜ë‹¤", "ë¶„ë…¸í•˜ë‹¤"],
            "ê±±ì •": ["ê±±ì •í•˜ë‹¤", "ë¶ˆì•ˆí•˜ë‹¤", "ê·¼ì‹¬í•˜ë‹¤"],
        }

        emotional_features["detected_emotions"] = []
        for emotion, keywords in emotions.items():
            for keyword in keywords:
                if keyword in text:
                    emotional_features["detected_emotions"].append(emotion)
                    break

        return emotional_features

    def calculate_context_score(self, contextual_features: Dict[str, Any]) -> float:
        """ë¬¸ë§¥ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0

        # ê° ë¬¸ë§¥ ìœ í˜•ë³„ ì ìˆ˜ ê³„ì‚°
        if (
            contextual_features.get("temporal_context", {}).get("temporal_density", 0)
            > 0
        ):
            score += 0.2

        if contextual_features.get("spatial_context", {}).get("spatial_density", 0) > 0:
            score += 0.2

        if contextual_features.get("social_context", {}).get("social_density", 0) > 0:
            score += 0.2

        if contextual_features.get("topical_context", {}).get("detected_topics"):
            score += 0.2

        if contextual_features.get("emotional_context", {}).get("detected_emotions"):
            score += 0.2

        return min(1.0, score)

    def find_related_contexts(self, text: str, context_type: str) -> List[str]:
        """ê´€ë ¨ ë¬¸ë§¥ ì°¾ê¸°"""
        related_contexts = []

        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ ë¬¸ë§¥ ì°¾ê¸°
        keywords = text.split()[:5]  # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ

        for keyword in keywords:
            if len(keyword) > 1:
                related_contexts.append(f"í‚¤ì›Œë“œ '{keyword}' ê´€ë ¨ ë¬¸ë§¥")

        return related_contexts


class MultilingualSupport:
    """ë‹¤êµ­ì–´ ì§€ì› ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.supported_languages = {}
        self.language_detectors = {}
        self.translation_models = {}
        self.processing_rules = {}

    def detect_language(self, text: str) -> str:
        """ì–¸ì–´ ê°ì§€"""
        # ê°„ë‹¨í•œ ì–¸ì–´ ê°ì§€ (í•œêµ­ì–´ ì¤‘ì‹¬)
        korean_chars = len(re.findall(r"[ê°€-í£]", text))
        english_chars = len(re.findall(r"[a-zA-Z]", text))

        if korean_chars > english_chars:
            return "ko"
        elif english_chars > korean_chars:
            return "en"
        else:
            return "unknown"

    def add_language_support(
        self,
        language_code: str,
        language_name: str,
        features: List[str],
        rules: Dict[str, Any],
    ):
        """ì–¸ì–´ ì§€ì› ì¶”ê°€"""
        language_support = LanguageSupport(
            language_code=language_code,
            language_name=language_name,
            supported_features=features,
            processing_rules=rules,
        )

        self.supported_languages[language_code] = language_support
        logger.info(f"ì–¸ì–´ ì§€ì› ì¶”ê°€ë¨: {language_code} - {language_name}")

    def get_language_support(self, language_code: str) -> Optional[LanguageSupport]:
        """ì–¸ì–´ ì§€ì› ì •ë³´ ì¡°íšŒ"""
        return self.supported_languages.get(language_code)

    def process_multilingual_text(
        self, text: str, target_language: str = None
    ) -> Dict[str, Any]:
        """ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        detected_language = self.detect_language(text)

        result = {
            "original_text": text,
            "detected_language": detected_language,
            "target_language": target_language,
            "processing_result": {},
        }

        # ì–¸ì–´ë³„ ì²˜ë¦¬ ê·œì¹™ ì ìš©
        if detected_language in self.supported_languages:
            language_support = self.supported_languages[detected_language]
            result["processing_result"] = self.apply_language_rules(
                text, language_support.processing_rules
            )

        return result

    def apply_language_rules(self, text: str, rules: Dict[str, Any]) -> Dict[str, Any]:
        """ì–¸ì–´ë³„ ì²˜ë¦¬ ê·œì¹™ ì ìš©"""
        result = {}

        for rule_name, rule_func in rules.items():
            try:
                result[rule_name] = rule_func(text)
            except Exception as e:
                logger.error(f"ê·œì¹™ ì ìš© ì¤‘ ì˜¤ë¥˜: {rule_name} - {e}")
                result[rule_name] = None

        return result


class NaturalLanguageProcessingSystem:
    """ìì—°ì–´ ì²˜ë¦¬ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.text_analyzer = AdvancedTextAnalyzer()
        self.semantic_extractor = SemanticExtractor()
        self.contextual_processor = ContextualProcessor()
        self.multilingual_support = MultilingualSupport()
        self.system_status = "active"
        self.performance_metrics = defaultdict(float)

        # ê¸°ë³¸ ì–¸ì–´ ì§€ì› ì„¤ì •
        self.setup_default_language_support()

    def setup_default_language_support(self):
        """ê¸°ë³¸ ì–¸ì–´ ì§€ì› ì„¤ì •"""
        # í•œêµ­ì–´ ì§€ì›
        self.multilingual_support.add_language_support(
            "ko",
            "Korean",
            ["text_analysis", "semantic_extraction", "contextual_analysis"],
            {
                "preprocessing": lambda text: text.strip(),
                "tokenization": lambda text: text.split(),
                "normalization": lambda text: unicodedata.normalize("NFKC", text),
            },
        )

        # ì˜ì–´ ì§€ì›
        self.multilingual_support.add_language_support(
            "en",
            "English",
            ["text_analysis", "semantic_extraction", "contextual_analysis"],
            {
                "preprocessing": lambda text: text.strip(),
                "tokenization": lambda text: text.split(),
                "normalization": lambda text: text.lower(),
            },
        )

    async def process_text(
        self, text: str, processing_type: str = "comprehensive"
    ) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        start_time = time.time()

        try:
            # ì–¸ì–´ ê°ì§€
            detected_language = self.multilingual_support.detect_language(text)

            # í…ìŠ¤íŠ¸ ë¶„ì„
            text_analysis = self.text_analyzer.analyze_text(text, "comprehensive")

            # ì˜ë¯¸ ì¶”ì¶œ
            semantic_extraction = self.semantic_extractor.extract_semantics(text)

            # ë¬¸ë§¥ ë¶„ì„
            contextual_analysis = self.contextual_processor.analyze_context(
                text, "general"
            )

            # ë‹¤êµ­ì–´ ì²˜ë¦¬
            multilingual_result = self.multilingual_support.process_multilingual_text(
                text, detected_language
            )

            # ê²°ê³¼ í†µí•©
            result = {
                "processing_type": processing_type,
                "detected_language": detected_language,
                "text_analysis": text_analysis.__dict__,
                "semantic_extraction": semantic_extraction.__dict__,
                "contextual_analysis": contextual_analysis.__dict__,
                "multilingual_processing": multilingual_result,
                "processing_time": time.time() - start_time,
                "system_status": self.system_status,
            }

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.performance_metrics["processing_time"] = result["processing_time"]
            self.performance_metrics["request_count"] += 1

            return result

        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "error": str(e),
                "status": "error",
                "processing_time": time.time() - start_time,
            }

    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
        return {
            "system_status": self.system_status,
            "performance_metrics": dict(self.performance_metrics),
            "supported_languages": list(
                self.multilingual_support.supported_languages.keys()
            ),
            "component_status": {
                "text_analyzer": "active",
                "semantic_extractor": "active",
                "contextual_processor": "active",
                "multilingual_support": "active",
            },
        }

    def get_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë³´ê³ ì„œ"""
        return {
            "total_requests": self.performance_metrics["request_count"],
            "avg_processing_time": self.performance_metrics["processing_time"],
            "system_uptime": time.time(),
            "component_performance": {
                "text_analyzer": "high",
                "semantic_extractor": "high",
                "contextual_processor": "high",
                "multilingual_support": "high",
            },
        }


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_natural_language_processing_system():
    """ìì—°ì–´ ì²˜ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ ìì—°ì–´ ì²˜ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")

    nlp_system = NaturalLanguageProcessingSystem()

    # 1. í…ìŠ¤íŠ¸ ë¶„ì„ í…ŒìŠ¤íŠ¸
    print("\n1. í…ìŠ¤íŠ¸ ë¶„ì„ í…ŒìŠ¤íŠ¸")
    test_text = "ì˜¤ëŠ˜ì€ ì •ë§ ì¢‹ì€ ë‚ ì”¨ì…ë‹ˆë‹¤. ì¹œêµ¬ë“¤ê³¼ í•¨ê»˜ ê³µì›ì—ì„œ í”¼í¬ë‹‰ì„ ì¦ê²¼ì–´ìš”."

    analysis_result = await nlp_system.process_text(test_text, "comprehensive")
    print(f"í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼: {analysis_result}")

    # 2. ì˜ë¯¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    print("\n2. ì˜ë¯¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
    semantic_text = "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì•¼ì—ì„œ í˜ì‹ ì„ ê°€ì ¸ì˜¤ê³  ìˆìŠµë‹ˆë‹¤."

    semantic_result = await nlp_system.process_text(semantic_text, "semantic")
    print(f"ì˜ë¯¸ ì¶”ì¶œ ê²°ê³¼: {semantic_result}")

    # 3. ë¬¸ë§¥ ë¶„ì„ í…ŒìŠ¤íŠ¸
    print("\n3. ë¬¸ë§¥ ë¶„ì„ í…ŒìŠ¤íŠ¸")
    context_text = (
        "2024ë…„ 3ì›” 15ì¼ ì„œìš¸ì—ì„œ ì—´ë¦° AI ì»¨í¼ëŸ°ìŠ¤ì—ì„œ ìƒˆë¡œìš´ ê¸°ìˆ ì´ ë°œí‘œë˜ì—ˆìŠµë‹ˆë‹¤."
    )

    context_result = await nlp_system.process_text(context_text, "contextual")
    print(f"ë¬¸ë§¥ ë¶„ì„ ê²°ê³¼: {context_result}")

    # 4. ë‹¤êµ­ì–´ ì§€ì› í…ŒìŠ¤íŠ¸
    print("\n4. ë‹¤êµ­ì–´ ì§€ì› í…ŒìŠ¤íŠ¸")
    english_text = (
        "Today is a beautiful day. I enjoyed a picnic with friends in the park."
    )

    multilingual_result = await nlp_system.process_text(english_text, "multilingual")
    print(f"ë‹¤êµ­ì–´ ì²˜ë¦¬ ê²°ê³¼: {multilingual_result}")

    # 5. ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ
    print("\n5. ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ")
    status = nlp_system.get_system_status()
    print(f"ì‹œìŠ¤í…œ ìƒíƒœ: {status}")

    # 6. ì„±ëŠ¥ ë³´ê³ ì„œ
    print("\n6. ì„±ëŠ¥ ë³´ê³ ì„œ")
    performance = nlp_system.get_performance_report()
    print(f"ì„±ëŠ¥ ë³´ê³ ì„œ: {performance}")

    print("\nâœ… ìì—°ì–´ ì²˜ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_natural_language_processing_system())
