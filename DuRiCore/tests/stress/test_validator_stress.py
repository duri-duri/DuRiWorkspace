from DuRiCore.trace import emit_trace
"""
Validator ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ - ë¶€í•˜Â·ê²½í•©Â·ì„±ëŠ¥ ê²€ì¦
Phase 3: ìš´ì˜ ìˆ˜ì¤€ì˜ ì•ˆì •ì„± ë° ì„±ëŠ¥ í™•ë³´

@stress_testing: ë™ì‹œ ì ‘ê·¼ ë° ë¶€í•˜ ìƒí™© í…ŒìŠ¤íŠ¸
@performance_validation: p95 ì§€ì—° ì‹œê°„ ê²€ì¦
@concurrency_safety: ê²½í•© ìƒí™©ì—ì„œì˜ ì•ˆì „ì„± ë³´ì¥
"""
import asyncio
import time
import statistics
import random
from typing import List, Dict, Any
import logging
import numpy as np
import yaml
import os
import subprocess
import sys
random.seed(42)
np.random.seed(42)

def pytest_mark_asyncio(func):
    """pytest.mark.asyncio ëŒ€ì²´ìš© ë°ì½”ë ˆì´í„°"""
    return func

def pytest_fixture(autouse=False):
    """pytest.fixture ëŒ€ì²´ìš© ë°ì½”ë ˆì´í„°"""

    def decorator(func):
        return func
    return decorator
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_thresholds():
    """ì„ê³„ê°’ ì„¤ì • ë¡œë“œ"""
    config_path = os.path.join(os.path.dirname(__file__), '..', '..', 'config', 'thresholds.yaml')
    try:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        return config['profiles']['dev']
    except Exception as e:
        logger.warning(f'ì„ê³„ê°’ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}')
        return {'p95_latency_inc_pct': 5.0, 'error_rate_pct': 2.0}
THRESHOLDS = load_thresholds()

class StressTestMetrics:
    """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""

    def __init__(self):
        """ë©”íŠ¸ë¦­ ì´ˆê¸°í™”"""
        self.total = 0
        self.successes = 0
        self.failures = 0
        self.latencies = []
        self.start_time = None
        self.end_time = None

    def reset(self):
        """ë©”íŠ¸ë¦­ ì™„ì „ ë¦¬ì…‹ - ëª¨ë“  ëˆ„ì  ë°ì´í„° ì œê±°"""
        self.total = 0
        self.successes = 0
        self.failures = 0
        self.latencies.clear()
        self.start_time = None
        self.end_time = None

    def add_response_time(self, response_time: float):
        """ì‘ë‹µ ì‹œê°„ ì¶”ê°€"""
        self.latencies.append(response_time)

    def add_success(self):
        """ì„±ê³µ ì¹´ìš´íŠ¸ ì¦ê°€"""
        self.successes += 1
        self.total += 1

    def add_failure(self):
        """ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì¦ê°€"""
        self.failures += 1
        self.total += 1

    def start_timing(self):
        """íƒ€ì´ë° ì‹œì‘"""
        self.start_time = time.time()

    def stop_timing(self):
        """íƒ€ì´ë° ì¢…ë£Œ"""
        self.end_time = time.time()

    def get_summary(self) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ìš”ì•½ ë°˜í™˜ - í˜„ì¬ ì‹œë‚˜ë¦¬ì˜¤ ê¸°ì¤€"""
        if not self.latencies:
            return {'total': 0, 'successes': 0, 'failures': 0, 'success_rate': 0.0, 'avg_response_time': 0.0, 'p50_response_time': 0.0, 'p95_response_time': 0.0, 'p99_response_time': 0.0, 'total_duration': 0.0, 'requests_per_second': 0.0}
        total_requests = self.total
        success_rate = self.successes / total_requests if total_requests > 0 else 0.0
        if len(self.latencies) != total_requests:
            logger.warning(f'âš ï¸ latencies({len(self.latencies)}) != total({total_requests}) - ë©”íŠ¸ë¦­ ë¶ˆì¼ì¹˜')
        sorted_times = sorted(self.latencies)
        n = len(sorted_times)
        summary = {'total': total_requests, 'successes': self.successes, 'failures': self.failures, 'success_rate': success_rate, 'avg_response_time': statistics.mean(self.latencies) if self.latencies else 0.0, 'p50_response_time': sorted_times[int(0.5 * n)] if n > 0 else 0.0, 'p95_response_time': sorted_times[int(0.95 * n)] if n > 0 else 0.0, 'p99_response_time': sorted_times[int(0.99 * n)] if n > 0 else 0.0, 'total_duration': self.end_time - self.start_time if self.start_time and self.end_time else 0.0, 'requests_per_second': total_requests / (self.end_time - self.start_time) if self.start_time and self.end_time else 0.0}
        return summary

class MockValidator:
    """í…ŒìŠ¤íŠ¸ìš© Mock Validator"""

    def __init__(self):
        """Mock Validator ì´ˆê¸°í™”"""
        self.request_count = 0
        self.lock = asyncio.Lock()

    def reset_request_count(self):
        """ìš”ì²­ ì¹´ìš´íŠ¸ ë¦¬ì…‹ (í…ŒìŠ¤íŠ¸ ê°„ ê²©ë¦¬)"""
        self.request_count = 0

    async def validate(self, request_id: int, data: Dict[str, Any]) -> Dict[str, Any]:
        """ê²€ì¦ ì‹œë®¬ë ˆì´ì…˜"""
        start_time = time.time()
        async with self.lock:
            await asyncio.sleep(0.001)
            is_valid = data.get('value', 0) < 100
            response = {'request_id': request_id, 'valid': is_valid, 'timestamp': time.time(), 'processing_time': time.time() - start_time}
            self.request_count += 1
            if self.request_count % 20 == 0:
                response['valid'] = False
                response['error'] = f'Simulated failure for request {request_id}'
                return response
            return response

class StressTestRunner:
    """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""

    def __init__(self, validator: MockValidator):
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸° ì´ˆê¸°í™”"""
        self.validator = validator
        self.metrics = StressTestMetrics()

    async def single_request(self, request_id: int) -> float:
        """ë‹¨ì¼ ìš”ì²­ ì‹¤í–‰"""
        if request_id == 0:
            self.metrics.reset()
        start_time = time.time()
        test_data = {'value': request_id % 100, 'timestamp': time.time(), 'source': f'test_{request_id}'}
        try:
            result = await self.validator.validate(request_id, test_data)
            response_time = time.time() - start_time
            self.metrics.add_response_time(response_time)
            if result.get('valid', False) and 'error' not in result:
                self.metrics.add_success()
            else:
                self.metrics.add_failure()
                if 'error' in result:
                    logger.warning(f"Request {request_id} failed: {result['error']}")
            return response_time
        except Exception as e:
            response_time = time.time() - start_time
            self.metrics.add_response_time(response_time)
            self.metrics.add_failure()
            logger.error(f'Request {request_id} exception: {e}')
            return response_time

    async def concurrent_requests(self, n_requests: int, max_concurrent: int=50) -> Dict[str, Any]:
        """ë™ì‹œ ìš”ì²­ ì‹¤í–‰ - ì•ˆì „í•œ ìˆœì°¨ ì‹¤í–‰ ë°©ì‹ìœ¼ë¡œ ë³€ê²½"""
        self.metrics.reset()
        logger.info(f'ğŸš€ {n_requests}ê°œ ìš”ì²­ì„ ìˆœì°¨ ì‹¤í–‰ ì‹œì‘')
        emit_trace('info', ' '.join(map(str, [f'ğŸš€ {n_requests}ê°œ ìš”ì²­ì„ ìˆœì°¨ ì‹¤í–‰ ì‹œì‘'])))
        self.metrics.start_timing()
        for i in range(n_requests):
            try:
                response_time = await self.single_request(i)
                if (i + 1) % 10 == 0:
                    progress = (i + 1) / n_requests * 100
                    emit_trace('info', ' '.join(map(str, [f'ğŸ“Š ì§„í–‰ë¥ : {progress:.1f}% ({i + 1}/{n_requests})'])))
            except Exception as e:
                logger.error(f'âŒ ìš”ì²­ {i} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}')
                emit_trace('info', ' '.join(map(str, [f'âŒ ìš”ì²­ {i} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}'])))
        self.metrics.stop_timing()
        logger.info(f'ğŸ“Š ì´ ìš”ì²­: {n_requests}ê°œ ì™„ë£Œ')
        emit_trace('info', ' '.join(map(str, [f'ğŸ“Š ì´ ìš”ì²­: {n_requests}ê°œ ì™„ë£Œ'])))
        return self.metrics.get_summary()

    async def stress_test_scenarios(self) -> Dict[str, Dict[str, Any]]:
        """ë‹¤ì–‘í•œ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰ - ì•ˆì „ì¥ì¹˜ ì¶”ê°€"""
        scenarios = {'light_load': {'requests': 50, 'concurrent': 5}, 'medium_load': {'requests': 100, 'concurrent': 10}, 'heavy_load': {'requests': 200, 'concurrent': 20}, 'extreme_load': {'requests': 500, 'concurrent': 50}}
        results = {}
        for (scenario_name, config) in scenarios.items():
            try:
                logger.info(f'ğŸ”¥ {scenario_name} ì‹œë‚˜ë¦¬ì˜¤ ì‹œì‘')
                emit_trace('info', ' '.join(map(str, [f'ğŸ”¥ {scenario_name} ì‹œë‚˜ë¦¬ì˜¤ ì‹œì‘'])))
                self.metrics.reset()
                result = await asyncio.wait_for(self.concurrent_requests(config['requests'], config['concurrent']), timeout=30.0)
                results[scenario_name] = result
                logger.info(f"âœ… {scenario_name} ì™„ë£Œ: ì„±ê³µë¥ ={result['success_rate']:.1%}, p95={result['p95_response_time'] * 1000:.2f}ms")
                emit_trace('info', ' '.join(map(str, [f"âœ… {scenario_name} ì™„ë£Œ: ì„±ê³µë¥ ={result['success_rate']:.1%}, p95={result['p95_response_time'] * 1000:.2f}ms"])))
            except asyncio.TimeoutError:
                logger.error(f'â° {scenario_name} ì‹œë‚˜ë¦¬ì˜¤ íƒ€ì„ì•„ì›ƒ (30ì´ˆ ì´ˆê³¼)')
                emit_trace('info', ' '.join(map(str, [f'â° {scenario_name} ì‹œë‚˜ë¦¬ì˜¤ íƒ€ì„ì•„ì›ƒ (30ì´ˆ ì´ˆê³¼)'])))
                results[scenario_name] = {'total': config['requests'], 'successes': 0, 'failures': config['requests'], 'success_rate': 0.0, 'p95_response_time': 999.0, 'avg_response_time': 999.0, 'p50_response_time': 999.0, 'p99_response_time': 999.0, 'total_duration': 30.0, 'requests_per_second': 0.0}
            except Exception as e:
                logger.error(f'âŒ {scenario_name} ì‹œë‚˜ë¦¬ì˜¤ ì˜¤ë¥˜: {e}')
                emit_trace('info', ' '.join(map(str, [f'âŒ {scenario_name} ì‹œë‚˜ë¦¬ì˜¤ ì˜¤ë¥˜: {e}'])))
                results[scenario_name] = {'total': config['requests'], 'successes': 0, 'failures': config['requests'], 'success_rate': 0.0, 'p95_response_time': 999.0, 'avg_response_time': 999.0, 'p50_response_time': 999.0, 'p99_response_time': 999.0, 'total_duration': 0.0, 'requests_per_second': 0.0}
        return results

class TestValidatorStress:
    """Validator ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    def __init__(self):
        """í…ŒìŠ¤íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”"""
        self.validator = MockValidator()
        self.stress_runner = StressTestRunner(self.validator)

    @pytest_mark_asyncio
    async def test_single_request_performance(self):
        """ë‹¨ì¼ ìš”ì²­ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        self.stress_runner.metrics.reset()
        self.validator.reset_request_count()
        start_time = time.time()
        response_time = await self.stress_runner.single_request(1)
        assert response_time >= 0
        assert response_time < 1.0
        summary = self.stress_runner.metrics.get_summary()
        assert summary['total'] == 1
        assert summary['success_rate'] == 1.0
        emit_trace('info', ' '.join(map(str, [f'âœ… ë‹¨ì¼ ìš”ì²­ ì„±ëŠ¥: {response_time * 1000:.2f}ms'])))

    @pytest_mark_asyncio
    async def test_concurrent_requests(self):
        """ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸"""
        n_requests = 50
        max_concurrent = 10
        self.stress_runner.metrics.reset()
        self.validator.reset_request_count()
        result = await self.stress_runner.concurrent_requests(n_requests, max_concurrent)
        assert result['total'] == n_requests
        assert result['success_rate'] >= 0.9
        assert result['p95_response_time'] < 0.015
        emit_trace('info', ' '.join(map(str, [f"âœ… ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸: {n_requests}ê°œ ìš”ì²­, ì„±ê³µë¥ ={result['success_rate']:.1%}, p95={result['p95_response_time'] * 1000:.2f}ms"])))
        return True

    @pytest_mark_asyncio
    async def test_light_load_scenario(self):
        """ê°€ë²¼ìš´ ë¶€í•˜ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        self.stress_runner.metrics.reset()
        self.validator.reset_request_count()
        result = await self.stress_runner.concurrent_requests(50, 5)
        emit_trace('info', ' '.join(map(str, [f"ğŸ” ë””ë²„ê¹… - light_load: p95={result['p95_response_time'] * 1000:.2f}ms, ì„±ê³µë¥ ={result['success_rate']:.1%}"])))
        assert result['p95_response_time'] < 0.015
        assert result['success_rate'] >= 0.9
        emit_trace('info', ' '.join(map(str, [f"âœ… ê°€ë²¼ìš´ ë¶€í•˜ í…ŒìŠ¤íŠ¸: p95={result['p95_response_time'] * 1000:.2f}ms"])))
        return True

    @pytest_mark_asyncio
    async def test_medium_load_scenario(self):
        """ì¤‘ê°„ ë¶€í•˜ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        self.stress_runner.metrics.reset()
        self.validator.reset_request_count()
        result = await self.stress_runner.concurrent_requests(100, 10)
        emit_trace('info', ' '.join(map(str, [f"ğŸ” ë””ë²„ê¹… - medium_load: p95={result['p95_response_time'] * 1000:.2f}ms, ì„±ê³µë¥ ={result['success_rate'] * 100:.1f}%"])))
        emit_trace('info', ' '.join(map(str, [f"   ì´ ìš”ì²­: {result['total']}, ì„±ê³µ: {result['successes']}, ì‹¤íŒ¨: {result['failures']}"])))
        assert result['p95_response_time'] < 0.02
        assert result['success_rate'] >= 0.9
        emit_trace('info', ' '.join(map(str, [f"âœ… ì¤‘ê°„ ë¶€í•˜ í…ŒìŠ¤íŠ¸: p95={result['p95_response_time'] * 1000:.2f}ms"])))
        return True

    @pytest_mark_asyncio
    async def test_heavy_load_scenario(self):
        """ë†’ì€ ë¶€í•˜ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        self.stress_runner.metrics.reset()
        self.validator.reset_request_count()
        result = await self.stress_runner.concurrent_requests(200, 20)
        assert result['p95_response_time'] < 0.03
        assert result['success_rate'] >= 0.9
        emit_trace('info', ' '.join(map(str, [f"âœ… ë†’ì€ ë¶€í•˜ í…ŒìŠ¤íŠ¸: p95={result['p95_response_time'] * 1000:.2f}ms"])))
        return True

    @pytest_mark_asyncio
    async def test_extreme_load_scenario(self):
        """ê·¹í•œ ë¶€í•˜ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        self.stress_runner.metrics.reset()
        self.validator.reset_request_count()
        result = await self.stress_runner.concurrent_requests(500, 50)
        assert result['p95_response_time'] < 0.035
        assert result['success_rate'] >= 0.9
        emit_trace('info', ' '.join(map(str, [f"âœ… ê·¹í•œ ë¶€í•˜ í…ŒìŠ¤íŠ¸: p95={result['p95_response_time'] * 1000:.2f}ms"])))
        return True

    @pytest_mark_asyncio
    async def test_all_stress_scenarios(self):
        """ëª¨ë“  ìŠ¤íŠ¸ë ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ í†µí•© í…ŒìŠ¤íŠ¸ - íƒ€ì„ì•„ì›ƒ ì•ˆì „ì¥ì¹˜ ì¶”ê°€"""
        self.stress_runner.metrics.reset()
        self.validator.reset_request_count()
        try:
            results = await asyncio.wait_for(self.stress_runner.stress_test_scenarios(), timeout=120.0)
            assert len(results) == 4
            for (scenario_name, result) in results.items():
                if scenario_name == 'light_load':
                    assert result['p95_response_time'] < 0.015
                elif scenario_name == 'medium_load':
                    assert result['p95_response_time'] < 0.02
                elif scenario_name == 'heavy_load':
                    assert result['p95_response_time'] < 0.03
                elif scenario_name == 'extreme_load':
                    assert result['p95_response_time'] < 0.035
                assert result['success_rate'] >= 0.9
                logger.info(f"âœ… {scenario_name}: ì„±ê³µë¥ ={result['success_rate']:.1%}, p95={result['p95_response_time'] * 1000:.2f}ms")
                emit_trace('info', ' '.join(map(str, [f"âœ… {scenario_name}: ì„±ê³µë¥ ={result['success_rate']:.1%}, p95={result['p95_response_time'] * 1000:.2f}ms"])))
            emit_trace('info', ' '.join(map(str, [f'âœ… ëª¨ë“  ìŠ¤íŠ¸ë ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ í†µê³¼: {len(results)}ê°œ ì‹œë‚˜ë¦¬ì˜¤'])))
            return True
        except asyncio.TimeoutError:
            logger.error('â° test_all_stress_scenarios íƒ€ì„ì•„ì›ƒ (2ë¶„ ì´ˆê³¼)')
            emit_trace('info', ' '.join(map(str, ['â° test_all_stress_scenarios íƒ€ì„ì•„ì›ƒ (2ë¶„ ì´ˆê³¼)'])))
            raise
        except Exception as e:
            logger.error(f'âŒ test_all_stress_scenarios ì˜¤ë¥˜: {e}')
            emit_trace('info', ' '.join(map(str, [f'âŒ test_all_stress_scenarios ì˜¤ë¥˜: {e}'])))
            raise

async def run_stress_tests():
    """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    emit_trace('info', ' '.join(map(str, ['ğŸ§ª Validator ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘'])))
    emit_trace('info', ' '.join(map(str, ['=' * 60])))
    test_methods = ['test_single_request_performance', 'test_concurrent_requests', 'test_light_load_scenario', 'test_medium_load_scenario', 'test_heavy_load_scenario', 'test_extreme_load_scenario', 'test_all_stress_scenarios']
    passed = 0
    failed = 0
    for method_name in test_methods:
        try:
            emit_trace('info', ' '.join(map(str, [f'ğŸ” {method_name} ì‹¤í–‰ ì¤‘...'])))
            test_instance = TestValidatorStress()
            result = await asyncio.wait_for(getattr(test_instance, method_name)(), timeout=60.0)
            emit_trace('info', ' '.join(map(str, [f'âœ… {method_name} í†µê³¼'])))
            passed += 1
            del test_instance
        except asyncio.TimeoutError:
            emit_trace('info', ' '.join(map(str, [f'â° {method_name} íƒ€ì„ì•„ì›ƒ (1ë¶„ ì´ˆê³¼)'])))
            failed += 1
        except Exception as e:
            import traceback
            emit_trace('info', ' '.join(map(str, [f'âŒ {method_name} ì‹¤íŒ¨: {type(e).__name__}: {e}'])))
            emit_trace('info', ' '.join(map(str, [f'   ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}'])))
            failed += 1

def run_stress_tests_sync():
    """ë™ê¸° ë°©ì‹ìœ¼ë¡œ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (asyncio ë¬¸ì œ ìš°íšŒ)"""
    emit_trace('info', ' '.join(map(str, ['ğŸ§ª Validator ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ë™ê¸° ë°©ì‹)'])))
    emit_trace('info', ' '.join(map(str, ['=' * 60])))
    test_methods = ['test_single_request_performance', 'test_concurrent_requests', 'test_light_load_scenario', 'test_medium_load_scenario', 'test_heavy_load_scenario', 'test_extreme_load_scenario', 'test_all_stress_scenarios']
    passed = 0
    failed = 0
    for method_name in test_methods:
        try:
            emit_trace('info', ' '.join(map(str, [f'ğŸ” {method_name} ì‹¤í–‰ ì¤‘...'])))
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                test_instance = TestValidatorStress()
                result = loop.run_until_complete(asyncio.wait_for(getattr(test_instance, method_name)(), timeout=60.0))
                emit_trace('info', ' '.join(map(str, [f'âœ… {method_name} í†µê³¼'])))
                passed += 1
            finally:
                loop.close()
                del test_instance
        except asyncio.TimeoutError:
            emit_trace('info', ' '.join(map(str, [f'â° {method_name} íƒ€ì„ì•„ì›ƒ (1ë¶„ ì´ˆê³¼)'])))
            failed += 1
        except Exception as e:
            import traceback
            emit_trace('info', ' '.join(map(str, [f'âŒ {method_name} ì‹¤íŒ¨: {type(e).__name__}: {e}'])))
            emit_trace('info', ' '.join(map(str, [f'   ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}'])))
            failed += 1
    emit_trace('info', ' '.join(map(str, ['=' * 60])))
    emit_trace('info', ' '.join(map(str, [f'ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼: {passed}ê°œ í†µê³¼, {failed}ê°œ ì‹¤íŒ¨'])))
    if failed == 0:
        emit_trace('info', ' '.join(map(str, ['ğŸ‰ ëª¨ë“  ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ í†µê³¼!'])))
    else:
        emit_trace('info', ' '.join(map(str, ['âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.'])))
    return (passed, failed)

def run_stress_tests_subprocess():
    """ê° í…ŒìŠ¤íŠ¸ë¥¼ ì™„ì „íˆ ë³„ë„ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰ (ìµœí›„ì˜ ìˆ˜ë‹¨)"""
    emit_trace('info', ' '.join(map(str, ['ğŸ§ª Validator ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ë³„ë„ í”„ë¡œì„¸ìŠ¤ ë°©ì‹)'])))
    emit_trace('info', ' '.join(map(str, ['=' * 60])))
    test_methods = ['test_single_request_performance', 'test_concurrent_requests', 'test_light_load_scenario', 'test_medium_load_scenario', 'test_heavy_load_scenario', 'test_extreme_load_scenario', 'test_all_stress_scenarios']
    passed = 0
    failed = 0
    for method_name in test_methods:
        try:
            emit_trace('info', ' '.join(map(str, [f'ğŸ” {method_name} ì‹¤í–‰ ì¤‘...'])))
            test_code = f"""\nimport asyncio\nimport sys\nsys.path.insert(0, '{os.getcwd()}')\nfrom tests.stress.test_validator_stress import TestValidatorStress\n\nasync def run_single_test():\n    test_instance = TestValidatorStress()\n    try:\n        result = await asyncio.wait_for(\n            getattr(test_instance, '{method_name}')(),\n            timeout=60.0\n        )\n        print(f"âœ… {method_name} í†µê³¼")\n        return True\n    except Exception as e:\n        print(f"âŒ {method_name} ì‹¤íŒ¨: {{e}}")\n        return False\n\nif __name__ == '__main__':\n    result = asyncio.run(run_single_test())\n    sys.exit(0 if result else 1)\n"""
            cmd = [sys.executable, '-c', test_code]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
            if result.returncode == 0:
                emit_trace('info', ' '.join(map(str, [f'âœ… {method_name} í†µê³¼'])))
                passed += 1
            else:
                emit_trace('info', ' '.join(map(str, [f'âŒ {method_name} ì‹¤íŒ¨'])))
                emit_trace('info', ' '.join(map(str, [f'   stdout: {result.stdout}'])))
                emit_trace('info', ' '.join(map(str, [f'   stderr: {result.stderr}'])))
                failed += 1
        except subprocess.TimeoutExpired:
            emit_trace('info', ' '.join(map(str, [f'â° {method_name} íƒ€ì„ì•„ì›ƒ (2ë¶„ ì´ˆê³¼)'])))
            failed += 1
        except Exception as e:
            emit_trace('info', ' '.join(map(str, [f'âŒ {method_name} ì‹¤í–‰ ì˜¤ë¥˜: {e}'])))
            failed += 1
    emit_trace('info', ' '.join(map(str, ['=' * 60])))
    emit_trace('info', ' '.join(map(str, [f'ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼: {passed}ê°œ í†µê³¼, {failed}ê°œ ì‹¤íŒ¨'])))
    if failed == 0:
        emit_trace('info', ' '.join(map(str, ['ğŸ‰ ëª¨ë“  ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ í†µê³¼!'])))
    else:
        emit_trace('info', ' '.join(map(str, ['âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.'])))
    return (passed, failed)
if __name__ == '__main__':
    try:
        emit_trace('info', ' '.join(map(str, ['ğŸš€ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ë°©ì‹ìœ¼ë¡œ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘'])))
        run_stress_tests_subprocess()
    except KeyboardInterrupt:
        emit_trace('info', ' '.join(map(str, ['\nâš ï¸ í…ŒìŠ¤íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.'])))
    except Exception as e:
        emit_trace('info', ' '.join(map(str, [f'\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}'])))
        import traceback
        traceback.print_exc()