#!/usr/bin/env python3
"""
DuRiCore Phase 5 Day 2 - ê³ ë„í™”ëœ ê¸°ì–µ ì‹œìŠ¤í…œ
ê¸°ì–µ ë¶„ë¥˜, ì—°ê´€ì„± ë¶„ì„, ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œ êµ¬í˜„
"""

import asyncio
import hashlib
import json
import logging
import math
from collections import Counter, defaultdict
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

import numpy as np

# Phase 6.2.6 ì‹œë§¨í‹± ì§€ì‹ ì—°ê²°ë§ í†µí•©
from semantic_knowledge_graph import ConceptType, InferenceType, SemanticKnowledgeGraph

logger = logging.getLogger(__name__)


class MemoryType(Enum):
    """ê¸°ì–µ íƒ€ì… ì—´ê±°í˜•"""

    EXPERIENCE = "experience"  # ê²½í—˜ ê¸°ì–µ
    KNOWLEDGE = "knowledge"  # ì§€ì‹ ê¸°ì–µ
    PATTERN = "pattern"  # íŒ¨í„´ ê¸°ì–µ
    EMOTION = "emotion"  # ê°ì • ê¸°ì–µ
    WORKING = "working"  # ì‘ì—… ê¸°ì–µ (Phase 6.2.2 ì¶”ê°€)


class AssociationType(Enum):
    """ì—°ê´€ì„± íƒ€ì… ì—´ê±°í˜•"""

    SEMANTIC = "semantic"  # ì˜ë¯¸ì  ì—°ê´€ì„±
    TEMPORAL = "temporal"  # ì‹œê°„ì  ì—°ê´€ì„±
    EMOTIONAL = "emotional"  # ê°ì •ì  ì—°ê´€ì„±
    CONTEXTUAL = "contextual"  # ë§¥ë½ì  ì—°ê´€ì„±
    OPERATIONAL = "operational"  # ì—°ì‚°ì  ì—°ê´€ì„± (Phase 6.2.2 ì¶”ê°€)


@dataclass
class MemoryEntry:
    """ê³ ë„í™”ëœ ë©”ëª¨ë¦¬ ì—”íŠ¸ë¦¬"""

    id: str
    content: str
    memory_type: MemoryType
    importance: float
    created_at: datetime
    accessed_count: int
    last_accessed: datetime
    associations: List[str]
    vector_data: List[float]
    metadata: Dict[str, Any]

    # ìƒˆë¡œìš´ í•„ë“œë“¤
    classification_confidence: float
    priority_score: float
    retention_score: float
    association_strength: Dict[str, float]
    tags: List[str]
    context_info: Dict[str, Any]


@dataclass
class AssociationLink:
    """ì—°ê´€ì„± ë§í¬"""

    source_id: str
    target_id: str
    association_type: AssociationType
    strength: float
    created_at: datetime
    metadata: Dict[str, Any]


@dataclass
class WorkingMemoryBuffer:
    """ì‘ì—… ê¸°ì–µ ë²„í¼ (Phase 6.2.2 ì¶”ê°€)"""

    id: str
    content: str
    operation_type: str  # 'addition', 'subtraction', 'comparison', 'integration'
    operands: List[str]  # ì—°ì‚°ì— ì‚¬ìš©ëœ ë©”ëª¨ë¦¬ IDë“¤
    result: str
    confidence: float
    created_at: datetime
    expires_at: datetime  # ì‘ì—… ê¸°ì–µì€ ì¼ì‹œì 
    access_count: int = 0

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()
        if self.expires_at is None:
            # ê¸°ë³¸ì ìœ¼ë¡œ 30ë¶„ í›„ ë§Œë£Œ
            self.expires_at = self.created_at + timedelta(minutes=30)


@dataclass
class MemoryOperation:
    """ë©”ëª¨ë¦¬ ì—°ì‚° ì •ë³´ (Phase 6.2.2 ì¶”ê°€)"""

    operation_id: str
    operation_type: str
    input_memories: List[str]
    output_memory: str
    operation_result: str
    confidence: float
    execution_time: float
    created_at: datetime


class EnhancedMemorySystem:
    """ê³ ë„í™”ëœ ê¸°ì–µ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.memories: Dict[str, MemoryEntry] = {}
        self.associations: Dict[str, AssociationLink] = {}
        self.memory_type_index: Dict[MemoryType, List[str]] = defaultdict(list)
        self.priority_queue: List[str] = []

        # Phase 6.2.2 - Working Memory ì—°ì‚° ë²„í¼ ì¶”ê°€
        self.working_memory_buffers: Dict[str, WorkingMemoryBuffer] = {}
        self.memory_operations: Dict[str, MemoryOperation] = {}
        self.operation_history: List[MemoryOperation] = []

        # ë¶„ë¥˜ ì‹œìŠ¤í…œ ì„¤ì •
        self.classification_threshold = 0.8
        self.association_threshold = 0.6

        # ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œ ì„¤ì •
        self.importance_weight = 0.4
        self.access_weight = 0.3
        self.recency_weight = 0.3

        # Phase 6.2.2 - Working Memory ì„¤ì •
        self.working_memory_capacity = 7  # Miller's Law (7Â±2)
        self.working_memory_ttl = 1800  # 30ë¶„ (ì´ˆ ë‹¨ìœ„)
        self.operation_confidence_threshold = 0.7

        # Phase 6.2.6 - ì‹œë§¨í‹± ì§€ì‹ ì—°ê²°ë§ ì„¤ì •
        self.semantic_graph = SemanticKnowledgeGraph()

        # ë²¡í„° ì°¨ì›
        self.vector_dim = 128

        logger.info("ê³ ë„í™”ëœ ê¸°ì–µ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (Phase 6.2.2, 6.2.6 í¬í•¨)")

    async def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì…ë ¥ ë°ì´í„° ì²˜ë¦¬ (í†µí•© ë£¨í”„ìš©)"""
        try:
            content = input_data.get("content", "")
            context = input_data.get("context", {})
            importance = input_data.get("importance", 0.5)

            # ë©”ëª¨ë¦¬ ì €ì¥
            memory_id = await self.store_memory(content, context, importance)

            # ê´€ë ¨ ë©”ëª¨ë¦¬ ê²€ìƒ‰
            related_memories = await self.get_related_memories(memory_id, limit=5)

            # ë©”ëª¨ë¦¬ í†µê³„
            statistics = await self.get_memory_statistics()

            return {
                "success": True,
                "memory_id": memory_id,
                "related_memories": related_memories,
                "statistics": statistics,
                "data": {
                    "content": content,
                    "context": context,
                    "importance": importance,
                    "memory_id": memory_id,
                },
            }

        except Exception as e:
            logger.error(f"ì…ë ¥ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "data": {}}

    async def store_memory(self, content: str, context: Dict[str, Any], importance: float = 0.5) -> str:
        """ë©”ëª¨ë¦¬ ì €ì¥ (ê³ ë„í™”ëœ ë²„ì „)"""
        try:
            # ê³ ìœ  ID ìƒì„±
            memory_id = self._generate_memory_id(content, context)

            # ê¸°ì–µ íƒ€ì… ë¶„ë¥˜
            memory_type, confidence = await self._classify_memory_type(content, context)

            # ë²¡í„° ë°ì´í„° ìƒì„±
            vector_data = await self._create_enhanced_vector(content, context)

            # íƒœê·¸ ì¶”ì¶œ
            tags = await self._extract_enhanced_tags(content, context)

            # ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°
            priority_score = self._calculate_priority_score(importance, 0, datetime.now())

            # ë³´ì¡´ ì ìˆ˜ ê³„ì‚°
            retention_score = self._calculate_retention_score(content, memory_type, importance)

            # ë©”ëª¨ë¦¬ ì—”íŠ¸ë¦¬ ìƒì„±
            memory_entry = MemoryEntry(
                id=memory_id,
                content=content,
                memory_type=memory_type,
                importance=importance,
                created_at=datetime.now(),
                accessed_count=0,
                last_accessed=datetime.now(),
                associations=[],
                vector_data=vector_data,
                metadata=context,
                classification_confidence=confidence,
                priority_score=priority_score,
                retention_score=retention_score,
                association_strength={},
                tags=tags,
                context_info=context,
            )

            # ì €ì¥
            self.memories[memory_id] = memory_entry
            self.memory_type_index[memory_type].append(memory_id)

            # ìš°ì„ ìˆœìœ„ í ì—…ë°ì´íŠ¸
            self._update_priority_queue(memory_id, priority_score)

            # ì—°ê´€ì„± ë¶„ì„ ë° ì—°ê²°
            await self._analyze_and_link_associations(memory_id)

            logger.info(f"ê³ ë„í™”ëœ ë©”ëª¨ë¦¬ ì €ì¥ ì™„ë£Œ: {memory_id} (íƒ€ì…: {memory_type.value})")
            return memory_id

        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ì €ì¥ ì˜¤ë¥˜: {e}")
            return ""

    async def search_memories(
        self, query: str, memory_type: Optional[MemoryType] = None, limit: int = 10
    ) -> List[Tuple[MemoryEntry, float]]:
        """ê³ ë„í™”ëœ ë©”ëª¨ë¦¬ ê²€ìƒ‰"""
        try:
            results = []
            query_vector = await self._create_enhanced_vector(query, {})

            # ê²€ìƒ‰ ëŒ€ìƒ ê²°ì •
            search_ids = self._get_search_targets(memory_type)

            for memory_id in search_ids:
                if memory_id in self.memories:
                    memory = self.memories[memory_id]
                    similarity = self._cosine_similarity(query_vector, memory.vector_data)

                    if similarity > 0.3:  # ì„ê³„ê°’
                        results.append((memory, similarity))

            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            results.sort(key=lambda x: x[1], reverse=True)

            # ì ‘ê·¼ íšŸìˆ˜ ì—…ë°ì´íŠ¸
            for memory, _ in results[:limit]:
                memory.accessed_count += 1
                memory.last_accessed = datetime.now()
                self._update_priority_queue(memory.id, memory.priority_score)

            return results[:limit]

        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    async def get_related_memories(
        self,
        memory_id: str,
        association_type: Optional[AssociationType] = None,
        limit: int = 5,
    ) -> List[Tuple[MemoryEntry, float]]:
        """ì—°ê´€ ë©”ëª¨ë¦¬ ê²€ìƒ‰"""
        try:
            if memory_id not in self.memories:
                return []

            related_memories = []
            memory = self.memories[memory_id]  # noqa: F841

            # ì—°ê´€ì„± ë§í¬ ê²€ìƒ‰
            for link_id, link in self.associations.items():
                if (link.source_id == memory_id or link.target_id == memory_id) and (
                    association_type is None or link.association_type == association_type
                ):
                    related_id = link.target_id if link.source_id == memory_id else link.source_id
                    if related_id in self.memories:
                        related_memory = self.memories[related_id]
                        related_memories.append((related_memory, link.strength))

            # ê°•ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            related_memories.sort(key=lambda x: x[1], reverse=True)

            return related_memories[:limit]

        except Exception as e:
            logger.error(f"ì—°ê´€ ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    async def update_memory_importance(self, memory_id: str, new_importance: float):
        """ë©”ëª¨ë¦¬ ì¤‘ìš”ë„ ì—…ë°ì´íŠ¸"""
        try:
            if memory_id in self.memories:
                memory = self.memories[memory_id]
                memory.importance = new_importance
                memory.priority_score = self._calculate_priority_score(
                    new_importance, memory.accessed_count, memory.last_accessed
                )
                self._update_priority_queue(memory_id, memory.priority_score)

                logger.info(f"ë©”ëª¨ë¦¬ ì¤‘ìš”ë„ ì—…ë°ì´íŠ¸: {memory_id} -> {new_importance}")

        except Exception as e:
            logger.error(f"ì¤‘ìš”ë„ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")

    async def get_memory_statistics(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„ ì •ë³´"""
        try:
            stats = {
                "total_memories": len(self.memories),
                "memory_types": {},
                "priority_distribution": {},
                "association_stats": {},
                "access_patterns": {},
            }

            # íƒ€ì…ë³„ í†µê³„
            for memory_type in MemoryType:
                type_memories = [m for m in self.memories.values() if m.memory_type == memory_type]
                stats["memory_types"][memory_type.value] = {
                    "count": len(type_memories),
                    "avg_importance": (np.mean([m.importance for m in type_memories]) if type_memories else 0),
                    "avg_priority": (np.mean([m.priority_score for m in type_memories]) if type_memories else 0),
                }

            # ìš°ì„ ìˆœìœ„ ë¶„í¬
            priorities = [m.priority_score for m in self.memories.values()]
            stats["priority_distribution"] = {
                "min": min(priorities) if priorities else 0,
                "max": max(priorities) if priorities else 0,
                "mean": np.mean(priorities) if priorities else 0,
                "std": np.std(priorities) if priorities else 0,
            }

            # ì—°ê´€ì„± í†µê³„
            association_types = [link.association_type for link in self.associations.values()]
            type_counts = Counter(association_types)
            stats["association_stats"] = {
                "total_links": len(self.associations),
                "type_distribution": {t.value: count for t, count in type_counts.items()},
            }

            # ì ‘ê·¼ íŒ¨í„´
            access_counts = [m.accessed_count for m in self.memories.values()]
            stats["access_patterns"] = {
                "total_accesses": sum(access_counts),
                "avg_accesses": np.mean(access_counts) if access_counts else 0,
                "most_accessed": max(access_counts) if access_counts else 0,
            }

            return stats

        except Exception as e:
            logger.error(f"í†µê³„ ìƒì„± ì˜¤ë¥˜: {e}")
            return {}

    async def cleanup_low_priority_memories(self, threshold: float = 0.3):
        """ë‚®ì€ ìš°ì„ ìˆœìœ„ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            to_delete = []

            for memory_id, memory in self.memories.items():
                if memory.priority_score < threshold and memory.accessed_count < 3:
                    to_delete.append(memory_id)

            for memory_id in to_delete:
                await self._delete_memory(memory_id)

            logger.info(f"ë‚®ì€ ìš°ì„ ìˆœìœ„ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {len(to_delete)}ê°œ ì‚­ì œ")
            return len(to_delete)

        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì˜¤ë¥˜: {e}")
            return 0

    # ë‚´ë¶€ ë©”ì„œë“œë“¤
    def _generate_memory_id(self, content: str, context: Dict[str, Any]) -> str:
        """ê³ ìœ  ë©”ëª¨ë¦¬ ID ìƒì„±"""
        data = f"{content}{json.dumps(context, sort_keys=True)}"
        return hashlib.sha256(data.encode()).hexdigest()[:16]

    async def _classify_memory_type(self, content: str, context: Dict[str, Any]) -> Tuple[MemoryType, float]:
        """ê¸°ì–µ íƒ€ì… ë¶„ë¥˜"""
        try:
            # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
            keywords = {
                MemoryType.EXPERIENCE: ["ê²½í—˜", "ë°œìƒ", "ì¼ì–´ë‚¬ë‹¤", "í–ˆë‹¤", "í–ˆë‹¤"],
                MemoryType.KNOWLEDGE: ["ì•Œë‹¤", "ì´í•´", "í•™ìŠµ", "ì •ë³´", "ì§€ì‹"],
                MemoryType.PATTERN: ["íŒ¨í„´", "ë°˜ë³µ", "ê·œì¹™", "ìŠµê´€", "ê²½í–¥"],
                MemoryType.EMOTION: ["ê°ì •", "ê¸°ë¶„", "ëŠë‚Œ", "í–‰ë³µ", "ìŠ¬í””", "í™”ë‚¨"],
            }

            scores = {}
            for memory_type, type_keywords in keywords.items():
                score = sum(1 for keyword in type_keywords if keyword in content)
                scores[memory_type] = score

            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ë°˜ì˜
            if "emotion" in context:
                scores[MemoryType.EMOTION] += 2

            if "learning" in context:
                scores[MemoryType.KNOWLEDGE] += 2

            # ìµœê³  ì ìˆ˜ ì„ íƒ
            best_type = max(scores, key=scores.get)
            confidence = min(scores[best_type] / 3.0, 1.0)  # ìµœëŒ€ 1.0

            return best_type, confidence

        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ íƒ€ì… ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
            return MemoryType.EXPERIENCE, 0.5

    async def _create_enhanced_vector(self, content: str, context: Dict[str, Any]) -> List[float]:
        """ê³ ë„í™”ëœ ë²¡í„° ìƒì„±"""
        try:
            # ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ë²¡í„° ìƒì„± (ì‹¤ì œë¡œëŠ” ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©)
            vector = []
            for i in range(self.vector_dim):
                # ë‚´ìš©ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¡°í•©í•œ í•´ì‹œ
                data = f"{content}{json.dumps(context)}{i}"
                hash_val = int(hashlib.md5(data.encode()).hexdigest(), 16)
                vector.append((hash_val % 1000) / 1000.0)  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”

            return vector

        except Exception as e:
            logger.error(f"ë²¡í„° ìƒì„± ì˜¤ë¥˜: {e}")
            return [0.0] * self.vector_dim

    async def _extract_enhanced_tags(self, content: str, context: Dict[str, Any]) -> List[str]:
        """ê³ ë„í™”ëœ íƒœê·¸ ì¶”ì¶œ"""
        try:
            tags = []

            # ê¸°ë³¸ íƒœê·¸
            if "emotion" in context:
                tags.append("ê°ì •")
            if "learning" in context:
                tags.append("í•™ìŠµ")
            if "important" in context:
                tags.append("ì¤‘ìš”")

            # ë‚´ìš© ê¸°ë°˜ íƒœê·¸
            content_words = content.split()
            if len(content_words) > 3:
                tags.extend(content_words[:3])

            return list(set(tags))  # ì¤‘ë³µ ì œê±°

        except Exception as e:
            logger.error(f"íƒœê·¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []

    def _calculate_priority_score(self, importance: float, access_count: int, last_accessed: datetime) -> float:
        """ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ì‹œê°„ ê°€ì¤‘ì¹˜ (ìµœê·¼ì¼ìˆ˜ë¡ ë†’ìŒ)
            time_diff = datetime.now() - last_accessed
            time_weight = max(0, 1 - (time_diff.total_seconds() / (24 * 3600)))  # 24ì‹œê°„ ê¸°ì¤€

            # ì ‘ê·¼ ë¹ˆë„ ê°€ì¤‘ì¹˜
            access_weight = min(1.0, access_count / 10.0)  # ìµœëŒ€ 10íšŒ ê¸°ì¤€

            # ì¢…í•© ì ìˆ˜
            priority_score = (
                self.importance_weight * importance
                + self.access_weight * access_weight
                + self.recency_weight * time_weight
            )

            return min(1.0, priority_score)

        except Exception as e:
            logger.error(f"ìš°ì„ ìˆœìœ„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return importance

    def _calculate_retention_score(self, content: str, memory_type: MemoryType, importance: float) -> float:
        """ë³´ì¡´ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê¸°ë³¸ ì ìˆ˜
            base_score = importance

            # íƒ€ì…ë³„ ê°€ì¤‘ì¹˜
            type_weights = {
                MemoryType.EXPERIENCE: 0.8,
                MemoryType.KNOWLEDGE: 0.9,
                MemoryType.PATTERN: 0.7,
                MemoryType.EMOTION: 0.6,
            }

            type_weight = type_weights.get(memory_type, 0.7)

            # ë‚´ìš© ê¸¸ì´ ê°€ì¤‘ì¹˜
            length_weight = min(1.0, len(content) / 100.0)

            retention_score = base_score * type_weight * (0.7 + 0.3 * length_weight)

            return min(1.0, retention_score)

        except Exception as e:
            logger.error(f"ë³´ì¡´ ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return importance

    def _update_priority_queue(self, memory_id: str, priority_score: float):
        """ìš°ì„ ìˆœìœ„ í ì—…ë°ì´íŠ¸"""
        try:
            # ê¸°ì¡´ í•­ëª© ì œê±°
            if memory_id in self.priority_queue:
                self.priority_queue.remove(memory_id)

            # ìƒˆ ìœ„ì¹˜ì— ì‚½ì…
            insert_index = 0
            for i, existing_id in enumerate(self.priority_queue):
                if existing_id in self.memories:
                    existing_priority = self.memories[existing_id].priority_score
                    if priority_score > existing_priority:
                        insert_index = i
                        break
                    insert_index = i + 1
                else:
                    insert_index = i

            self.priority_queue.insert(insert_index, memory_id)

        except Exception as e:
            logger.error(f"ìš°ì„ ìˆœìœ„ í ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")

    def _get_search_targets(self, memory_type: Optional[MemoryType]) -> List[str]:
        """ê²€ìƒ‰ ëŒ€ìƒ ê²°ì •"""
        if memory_type:
            return self.memory_type_index.get(memory_type, [])
        else:
            return list(self.memories.keys())

    async def _analyze_and_link_associations(self, memory_id: str):
        """ì—°ê´€ì„± ë¶„ì„ ë° ì—°ê²°"""
        try:
            if memory_id not in self.memories:
                return

            new_memory = self.memories[memory_id]

            for existing_id, existing_memory in self.memories.items():
                if existing_id == memory_id:
                    continue

                # ì˜ë¯¸ì  ì—°ê´€ì„±
                semantic_similarity = self._cosine_similarity(new_memory.vector_data, existing_memory.vector_data)

                if semantic_similarity > self.association_threshold:
                    await self._create_association_link(
                        memory_id,
                        existing_id,
                        AssociationType.SEMANTIC,
                        semantic_similarity,
                    )

                # ì‹œê°„ì  ì—°ê´€ì„±
                time_diff = abs((new_memory.created_at - existing_memory.created_at).total_seconds())
                if time_diff < 3600:  # 1ì‹œê°„ ì´ë‚´
                    temporal_strength = max(0, 1 - (time_diff / 3600))
                    if temporal_strength > self.association_threshold:
                        await self._create_association_link(
                            memory_id,
                            existing_id,
                            AssociationType.TEMPORAL,
                            temporal_strength,
                        )

                # ê°ì •ì  ì—°ê´€ì„± (ê°ì • ë©”ëª¨ë¦¬ì¸ ê²½ìš°)
                if new_memory.memory_type == MemoryType.EMOTION and existing_memory.memory_type == MemoryType.EMOTION:
                    emotion_similarity = self._cosine_similarity(new_memory.vector_data, existing_memory.vector_data)
                    if emotion_similarity > self.association_threshold:
                        await self._create_association_link(
                            memory_id,
                            existing_id,
                            AssociationType.EMOTIONAL,
                            emotion_similarity,
                        )

        except Exception as e:
            logger.error(f"ì—°ê´€ì„± ë¶„ì„ ì˜¤ë¥˜: {e}")

    async def _create_association_link(
        self,
        source_id: str,
        target_id: str,
        association_type: AssociationType,
        strength: float,
    ):
        """ì—°ê´€ì„± ë§í¬ ìƒì„±"""
        try:
            link_id = f"{source_id}_{target_id}_{association_type.value}"

            link = AssociationLink(
                source_id=source_id,
                target_id=target_id,
                association_type=association_type,
                strength=strength,
                created_at=datetime.now(),
                metadata={},
            )

            self.associations[link_id] = link

            # ë©”ëª¨ë¦¬ì— ì—°ê´€ ì •ë³´ ì¶”ê°€
            if source_id in self.memories:
                self.memories[source_id].associations.append(target_id)
                self.memories[source_id].association_strength[target_id] = strength

            if target_id in self.memories:
                self.memories[target_id].associations.append(source_id)
                self.memories[target_id].association_strength[source_id] = strength

        except Exception as e:
            logger.error(f"ì—°ê´€ì„± ë§í¬ ìƒì„± ì˜¤ë¥˜: {e}")

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            if len(vec1) != len(vec2):
                return 0.0

            dot_product = sum(a * b for a, b in zip(vec1, vec2))  # noqa: B905
            norm1 = math.sqrt(sum(a * a for a in vec1))
            norm2 = math.sqrt(sum(a * a for a in vec2))

            if norm1 == 0 or norm2 == 0:
                return 0.0

            return dot_product / (norm1 * norm2)

        except Exception as e:
            logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0

    async def _delete_memory(self, memory_id: str):
        """ë©”ëª¨ë¦¬ ì‚­ì œ"""
        try:
            if memory_id in self.memories:
                # ì—°ê´€ì„± ë§í¬ ì œê±°
                links_to_remove = []
                for link_id, link in self.associations.items():
                    if link.source_id == memory_id or link.target_id == memory_id:
                        links_to_remove.append(link_id)

                for link_id in links_to_remove:
                    del self.associations[link_id]

                # íƒ€ì… ì¸ë±ìŠ¤ì—ì„œ ì œê±°
                memory = self.memories[memory_id]
                if memory.memory_type in self.memory_type_index:
                    if memory_id in self.memory_type_index[memory.memory_type]:
                        self.memory_type_index[memory.memory_type].remove(memory_id)

                # ìš°ì„ ìˆœìœ„ íì—ì„œ ì œê±°
                if memory_id in self.priority_queue:
                    self.priority_queue.remove(memory_id)

                # ë©”ëª¨ë¦¬ ì‚­ì œ
                del self.memories[memory_id]

                logger.info(f"ë©”ëª¨ë¦¬ ì‚­ì œ ì™„ë£Œ: {memory_id}")

        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ì‚­ì œ ì˜¤ë¥˜: {e}")

    # Phase 6.2.2 - Working Memory ì—°ì‚° ê¸°ëŠ¥ë“¤

    async def perform_memory_operation(self, operation_type: str, memory_ids: List[str]) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì—°ì‚° ìˆ˜í–‰"""
        try:
            start_time = time.time()  # noqa: F821

            # ì…ë ¥ ë©”ëª¨ë¦¬ ê²€ì¦
            input_memories = []
            for memory_id in memory_ids:
                if memory_id in self.memories:
                    input_memories.append(self.memories[memory_id])
                else:
                    return {"success": False, "error": f"Memory {memory_id} not found"}

            if len(input_memories) < 2:
                return {
                    "success": False,
                    "error": "At least 2 memories required for operation",
                }

            # ì—°ì‚° ìˆ˜í–‰
            operation_result = await self._execute_memory_operation(operation_type, input_memories)

            # ê²°ê³¼ë¥¼ ì‘ì—… ê¸°ì–µ ë²„í¼ì— ì €ì¥
            buffer_id = await self._store_in_working_memory(operation_type, memory_ids, operation_result)

            # ì—°ì‚° ê¸°ë¡
            execution_time = time.time() - start_time  # noqa: F821
            operation_record = MemoryOperation(
                operation_id=f"op_{int(time.time() * 1000)}",  # noqa: F821
                operation_type=operation_type,
                input_memories=memory_ids,
                output_memory=buffer_id,
                operation_result=operation_result["result"],
                confidence=operation_result["confidence"],
                execution_time=execution_time,
                created_at=datetime.now(),
            )

            self.memory_operations[operation_record.operation_id] = operation_record
            self.operation_history.append(operation_record)

            logger.info(f"ğŸ§  ë©”ëª¨ë¦¬ ì—°ì‚° ì™„ë£Œ: {operation_type} (ì‹ ë¢°ë„: {operation_result['confidence']:.3f})")

            return {
                "success": True,
                "operation_id": operation_record.operation_id,
                "buffer_id": buffer_id,
                "result": operation_result["result"],
                "confidence": operation_result["confidence"],
                "execution_time": execution_time,
            }

        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ì—°ì‚° ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    async def _execute_memory_operation(self, operation_type: str, memories: List[MemoryEntry]) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì—°ì‚° ì‹¤í–‰"""
        if operation_type == "addition":
            return await self._add_memories(memories)
        elif operation_type == "subtraction":
            return await self._subtract_memories(memories)
        elif operation_type == "comparison":
            return await self._compare_memories(memories)
        elif operation_type == "integration":
            return await self._integrate_memories(memories)
        else:
            return {"result": "Unknown operation", "confidence": 0.0}

    async def _add_memories(self, memories: List[MemoryEntry]) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì¶”ê°€ ì—°ì‚°"""
        # ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ë©”ëª¨ë¦¬ í†µí•©
        combined_content = " ".join([m.content for m in memories])
        combined_tags = list(set([tag for m in memories for tag in m.tags]))  # noqa: F841

        # ì‹ ë¢°ë„ ê³„ì‚° (í‰ê·  + ìœ ì‚¬ì„± ë³´ë„ˆìŠ¤)
        avg_confidence = sum(m.classification_confidence for m in memories) / len(memories)
        similarity_bonus = self._calculate_memory_similarity(memories) * 0.1

        confidence = min(1.0, avg_confidence + similarity_bonus)

        return {"result": f"í†µí•©ëœ ì •ë³´: {combined_content}", "confidence": confidence}

    async def _subtract_memories(self, memories: List[MemoryEntry]) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì°¨ê° ì—°ì‚°"""
        if len(memories) < 2:
            return {
                "result": "Insufficient memories for subtraction",
                "confidence": 0.0,
            }

        # ì²« ë²ˆì§¸ ë©”ëª¨ë¦¬ì—ì„œ ë‚˜ë¨¸ì§€ ë©”ëª¨ë¦¬ë“¤ì˜ ê³µí†µ ìš”ì†Œ ì œê±°
        base_memory = memories[0]
        other_memories = memories[1:]

        # ê³µí†µ íƒœê·¸ ì°¾ê¸°
        common_tags = set(base_memory.tags)
        for memory in other_memories:
            common_tags = common_tags.intersection(set(memory.tags))

        # ì°¨ê°ëœ ê²°ê³¼
        remaining_tags = set(base_memory.tags) - common_tags  # noqa: F841
        result_content = f"ì°¨ê°ëœ ì •ë³´: {base_memory.content} (ì œì™¸: {', '.join(common_tags)})"

        confidence = base_memory.classification_confidence * 0.8

        return {"result": result_content, "confidence": confidence}

    async def _compare_memories(self, memories: List[MemoryEntry]) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ë¹„êµ ì—°ì‚°"""
        if len(memories) < 2:
            return {"result": "Insufficient memories for comparison", "confidence": 0.0}

        # ë©”ëª¨ë¦¬ ê°„ ìœ ì‚¬ì„± ë¶„ì„
        similarities = []
        for i in range(len(memories)):
            for j in range(i + 1, len(memories)):
                similarity = self._cosine_similarity(memories[i].vector_data, memories[j].vector_data)
                similarities.append(similarity)

        avg_similarity = sum(similarities) / len(similarities) if similarities else 0.0

        # ë¹„êµ ê²°ê³¼ ìƒì„±
        if avg_similarity > 0.7:
            comparison_result = "ë†’ì€ ìœ ì‚¬ì„±"
        elif avg_similarity > 0.4:
            comparison_result = "ì¤‘ê°„ ìœ ì‚¬ì„±"
        else:
            comparison_result = "ë‚®ì€ ìœ ì‚¬ì„±"

        result_content = f"ë¹„êµ ê²°ê³¼: {comparison_result} (í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.3f})"

        return {"result": result_content, "confidence": avg_similarity}

    async def _integrate_memories(self, memories: List[MemoryEntry]) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µí•© ì—°ì‚°"""
        # ë©”ëª¨ë¦¬ë“¤ì„ ì˜ë¯¸ì ìœ¼ë¡œ í†µí•©
        integrated_content = "í†µí•©ëœ ì§€ì‹: "
        integrated_parts = []

        for memory in memories:
            integrated_parts.append(f"{memory.content}")

        integrated_content += " + ".join(integrated_parts)

        # í†µí•© ì‹ ë¢°ë„ ê³„ì‚°
        avg_confidence = sum(m.classification_confidence for m in memories) / len(memories)
        integration_bonus = 0.1  # í†µí•© ìì²´ì˜ ê°€ì¹˜

        confidence = min(1.0, avg_confidence + integration_bonus)

        return {"result": integrated_content, "confidence": confidence}

    async def _store_in_working_memory(
        self,
        operation_type: str,
        memory_ids: List[str],
        operation_result: Dict[str, Any],
    ) -> str:
        """ì‘ì—… ê¸°ì–µ ë²„í¼ì— ì €ì¥"""
        buffer_id = f"wm_{int(time.time() * 1000)}"  # noqa: F821

        # ì‘ì—… ê¸°ì–µ ìš©ëŸ‰ í™•ì¸
        if len(self.working_memory_buffers) >= self.working_memory_capacity:
            # ê°€ì¥ ì˜¤ë˜ëœ ë²„í¼ ì œê±°
            oldest_buffer = min(self.working_memory_buffers.values(), key=lambda x: x.created_at)
            del self.working_memory_buffers[oldest_buffer.id]

        # ìƒˆ ë²„í¼ ìƒì„±
        buffer = WorkingMemoryBuffer(
            id=buffer_id,
            content=operation_result["result"],
            operation_type=operation_type,
            operands=memory_ids,
            result=operation_result["result"],
            confidence=operation_result["confidence"],
            created_at=datetime.now(),
            expires_at=datetime.now() + timedelta(seconds=self.working_memory_ttl),
        )

        self.working_memory_buffers[buffer_id] = buffer

        logger.info(f"ğŸ’¾ ì‘ì—… ê¸°ì–µ ë²„í¼ ì €ì¥: {buffer_id}")
        return buffer_id

    def _calculate_memory_similarity(self, memories: List[MemoryEntry]) -> float:
        """ë©”ëª¨ë¦¬ ê°„ ìœ ì‚¬ì„± ê³„ì‚°"""
        if len(memories) < 2:
            return 0.0

        similarities = []
        for i in range(len(memories)):
            for j in range(i + 1, len(memories)):
                similarity = self._cosine_similarity(memories[i].vector_data, memories[j].vector_data)
                similarities.append(similarity)

        return sum(similarities) / len(similarities) if similarities else 0.0

    async def get_working_memory_status(self) -> Dict[str, Any]:
        """ì‘ì—… ê¸°ì–µ ìƒíƒœ ì¡°íšŒ"""
        # ë§Œë£Œëœ ë²„í¼ ì •ë¦¬
        current_time = datetime.now()
        expired_buffers = [
            buffer_id for buffer_id, buffer in self.working_memory_buffers.items() if current_time > buffer.expires_at
        ]

        for buffer_id in expired_buffers:
            del self.working_memory_buffers[buffer_id]

        return {
            "total_buffers": len(self.working_memory_buffers),
            "capacity": self.working_memory_capacity,
            "utilization": len(self.working_memory_buffers) / self.working_memory_capacity,
            "buffers": [
                {
                    "id": buffer.id,
                    "content": buffer.content,
                    "operation_type": buffer.operation_type,
                    "confidence": buffer.confidence,
                    "expires_at": buffer.expires_at.isoformat(),
                    "access_count": buffer.access_count,
                }
                for buffer in self.working_memory_buffers.values()
            ],
            "operation_history_count": len(self.operation_history),
        }

    async def access_working_memory(self, buffer_id: str) -> Dict[str, Any]:
        """ì‘ì—… ê¸°ì–µ ë²„í¼ ì ‘ê·¼"""
        if buffer_id not in self.working_memory_buffers:
            return {"success": False, "error": "Buffer not found"}

        buffer = self.working_memory_buffers[buffer_id]

        # ë§Œë£Œ í™•ì¸
        if datetime.now() > buffer.expires_at:
            del self.working_memory_buffers[buffer_id]
            return {"success": False, "error": "Buffer expired"}

        # ì ‘ê·¼ íšŸìˆ˜ ì¦ê°€
        buffer.access_count += 1

        return {
            "success": True,
            "buffer": {
                "id": buffer.id,
                "content": buffer.content,
                "operation_type": buffer.operation_type,
                "confidence": buffer.confidence,
                "access_count": buffer.access_count,
            },
        }

    # Phase 6.2.6 - ì‹œë§¨í‹± ì§€ì‹ ì—°ê²°ë§ ê¸°ëŠ¥ë“¤

    async def add_semantic_concept(
        self,
        name: str,
        concept_type: ConceptType,
        description: str = "",
        properties: Dict[str, Any] = None,
        confidence: float = 0.8,
    ) -> str:
        """ì‹œë§¨í‹± ê°œë… ì¶”ê°€"""
        try:
            concept_id = await self.semantic_graph.add_concept(name, concept_type, description, properties, confidence)
            logger.info(f"ì‹œë§¨í‹± ê°œë… ì¶”ê°€: {name}")
            return concept_id
        except Exception as e:
            logger.error(f"ì‹œë§¨í‹± ê°œë… ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return None

    async def add_semantic_inference(
        self,
        source_name: str,
        target_name: str,
        inference_type: InferenceType,
        confidence: float = 0.7,
        evidence: List[str] = None,
    ) -> str:
        """ì‹œë§¨í‹± ì¶”ë¡  ì¶”ê°€"""
        try:
            edge_id = await self.semantic_graph.add_inference(
                source_name, target_name, inference_type, confidence, evidence
            )
            logger.info(f"ì‹œë§¨í‹± ì¶”ë¡  ì¶”ê°€: {source_name} -> {target_name}")
            return edge_id
        except Exception as e:
            logger.error(f"ì‹œë§¨í‹± ì¶”ë¡  ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return None

    async def find_semantic_path(
        self, source_name: str, target_name: str, max_length: int = 5
    ) -> Optional[Dict[str, Any]]:
        """ì‹œë§¨í‹± ê²½ë¡œ ì°¾ê¸°"""
        try:
            path = await self.semantic_graph.find_semantic_path(source_name, target_name, max_length)
            if path:
                return {
                    "success": True,
                    "path": {
                        "source": path.source_concept,
                        "target": path.target_concept,
                        "path_length": path.path_length,
                        "total_confidence": path.total_confidence,
                        "inference_chain": [inference.value for inference in path.inference_chain],
                    },
                }
            else:
                return {"success": False, "error": "ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"}
        except Exception as e:
            logger.error(f"ì‹œë§¨í‹± ê²½ë¡œ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    async def infer_semantic_knowledge(
        self, concept_name: str, inference_type: InferenceType = None
    ) -> List[Dict[str, Any]]:
        """ì‹œë§¨í‹± ì§€ì‹ ì¶”ë¡ """
        try:
            inferences = await self.semantic_graph.infer_new_knowledge(concept_name, inference_type)
            logger.info(f"ì‹œë§¨í‹± ì§€ì‹ ì¶”ë¡  ì™„ë£Œ: {concept_name} -> {len(inferences)}ê°œ ì¶”ë¡ ")
            return inferences
        except Exception as e:
            logger.error(f"ì‹œë§¨í‹± ì§€ì‹ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return []

    async def analyze_semantic_similarity(self, concept1_name: str, concept2_name: str) -> float:
        """ì‹œë§¨í‹± ìœ ì‚¬ë„ ë¶„ì„"""
        try:
            similarity = await self.semantic_graph.analyze_semantic_similarity(concept1_name, concept2_name)
            logger.info(f"ì‹œë§¨í‹± ìœ ì‚¬ë„ ë¶„ì„: {concept1_name} vs {concept2_name} = {similarity:.3f}")
            return similarity
        except Exception as e:
            logger.error(f"ì‹œë§¨í‹± ìœ ì‚¬ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.0

    async def get_semantic_graph_status(self) -> Dict[str, Any]:
        """ì‹œë§¨í‹± ê·¸ë˜í”„ ìƒíƒœ ë°˜í™˜"""
        try:
            status = await self.semantic_graph.get_knowledge_graph_status()
            return {
                "success": status.success,
                "concept_count": status.concept_count,
                "edge_count": status.edge_count,
                "average_confidence": status.average_confidence,
                "graph_density": status.graph_density,
                "connected_components": status.connected_components,
                "created_at": status.created_at,
            }
        except Exception as e:
            logger.error(f"ì‹œë§¨í‹± ê·¸ë˜í”„ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    async def optimize_semantic_graph(self) -> Dict[str, Any]:
        """ì‹œë§¨í‹± ê·¸ë˜í”„ ìµœì í™”"""
        try:
            optimization_results = await self.semantic_graph.semantic_optimizer.optimize_graph()
            logger.info(f"ì‹œë§¨í‹± ê·¸ë˜í”„ ìµœì í™” ì™„ë£Œ: {optimization_results}")
            return optimization_results
        except Exception as e:
            logger.error(f"ì‹œë§¨í‹± ê·¸ë˜í”„ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {}


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_enhanced_memory_system():
    """ê³ ë„í™”ëœ ê¸°ì–µ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("=== DuRiCore Phase 5 Day 2 - ê³ ë„í™”ëœ ê¸°ì–µ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")

    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    memory_system = EnhancedMemorySystem()

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_memories = [
        {
            "content": "ì˜¤ëŠ˜ ìƒˆë¡œìš´ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ í•™ìŠµí–ˆë‹¤. ë§¤ìš° í¥ë¯¸ë¡œì› ë‹¤.",
            "context": {"type": "learning", "emotion": "excited", "importance": "high"},
            "importance": 0.9,
        },
        {
            "content": "ì¹œêµ¬ì™€ í•¨ê»˜ ì˜í™”ë¥¼ ë´¤ë‹¤. ì •ë§ ì¬ë¯¸ìˆì—ˆë‹¤.",
            "context": {
                "type": "experience",
                "emotion": "happy",
                "importance": "medium",
            },
            "importance": 0.7,
        },
        {
            "content": "ì½”ë”©í•  ë•Œ í•­ìƒ ê°™ì€ íŒ¨í„´ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤.",
            "context": {
                "type": "pattern",
                "emotion": "neutral",
                "importance": "medium",
            },
            "importance": 0.6,
        },
        {
            "content": "ì‹œí—˜ ê²°ê³¼ê°€ ë‚˜ì™”ëŠ”ë° ì‹¤ë§ìŠ¤ëŸ½ë‹¤.",
            "context": {
                "type": "emotion",
                "emotion": "disappointed",
                "importance": "high",
            },
            "importance": 0.8,
        },
    ]

    # ë©”ëª¨ë¦¬ ì €ì¥ í…ŒìŠ¤íŠ¸
    print("\n1. ë©”ëª¨ë¦¬ ì €ì¥ í…ŒìŠ¤íŠ¸")
    memory_ids = []
    for i, test_memory in enumerate(test_memories):
        memory_id = await memory_system.store_memory(
            test_memory["content"], test_memory["context"], test_memory["importance"]
        )
        memory_ids.append(memory_id)
        print(f"ë©”ëª¨ë¦¬ {i+1} ì €ì¥: {memory_id}")

    # ë©”ëª¨ë¦¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\n2. ë©”ëª¨ë¦¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    search_results = await memory_system.search_memories("í•™ìŠµ", limit=3)
    print(f"'í•™ìŠµ' ê´€ë ¨ ë©”ëª¨ë¦¬ ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ")
    for memory, similarity in search_results:
        print(f"  - {memory.content[:50]}... (ìœ ì‚¬ë„: {similarity:.3f})")

    # ì—°ê´€ ë©”ëª¨ë¦¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\n3. ì—°ê´€ ë©”ëª¨ë¦¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    if memory_ids:
        related_memories = await memory_system.get_related_memories(memory_ids[0], limit=3)
        print(f"ì²« ë²ˆì§¸ ë©”ëª¨ë¦¬ì˜ ì—°ê´€ ë©”ëª¨ë¦¬: {len(related_memories)}ê°œ")
        for memory, strength in related_memories:
            print(f"  - {memory.content[:50]}... (ê°•ë„: {strength:.3f})")

    # í†µê³„ ì •ë³´ í…ŒìŠ¤íŠ¸
    print("\n4. í†µê³„ ì •ë³´ í…ŒìŠ¤íŠ¸")
    stats = await memory_system.get_memory_statistics()
    print(f"ì´ ë©”ëª¨ë¦¬ ìˆ˜: {stats.get('total_memories', 0)}")
    print(f"íƒ€ì…ë³„ ë¶„í¬: {stats.get('memory_types', {})}")

    # ìš°ì„ ìˆœìœ„ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸
    print("\n5. ìš°ì„ ìˆœìœ„ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸")
    if memory_ids:
        await memory_system.update_memory_importance(memory_ids[0], 0.95)
        updated_memory = memory_system.memories.get(memory_ids[0])
        if updated_memory:
            print(f"ìš°ì„ ìˆœìœ„ ì—…ë°ì´íŠ¸: {updated_memory.priority_score:.3f}")

    # Phase 6.2.2 - Working Memory ì—°ì‚° í…ŒìŠ¤íŠ¸
    print("\n6. Working Memory ì—°ì‚° í…ŒìŠ¤íŠ¸")
    if len(memory_ids) >= 2:
        # ë©”ëª¨ë¦¬ ì¶”ê°€ ì—°ì‚°
        add_result = await memory_system.perform_memory_operation("addition", memory_ids[:2])
        print(f"ë©”ëª¨ë¦¬ ì¶”ê°€ ì—°ì‚°: {add_result.get('success', False)}")
        if add_result.get("success"):
            print(f"  ê²°ê³¼: {add_result.get('result', '')[:50]}...")
            print(f"  ì‹ ë¢°ë„: {add_result.get('confidence', 0):.3f}")

        # ë©”ëª¨ë¦¬ ë¹„êµ ì—°ì‚°
        compare_result = await memory_system.perform_memory_operation("comparison", memory_ids[:2])
        print(f"ë©”ëª¨ë¦¬ ë¹„êµ ì—°ì‚°: {compare_result.get('success', False)}")
        if compare_result.get("success"):
            print(f"  ê²°ê³¼: {compare_result.get('result', '')[:50]}...")
            print(f"  ì‹ ë¢°ë„: {compare_result.get('confidence', 0):.3f}")

        # ë©”ëª¨ë¦¬ í†µí•© ì—°ì‚°
        integrate_result = await memory_system.perform_memory_operation("integration", memory_ids)
        print(f"ë©”ëª¨ë¦¬ í†µí•© ì—°ì‚°: {integrate_result.get('success', False)}")
        if integrate_result.get("success"):
            print(f"  ê²°ê³¼: {integrate_result.get('result', '')[:50]}...")
            print(f"  ì‹ ë¢°ë„: {integrate_result.get('confidence', 0):.3f}")

    # Working Memory ìƒíƒœ ì¡°íšŒ
    print("\n7. Working Memory ìƒíƒœ ì¡°íšŒ")
    wm_status = await memory_system.get_working_memory_status()
    print(f"ì‘ì—… ê¸°ì–µ ë²„í¼ ìˆ˜: {wm_status.get('total_buffers', 0)}")
    print(f"ìš©ëŸ‰: {wm_status.get('capacity', 0)}")
    print(f"ì‚¬ìš©ë¥ : {wm_status.get('utilization', 0):.1%}")
    print(f"ì—°ì‚° íˆìŠ¤í† ë¦¬ ìˆ˜: {wm_status.get('operation_history_count', 0)}")

    # Working Memory ë²„í¼ ì ‘ê·¼ í…ŒìŠ¤íŠ¸
    print("\n8. Working Memory ë²„í¼ ì ‘ê·¼ í…ŒìŠ¤íŠ¸")
    if wm_status.get("buffers"):
        buffer_id = wm_status["buffers"][0]["id"]
        access_result = await memory_system.access_working_memory(buffer_id)
        print(f"ë²„í¼ ì ‘ê·¼: {access_result.get('success', False)}")
        if access_result.get("success"):
            buffer_info = access_result.get("buffer", {})
            print(f"  ë‚´ìš©: {buffer_info.get('content', '')[:50]}...")
            print(f"  ì—°ì‚° ìœ í˜•: {buffer_info.get('operation_type', '')}")
            print(f"  ì ‘ê·¼ íšŸìˆ˜: {buffer_info.get('access_count', 0)}")

    print("\n=== Phase 6.2.2 Working Memory ì—°ì‚° í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")


if __name__ == "__main__":
    asyncio.run(test_enhanced_memory_system())
