#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DuRi Integrated Evolution System - Enhanced Version with Performance Optimization

ì´ ëª¨ë“ˆì€ DuRiì˜ ëª¨ë“  ì§„í™” ì‹œìŠ¤í…œë“¤ì„ í†µí•©í•˜ëŠ” ë©”ì¸ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
Phase Z(ìê°€ ë°˜ì„±), Phase Î©(ìƒì¡´ ë³¸ëŠ¥), Self-Rewriting, Genetic Evolution, MetaCoderë¥¼ í†µí•©í•˜ì—¬
ìê·¹-ì§„í™”-ìˆ˜ì • ë£¨í”„ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ìê·¹ ê¸°ë°˜ ì§„í™” íŠ¸ë¦¬ê±°
- í†µí•© ì§„í™” ë£¨í”„ ê´€ë¦¬
- ìê°€ ìˆ˜ì • ë° êµ¬ì¡° ì§„í™”
- ì„±ëŠ¥ í‰ê°€ ë° ë°˜ì„±
- ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™” (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)
- ìºì‹± ì‹œìŠ¤í…œ (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)
- ë¡œë“œ ë°¸ëŸ°ì‹± (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)
- í•™ìŠµ ê¸°ë°˜ ì§„í™”
- ì ì‘í˜• íŠ¸ë¦¬ê±°
- ë¶„ì‚° ì§„í™”
"""

import asyncio
import hashlib
import json
import logging
import multiprocessing as mp
import time
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import numpy as np
# ê¸°ì¡´ ì‹œìŠ¤í…œë“¤ import
from duri_thought_flow import DuRiThoughtFlow, ThoughtFlowResult

# Phase Î© ì‹œìŠ¤í…œ import (ìˆœí™˜ import ë°©ì§€ë¥¼ ìœ„í•´ ë™ì  import ì‹œìŠ¤í…œ êµ¬í˜„)
# í•„ìš”í•  ë•Œë§Œ ë™ì ìœ¼ë¡œ importí•˜ì—¬ ìˆœí™˜ ì°¸ì¡° ë°©ì§€
PHASE_OMEGA_AVAILABLE = False
PHASE_OMEGA_MODULE = None


def _get_phase_omega_system():
    """Phase Î© ì‹œìŠ¤í…œì„ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    global PHASE_OMEGA_AVAILABLE, PHASE_OMEGA_MODULE

    if PHASE_OMEGA_AVAILABLE:
        return PHASE_OMEGA_MODULE

    try:
        import importlib

        PHASE_OMEGA_MODULE = importlib.import_module("phase_omega_integration")
        PHASE_OMEGA_AVAILABLE = True
        logger.info("Phase Î© ì‹œìŠ¤í…œ ë™ì  import ì„±ê³µ")
        return PHASE_OMEGA_MODULE
    except ImportError as e:
        logger.warning(f"Phase Î© ì‹œìŠ¤í…œ ë™ì  import ì‹¤íŒ¨: {e}")
        return None


from genetic_evolution_engine import (EvolutionResult, GeneticEvolutionEngine,
                                      GeneticIndividual)
from meta_coder import CodeAnalysis, MetaCoder, RefactorProposal
from self_rewriting_module import (CodeAssessment, RewriteProposal,
                                   SelfRewritingModule)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class EvolutionTrigger(Enum):
    """ì§„í™” íŠ¸ë¦¬ê±° ì—´ê±°í˜•"""

    REFLECTION_SCORE_LOW = "reflection_score_low"
    SURVIVAL_THREAT = "survival_threat"
    PERFORMANCE_DEGRADATION = "performance_degradation"
    GOAL_MISALIGNMENT = "goal_misalignment"
    EXTERNAL_STIMULUS = "external_stimulus"
    SELF_IMPROVEMENT_OPPORTUNITY = "self_improvement_opportunity"
    LEARNING_BASED_EVOLUTION = "learning_based_evolution"  # ìƒˆë¡œ ì¶”ê°€
    ADAPTIVE_TRIGGER = "adaptive_trigger"  # ìƒˆë¡œ ì¶”ê°€


class EvolutionPhase(Enum):
    """ì§„í™” ë‹¨ê³„ ì—´ê±°í˜•"""

    STIMULUS_DETECTION = "stimulus_detection"
    REFLECTION_ANALYSIS = "reflection_analysis"
    GOAL_GENERATION = "goal_generation"
    EVOLUTION_EXECUTION = "evolution_execution"
    SELF_MODIFICATION = "self_modification"
    VALIDATION = "validation"
    INTEGRATION = "integration"
    LEARNING_ANALYSIS = "learning_analysis"  # ìƒˆë¡œ ì¶”ê°€
    ADAPTIVE_OPTIMIZATION = "adaptive_optimization"  # ìƒˆë¡œ ì¶”ê°€


class TaskPriority(Enum):
    """ì‘ì—… ìš°ì„ ìˆœìœ„ (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)"""

    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4


class TaskStatus(Enum):
    """ì‘ì—… ìƒíƒœ (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)"""

    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


@dataclass
class ParallelTask:
    """ë³‘ë ¬ ì‘ì—… ì •ë³´ (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)"""

    id: str
    name: str
    function: Any
    args: tuple = ()
    kwargs: dict = None
    priority: TaskPriority = TaskPriority.MEDIUM
    status: TaskStatus = TaskStatus.PENDING
    created_at: datetime = None
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    result: Any = None
    error: Optional[str] = None
    execution_time: float = 0.0

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()
        if self.kwargs is None:
            self.kwargs = {}


@dataclass
class StimulusEvent:
    """ìê·¹ ì´ë²¤íŠ¸ ë°ì´í„° í´ë˜ìŠ¤"""

    event_id: str
    trigger_type: EvolutionTrigger
    input_data: Dict[str, Any]
    context: Dict[str, Any]
    timestamp: datetime = field(default_factory=datetime.now)
    intensity: float = 0.0
    description: str = ""
    learning_pattern: Optional[Dict[str, Any]] = None  # ìƒˆë¡œ ì¶”ê°€


@dataclass
class EvolutionSession:
    """ì§„í™” ì„¸ì…˜ ë°ì´í„° í´ë˜ìŠ¤"""

    session_id: str
    stimulus_event: StimulusEvent
    phases: List[EvolutionPhase] = field(default_factory=list)
    results: Dict[str, Any] = field(default_factory=dict)
    start_time: datetime = field(default_factory=datetime.now)
    end_time: Optional[datetime] = None
    success: bool = False
    error_message: Optional[str] = None
    performance_metrics: Dict[str, float] = field(default_factory=dict)  # ìƒˆë¡œ ì¶”ê°€


@dataclass
class IntegratedEvolutionResult:
    """í†µí•© ì§„í™” ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""

    session_id: str
    stimulus_event: StimulusEvent
    thought_flow_result: Optional[ThoughtFlowResult] = None
    phase_omega_result: Optional[Any] = None  # Phase Omega ê²°ê³¼ë¥¼ Anyë¡œ ë³€ê²½
    self_rewriting_result: Optional[Any] = None
    genetic_evolution_result: Optional[EvolutionResult] = None
    meta_coding_result: Optional[Any] = None
    overall_improvement_score: float = 0.0
    evolution_time: float = 0.0
    success: bool = True
    error_message: Optional[str] = None
    learning_insights: List[str] = field(default_factory=list)  # ìƒˆë¡œ ì¶”ê°€
    adaptive_changes: Dict[str, Any] = field(default_factory=dict)  # ìƒˆë¡œ ì¶”ê°€


@dataclass
class LearningPattern:
    """í•™ìŠµ íŒ¨í„´ ë°ì´í„° í´ë˜ìŠ¤"""

    pattern_id: str
    trigger_type: EvolutionTrigger
    success_rate: float
    average_improvement: float
    execution_time: float
    frequency: int
    last_used: datetime
    adaptation_score: float = 0.0


@dataclass
class AdaptiveTrigger:
    """ì ì‘í˜• íŠ¸ë¦¬ê±° ë°ì´í„° í´ë˜ìŠ¤"""

    trigger_id: str
    base_trigger: EvolutionTrigger
    adaptive_threshold: float
    environmental_factors: Dict[str, float]
    success_history: List[float]
    adaptation_rate: float = 0.1


class DuRiIntegratedEvolutionSystem:
    """DuRi í†µí•© ì§„í™” ì‹œìŠ¤í…œ (ì„±ëŠ¥ ìµœì í™” í†µí•© ë²„ì „)"""

    def __init__(self):
        """í†µí•© ì§„í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        # ê¸°ì¡´ ì‹œìŠ¤í…œë“¤ ì´ˆê¸°í™”
        self.thought_flow = DuRiThoughtFlow(
            input_data={"task": "integrated_evolution", "phase": "integrated"},
            context={"goal": "self_evolution", "mode": "integration"},
        )
        self.self_rewriting = SelfRewritingModule()
        self.genetic_evolution = GeneticEvolutionEngine()
        self.meta_coder = MetaCoder()

        # Phase Î© ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë™ì )
        self.phase_omega = None
        self._try_initialize_phase_omega()

        # í†µí•© ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)
        self.enhanced_parallel_processor = self._initialize_enhanced_parallel_processor()
        self.performance_optimizer = self._initialize_performance_optimizer()
        self.act_r_parallel_processor = self._initialize_act_r_parallel_processor()

        # ìºì‹± ì‹œìŠ¤í…œ (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)
        self.cache = {}
        self.cache_ttl = 300  # 5ë¶„ ìºì‹œ
        self.cache_max_size = 1000

        # ë¡œë“œ ë°¸ëŸ°ì‹± ì‹œìŠ¤í…œ (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)
        self.node_status = {
            "brain_node": {"status": "active", "response_time": 0.0, "load": 0},
            "evolution_node": {"status": "active", "response_time": 0.0, "load": 0},
            "judgment_node": {"status": "active", "response_time": 0.0, "load": 0},
            "action_node": {"status": "active", "response_time": 0.0, "load": 0},
            "feedback_node": {"status": "active", "response_time": 0.0, "load": 0},
        }

        # ì„±ëŠ¥ ì„¤ì • (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)
        self.performance_config = {
            "enable_parallel_processing": True,
            "max_workers": 10,
            "thread_pool_size": 15,
            "process_pool_size": 4,
            "enable_learning_based_evolution": True,
            "enable_adaptive_triggers": True,
            "enable_caching": True,
            "enable_load_balancing": True,
        }

        # ì§„í™” ì„¤ì • (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)
        self.evolution_config = {
            "enable_self_rewriting": True,
            "enable_genetic_evolution": True,
            "enable_meta_coding": True,
            "learning_threshold": 0.3,
            "adaptive_threshold": 0.5,
            "min_improvement_score": 0.1,
        }

        # í†µí•© ì„±ëŠ¥ ë©”íŠ¸ë¦­ (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)
        self.performance_metrics = {
            "total_tasks": 0,
            "completed_tasks": 0,
            "failed_tasks": 0,
            "average_execution_time": 0.0,
            "parallel_efficiency": 0.0,
            "performance_improvement": 0.0,
            "cache_hits": 0,
            "cache_misses": 0,
            "cache_hit_rate": 0.0,
            "total_requests": 0,
            "error_count": 0,
        }

        # í•™ìŠµ ê¸°ë°˜ ì§„í™” ì‹œìŠ¤í…œ
        self.learning_patterns: Dict[str, LearningPattern] = {}
        self.adaptive_triggers: Dict[str, AdaptiveTrigger] = {}

        # ì§„í™” ì„¸ì…˜ ê´€ë¦¬
        self.evolution_sessions: List[EvolutionSession] = []
        self.stimulus_history: List[StimulusEvent] = []

        # ì„±ëŠ¥ ì¸¡ì •ìš© (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)
        self.baseline_execution_time = 0.215  # í˜„ì¬ ê¸°ì¤€ ì‹œê°„
        self.target_execution_time = 0.1  # ëª©í‘œ ì‹œê°„ (53% í–¥ìƒ)

        # ThreadPoolExecutor (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)
        self.executor = ThreadPoolExecutor(max_workers=10)

        logger.info("ğŸš€ DuRi í†µí•© ì§„í™” ì‹œìŠ¤í…œ (ì„±ëŠ¥ ìµœì í™” í†µí•© ë²„ì „) ì´ˆê¸°í™” ì™„ë£Œ")

    def _try_initialize_phase_omega(self):
        """Phase Î© ì‹œìŠ¤í…œì„ ë™ì ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ëŠ” ë©”ì„œë“œ"""
        if self.phase_omega is None:
            try:
                phase_omega_module = _get_phase_omega_system()
                if phase_omega_module:
                    self.phase_omega = phase_omega_module.DuRiPhaseOmega()
                    logger.info("Phase Î© ì‹œìŠ¤í…œ ë™ì  import ë° ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    logger.warning("Phase Î© ì‹œìŠ¤í…œ ë™ì  import ì‹¤íŒ¨ ë˜ëŠ” ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.error(f"Phase Î© ì‹œìŠ¤í…œ ë™ì  ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def _initialize_enhanced_parallel_processor(self):
        """í–¥ìƒëœ ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            from enhanced_act_r_parallel_processor import \
                EnhancedACTRParallelProcessor

            processor = EnhancedACTRParallelProcessor(max_concurrent_tasks=10)
            logger.info("âœ… í–¥ìƒëœ ACT-R ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ í†µí•© ì™„ë£Œ")
            return processor
        except ImportError as e:
            logger.warning(f"í–¥ìƒëœ ACT-R ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ í†µí•© ì‹¤íŒ¨: {e}")
            return None

    def _initialize_performance_optimizer(self):
        """ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            # ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œì„ ì§ì ‘ êµ¬í˜„
            class IntegratedPerformanceOptimizer:
                def __init__(self):
                    self.cache = {}
                    self.cache_ttl = 300
                    self.performance_metrics = {
                        "total_requests": 0,
                        "cache_hits": 0,
                        "cache_misses": 0,
                        "average_response_time": 0.0,
                        "parallel_requests": 0,
                        "error_count": 0,
                    }
                    self.executor = ThreadPoolExecutor(max_workers=10)

                async def optimize_request(
                    self, user_input: str, duri_response: str, metadata: Dict[str, Any]
                ) -> Dict[str, Any]:
                    """ìš”ì²­ ìµœì í™” ì²˜ë¦¬"""
                    try:
                        start_time = time.time()

                        # ìºì‹œ í™•ì¸
                        cache_key = self._generate_cache_key(user_input, duri_response, metadata)
                        cached_result = self._get_from_cache(cache_key)

                        if cached_result:
                            self.performance_metrics["cache_hits"] += 1
                            return cached_result

                        self.performance_metrics["cache_misses"] += 1

                        # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
                        result = await self._parallel_processing(
                            user_input, duri_response, metadata
                        )

                        # ê²°ê³¼ ìºì‹±
                        self._cache_result(cache_key, result)

                        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                        processing_time = time.time() - start_time
                        self._update_performance_metrics(processing_time)

                        return result

                    except Exception as e:
                        logger.error(f"âŒ ì„±ëŠ¥ ìµœì í™” ì˜¤ë¥˜: {e}")
                        self.performance_metrics["error_count"] += 1
                        raise

                def _generate_cache_key(
                    self, user_input: str, duri_response: str, metadata: Dict[str, Any]
                ) -> str:
                    """ìºì‹œ í‚¤ ìƒì„±"""
                    content = f"{user_input}:{duri_response}:{json.dumps(metadata, sort_keys=True)}"
                    return hashlib.md5(content.encode()).hexdigest()

                def _get_from_cache(self, cache_key: str) -> Optional[Any]:
                    """ìºì‹œì—ì„œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
                    if cache_key in self.cache:
                        cached_data = self.cache[cache_key]
                        if time.time() - cached_data["timestamp"] < self.cache_ttl:
                            return cached_data["result"]
                        else:
                            del self.cache[cache_key]
                    return None

                def _cache_result(self, cache_key: str, result: Any):
                    """ê²°ê³¼ ìºì‹±"""
                    if len(self.cache) >= 1000:  # ìµœëŒ€ ìºì‹œ í¬ê¸°
                        # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                        oldest_key = min(
                            self.cache.keys(), key=lambda k: self.cache[k]["timestamp"]
                        )
                        del self.cache[oldest_key]

                    self.cache[cache_key] = {"result": result, "timestamp": time.time()}

                async def _parallel_processing(
                    self, user_input: str, duri_response: str, metadata: Dict[str, Any]
                ) -> Dict[str, Any]:
                    """ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰"""
                    # ì‹¤ì œ ë³‘ë ¬ ì²˜ë¦¬ ë¡œì§ êµ¬í˜„
                    return {
                        "optimized_input": user_input,
                        "optimized_response": duri_response,
                        "metadata": metadata,
                        "processing_time": time.time(),
                    }

                def _update_performance_metrics(self, processing_time: float):
                    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
                    self.performance_metrics["total_requests"] += 1
                    self.performance_metrics["average_response_time"] = (
                        self.performance_metrics["average_response_time"]
                        * (self.performance_metrics["total_requests"] - 1)
                        + processing_time
                    ) / self.performance_metrics["total_requests"]

            optimizer = IntegratedPerformanceOptimizer()
            logger.info("âœ… ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ í†µí•© ì™„ë£Œ")
            return optimizer
        except Exception as e:
            logger.warning(f"ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ í†µí•© ì‹¤íŒ¨: {e}")
            return None

    def _initialize_act_r_parallel_processor(self):
        """ACT-R ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            from act_r_parallel_processor import ACTRParallelProcessor

            processor = ACTRParallelProcessor(max_concurrent_tasks=10)
            logger.info("âœ… ACT-R ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ í†µí•© ì™„ë£Œ")
            return processor
        except ImportError as e:
            logger.warning(f"ACT-R ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ í†µí•© ì‹¤íŒ¨: {e}")
            return None

    def _generate_cache_key(self, input_data: Dict[str, Any], context: Dict[str, Any]) -> str:
        """ìºì‹œ í‚¤ ìƒì„± (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©) - ìµœì í™”ëœ ë²„ì „"""
        try:
            # ë” íš¨ìœ¨ì ì¸ ìºì‹œ í‚¤ ìƒì„± ì•Œê³ ë¦¬ì¦˜
            optimized_key = self._optimize_cache_key(input_data, context)
            return optimized_key
        except Exception as e:
            logger.warning(f"ìµœì í™”ëœ ìºì‹œ í‚¤ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©: {e}")
            # ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ í´ë°±
            content = (
                f"{json.dumps(input_data, sort_keys=True)}:{json.dumps(context, sort_keys=True)}"
            )
            return hashlib.md5(content.encode()).hexdigest()

    def _optimize_cache_key(self, input_data: Dict[str, Any], context: Dict[str, Any]) -> str:
        """ìµœì í™”ëœ ìºì‹œ í‚¤ ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ ì œì™¸) - ê³ ê¸‰ ë²„ì „"""
        try:
            # ê³ ê¸‰ ìºì‹œ í‚¤ ìƒì„± ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
            return self._advanced_cache_key_generation(input_data, context)
        except Exception as e:
            logger.warning(f"ê³ ê¸‰ ìºì‹œ í‚¤ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©: {e}")
            # ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ í´ë°±
            important_data = self._extract_important_data(input_data)
            important_context = self._extract_important_context(context)
            normalized_data = self._normalize_data(important_data)
            normalized_context = self._normalize_data(important_context)
            key_content = f"{normalized_data}:{normalized_context}"
            return hashlib.md5(key_content.encode()).hexdigest()

    def _advanced_cache_key_generation(
        self, input_data: Dict[str, Any], context: Dict[str, Any]
    ) -> str:
        """ê³ ê¸‰ ìºì‹œ í‚¤ ìƒì„± ì•Œê³ ë¦¬ì¦˜"""
        try:
            # 1. ë°ì´í„° ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ ì ìš©
            weighted_data = self._apply_data_importance_weights(input_data)
            weighted_context = self._apply_context_priority_weights(context)

            # 2. íŒ¨í„´ ê¸°ë°˜ í‚¤ ìƒì„±
            pattern_key = self._generate_pattern_based_key(weighted_data, weighted_context)

            # 3. ì»¨í…ìŠ¤íŠ¸ ìš°ì„ ìˆœìœ„ ë¶„ì„
            priority_key = self._analyze_context_priority(weighted_context)

            # 4. ìµœì¢… í‚¤ ìƒì„±
            final_key_content = f"{pattern_key}:{priority_key}"
            return hashlib.md5(final_key_content.encode()).hexdigest()

        except Exception as e:
            logger.error(f"ê³ ê¸‰ ìºì‹œ í‚¤ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    def _apply_data_importance_weights(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ ì ìš©"""
        importance_weights = {
            "task": 1.0,  # ê°€ì¥ ì¤‘ìš”
            "data": 0.9,  # ë§¤ìš° ì¤‘ìš”
            "type": 0.8,  # ì¤‘ìš”
            "id": 0.7,  # ë³´í†µ
            "mode": 0.6,  # ë³´í†µ
            "goal": 0.5,  # ë‚®ìŒ
        }

        weighted_data = {}
        for key, value in input_data.items():
            weight = importance_weights.get(key, 0.3)  # ê¸°ë³¸ ê°€ì¤‘ì¹˜
            weighted_data[key] = {
                "value": value,
                "weight": weight,
                "importance_score": weight * len(str(value)),
            }

        return weighted_data

    def _apply_context_priority_weights(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ìš°ì„ ìˆœìœ„ ê°€ì¤‘ì¹˜ ì ìš©"""
        priority_weights = {
            "goal": 1.0,  # ê°€ì¥ ì¤‘ìš”
            "mode": 0.9,  # ë§¤ìš° ì¤‘ìš”
            "test_mode": 0.8,  # ì¤‘ìš”
            "performance_optimization": 0.7,  # ë³´í†µ
            "optimization_level": 0.6,  # ë³´í†µ
            "cache_strategy": 0.5,  # ë‚®ìŒ
        }

        weighted_context = {}
        for key, value in context.items():
            weight = priority_weights.get(key, 0.3)  # ê¸°ë³¸ ê°€ì¤‘ì¹˜
            weighted_context[key] = {
                "value": value,
                "weight": weight,
                "priority_score": weight * (1.0 if value else 0.5),
            }

        return weighted_context

    def _generate_pattern_based_key(
        self, weighted_data: Dict[str, Any], weighted_context: Dict[str, Any]
    ) -> str:
        """íŒ¨í„´ ê¸°ë°˜ í‚¤ ìƒì„±"""
        try:
            # 1. ë°ì´í„° íŒ¨í„´ ë¶„ì„
            data_pattern = self._analyze_data_pattern(weighted_data)

            # 2. ì»¨í…ìŠ¤íŠ¸ íŒ¨í„´ ë¶„ì„
            context_pattern = self._analyze_context_pattern(weighted_context)

            # 3. íŒ¨í„´ ì¡°í•©
            combined_pattern = f"{data_pattern}:{context_pattern}"

            return hashlib.md5(combined_pattern.encode()).hexdigest()[:16]

        except Exception as e:
            logger.error(f"íŒ¨í„´ ê¸°ë°˜ í‚¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return "default_pattern"

    def _analyze_data_pattern(self, weighted_data: Dict[str, Any]) -> str:
        """ë°ì´í„° íŒ¨í„´ ë¶„ì„"""
        try:
            # ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
            total_score = 0
            pattern_elements = []

            for key, data_info in weighted_data.items():
                score = data_info["importance_score"]
                total_score += score
                pattern_elements.append(f"{key}:{score:.2f}")

            # íŒ¨í„´ ì •ë ¬
            pattern_elements.sort(key=lambda x: float(x.split(":")[1]), reverse=True)

            # ìƒìœ„ 3ê°œ ìš”ì†Œë§Œ ì‚¬ìš©
            top_pattern = ":".join(pattern_elements[:3])

            return f"data_{total_score:.2f}_{top_pattern}"

        except Exception as e:
            logger.error(f"ë°ì´í„° íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return "default_data_pattern"

    def _analyze_context_pattern(self, weighted_context: Dict[str, Any]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ íŒ¨í„´ ë¶„ì„"""
        try:
            # ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°
            total_priority = 0
            priority_elements = []

            for key, context_info in weighted_context.items():
                priority = context_info["priority_score"]
                total_priority += priority
                priority_elements.append(f"{key}:{priority:.2f}")

            # ìš°ì„ ìˆœìœ„ ì •ë ¬
            priority_elements.sort(key=lambda x: float(x.split(":")[1]), reverse=True)

            # ìƒìœ„ 3ê°œ ìš”ì†Œë§Œ ì‚¬ìš©
            top_priority = ":".join(priority_elements[:3])

            return f"ctx_{total_priority:.2f}_{top_priority}"

        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return "default_context_pattern"

    def _analyze_context_priority(self, weighted_context: Dict[str, Any]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ìš°ì„ ìˆœìœ„ ë¶„ì„"""
        try:
            # ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°
            priority_scores = []
            for key, context_info in weighted_context.items():
                priority_scores.append(context_info["priority_score"])

            if priority_scores:
                avg_priority = sum(priority_scores) / len(priority_scores)
                max_priority = max(priority_scores)
                min_priority = min(priority_scores)

                return f"priority_{avg_priority:.2f}_{max_priority:.2f}_{min_priority:.2f}"
            else:
                return "priority_default"

        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ ìš°ì„ ìˆœìœ„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return "priority_error"

    def _segmented_cache_strategy(self):
        """ìºì‹œ ì „ëµ ì„¸ë¶„í™”"""
        try:
            # 1. ë°ì´í„° ìœ í˜•ë³„ ìºì‹œ ì „ëµ
            self._implement_data_type_cache_strategy()

            # 2. ì‚¬ìš© ë¹ˆë„ ê¸°ë°˜ ìºì‹œ ê´€ë¦¬
            self._implement_frequency_based_cache_management()

            # 3. ì˜ˆì¸¡ì  ìºì‹œ ë¡œë”©
            self._implement_predictive_cache_loading()

            logger.info("ìºì‹œ ì „ëµ ì„¸ë¶„í™” ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ìºì‹œ ì „ëµ ì„¸ë¶„í™” ì‹¤íŒ¨: {e}")

    def _implement_data_type_cache_strategy(self):
        """ë°ì´í„° ìœ í˜•ë³„ ìºì‹œ ì „ëµ êµ¬í˜„"""
        try:
            # ë°ì´í„° ìœ í˜•ë³„ ìºì‹œ ì„¤ì •
            self.data_type_cache_config = {
                "frequent": {"ttl": 1800, "max_size": 500, "priority": "high"},  # 30ë¶„
                "normal": {"ttl": 600, "max_size": 1000, "priority": "medium"},  # 10ë¶„
                "rare": {"ttl": 300, "max_size": 200, "priority": "low"},  # 5ë¶„
            }

        except Exception as e:
            logger.error(f"ë°ì´í„° ìœ í˜•ë³„ ìºì‹œ ì „ëµ êµ¬í˜„ ì‹¤íŒ¨: {e}")

    def _implement_frequency_based_cache_management(self):
        """ì‚¬ìš© ë¹ˆë„ ê¸°ë°˜ ìºì‹œ ê´€ë¦¬ êµ¬í˜„"""
        try:
            # ì‚¬ìš© ë¹ˆë„ ì¶”ì 
            if not hasattr(self, "cache_frequency_tracker"):
                self.cache_frequency_tracker = {}

            # ë¹ˆë„ ê¸°ë°˜ ìºì‹œ ì •ë¦¬
            self._cleanup_low_frequency_cache()

        except Exception as e:
            logger.error(f"ì‚¬ìš© ë¹ˆë„ ê¸°ë°˜ ìºì‹œ ê´€ë¦¬ êµ¬í˜„ ì‹¤íŒ¨: {e}")

    def _implement_predictive_cache_loading(self):
        """ì˜ˆì¸¡ì  ìºì‹œ ë¡œë”© êµ¬í˜„"""
        try:
            # ì˜ˆì¸¡ ëª¨ë¸ ì´ˆê¸°í™”
            if not hasattr(self, "cache_prediction_model"):
                self.cache_prediction_model = {
                    "patterns": {},
                    "predictions": {},
                    "accuracy": 0.0,
                }

            # ì˜ˆì¸¡ì  ìºì‹œ ë¡œë”© ì‹¤í–‰
            self._load_predictive_cache()

        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ì  ìºì‹œ ë¡œë”© êµ¬í˜„ ì‹¤íŒ¨: {e}")

    def _cleanup_low_frequency_cache(self):
        """ë‚®ì€ ë¹ˆë„ ìºì‹œ ì •ë¦¬"""
        try:
            current_time = time.time()
            low_frequency_threshold = 0.1  # 10% ë¯¸ë§Œ ì‚¬ìš© ë¹ˆë„

            # ë¹ˆë„ ë¶„ì„
            for key, data in self.cache.items():
                if "access_count" not in data:
                    data["access_count"] = 0

                # ì‚¬ìš© ë¹ˆë„ ê³„ì‚°
                age = current_time - data["timestamp"]
                frequency = data["access_count"] / max(age / 3600, 1)  # ì‹œê°„ë‹¹ ì ‘ê·¼ íšŸìˆ˜

                # ë‚®ì€ ë¹ˆë„ í•­ëª© ì œê±°
                if frequency < low_frequency_threshold and age > 300:  # 5ë¶„ ì´ìƒ ëœ í•­ëª©
                    del self.cache[key]
                    logger.debug(f"ë‚®ì€ ë¹ˆë„ ìºì‹œ í•­ëª© ì œê±°: {key[:20]}...")

        except Exception as e:
            logger.error(f"ë‚®ì€ ë¹ˆë„ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def _load_predictive_cache(self):
        """ì˜ˆì¸¡ì  ìºì‹œ ë¡œë”©"""
        try:
            # ì‚¬ìš© íŒ¨í„´ ë¶„ì„
            patterns = self._analyze_usage_patterns()

            # ì˜ˆì¸¡ì  ìºì‹œ ë¡œë”©
            for pattern, probability in patterns.items():
                if probability > 0.7:  # 70% ì´ìƒ í™•ë¥ 
                    self._preload_cache_for_pattern(pattern)

        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ì  ìºì‹œ ë¡œë”© ì‹¤íŒ¨: {e}")

    def _analyze_usage_patterns(self) -> Dict[str, float]:
        """ì‚¬ìš© íŒ¨í„´ ë¶„ì„"""
        try:
            patterns = {}

            # ìµœê·¼ ì‚¬ìš© íŒ¨í„´ ë¶„ì„
            recent_usage = list(self.cache.keys())[-10:]  # ìµœê·¼ 10ê°œ

            for key in recent_usage:
                if key in self.cache:
                    data = self.cache[key]
                    pattern = self._extract_pattern_from_key(key)

                    if pattern in patterns:
                        patterns[pattern] += 1
                    else:
                        patterns[pattern] = 1

            # í™•ë¥  ê³„ì‚°
            total_usage = len(recent_usage)
            if total_usage > 0:
                for pattern in patterns:
                    patterns[pattern] = patterns[pattern] / total_usage

            return patterns

        except Exception as e:
            logger.error(f"ì‚¬ìš© íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}

    def _extract_pattern_from_key(self, key: str) -> str:
        """í‚¤ì—ì„œ íŒ¨í„´ ì¶”ì¶œ"""
        try:
            # í‚¤ì˜ ì²« 8ìë¦¬ë¥¼ íŒ¨í„´ìœ¼ë¡œ ì‚¬ìš©
            return key[:8]
        except Exception as e:
            logger.error(f"íŒ¨í„´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return "default_pattern"

    def _preload_cache_for_pattern(self, pattern: str):
        """íŒ¨í„´ì— ëŒ€í•œ ì˜ˆì¸¡ì  ìºì‹œ ë¡œë”©"""
        try:
            # íŒ¨í„´ ê¸°ë°˜ ì˜ˆì¸¡ì  ìºì‹œ ë¡œë”©
            logger.debug(f"ì˜ˆì¸¡ì  ìºì‹œ ë¡œë”©: {pattern}")

        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ì  ìºì‹œ ë¡œë”© ì‹¤íŒ¨: {e}")

    def _predictive_cache_system(self):
        """ìºì‹œ ì˜ˆì¸¡ ì‹œìŠ¤í…œ"""
        try:
            # 1. ì‚¬ìš© íŒ¨í„´ ë¶„ì„
            usage_patterns = self._analyze_usage_patterns()

            # 2. ì˜ˆì¸¡ì  ìºì‹œ ë¡œë”©
            self._load_predictive_cache()

            # 3. ì ì‘í˜• ìºì‹œ ê´€ë¦¬
            self._adaptive_cache_management(usage_patterns)

            logger.info("ìºì‹œ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì‹¤í–‰ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ìºì‹œ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì‹¤íŒ¨: {e}")

    def _adaptive_cache_management(self, usage_patterns: Dict[str, float]):
        """ì ì‘í˜• ìºì‹œ ê´€ë¦¬"""
        try:
            # íŒ¨í„´ ê¸°ë°˜ ìºì‹œ í¬ê¸° ì¡°ì •
            total_probability = sum(usage_patterns.values())

            if total_probability > 0.8:  # ë†’ì€ ì‚¬ìš© íŒ¨í„´
                # ìºì‹œ í¬ê¸° ì¦ê°€
                new_size = min(self.cache_max_size * 1.5, 3000)
                if new_size != self.cache_max_size:
                    self.cache_max_size = int(new_size)
                    logger.info(f"ì ì‘í˜• ìºì‹œ í¬ê¸° ì¦ê°€: {new_size}")

            elif total_probability < 0.3:  # ë‚®ì€ ì‚¬ìš© íŒ¨í„´
                # ìºì‹œ í¬ê¸° ê°ì†Œ
                new_size = max(self.cache_max_size * 0.7, 500)
                if new_size != self.cache_max_size:
                    self.cache_max_size = int(new_size)
                    logger.info(f"ì ì‘í˜• ìºì‹œ í¬ê¸° ê°ì†Œ: {new_size}")

        except Exception as e:
            logger.error(f"ì ì‘í˜• ìºì‹œ ê´€ë¦¬ ì‹¤íŒ¨: {e}")

    def _get_from_cache(self, cache_key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©) - ê°œì„ ëœ ë²„ì „"""
        if cache_key in self.cache:
            cached_data = self.cache[cache_key]
            current_time = time.time()

            # TTL í™•ì¸
            if current_time - cached_data["timestamp"] < self.cache_ttl:
                # ë§ˆì§€ë§‰ ì ‘ê·¼ ì‹œê°„ ì—…ë°ì´íŠ¸
                cached_data["last_accessed"] = current_time
                self.performance_metrics["cache_hits"] += 1
                logger.debug(f"âš¡ ìºì‹œ íˆíŠ¸: {cache_key[:20]}...")
                return cached_data["result"]
            else:
                # ë§Œë£Œëœ í•­ëª© ì œê±°
                del self.cache[cache_key]

        self.performance_metrics["cache_misses"] += 1
        return None

    def _cache_result(self, cache_key: str, result: Any):
        """ê²°ê³¼ ìºì‹± (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©) - ê°œì„ ëœ ë²„ì „"""
        try:
            # ìºì‹œ í¬ê¸° í™•ì¸ ë° ì •ë¦¬
            if len(self.cache) >= self.cache_max_size:
                self._cleanup_lru_cache()

            # ìºì‹œì— ì €ì¥
            self.cache[cache_key] = {
                "result": result,
                "timestamp": time.time(),
                "last_accessed": time.time(),
            }

            logger.debug(f"ğŸ’¾ ìºì‹œ ì €ì¥: {cache_key[:20]}...")

        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def _update_cache_statistics(self):
        """ìºì‹œ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            total_requests = (
                self.performance_metrics["cache_hits"] + self.performance_metrics["cache_misses"]
            )
            if total_requests > 0:
                hit_rate = self.performance_metrics["cache_hits"] / total_requests
                self.performance_metrics["cache_hit_rate"] = hit_rate

                # ìºì‹œ ì „ëµ ìë™ ê°œì„ 
                if total_requests % 100 == 0:  # 100ë²ˆì§¸ ìš”ì²­ë§ˆë‹¤
                    self._improve_cache_strategy()

        except Exception as e:
            logger.error(f"ìºì‹œ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    def _generate_task_cache_key(self, task: ParallelTask) -> str:
        """ì‘ì—… ìºì‹œ í‚¤ ìƒì„± (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)"""
        content = f"{task.id}:{task.name}:{task.function.__name__}:{json.dumps(task.args, sort_keys=True)}:{json.dumps(task.kwargs, sort_keys=True)}"
        return hashlib.md5(content.encode()).hexdigest()

    async def _execute_parallel_tasks_with_optimization(
        self, tasks: List[ParallelTask]
    ) -> List[Any]:
        """ìµœì í™”ëœ ë³‘ë ¬ ì‘ì—… ì‹¤í–‰ (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)"""
        logger.info(f"âš¡ {len(tasks)}ê°œ ì‘ì—… ìµœì í™”ëœ ë³‘ë ¬ ì‹¤í–‰ ì‹œì‘")

        start_time = time.time()

        try:
            # ì‘ì—…ì„ ìš°ì„ ìˆœìœ„ë³„ë¡œ ì •ë ¬
            sorted_tasks = sorted(tasks, key=lambda x: x.priority.value)

            # ìºì‹œ í™•ì¸ ë° ë³‘ë ¬ ì‹¤í–‰
            coroutines = []
            for task in sorted_tasks:
                coroutine = self._execute_single_task_with_cache(task)
                coroutines.append(coroutine)

            # asyncio.gatherë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì‹¤í–‰
            results = await asyncio.gather(*coroutines, return_exceptions=True)

            execution_time = time.time() - start_time

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            await self._update_performance_metrics(execution_time, len(tasks))

            logger.info(f"âœ… ìµœì í™”ëœ ë³‘ë ¬ ì‹¤í–‰ ì™„ë£Œ: {execution_time:.3f}ì´ˆ")
            return results

        except Exception as e:
            logger.error(f"âŒ ìµœì í™”ëœ ë³‘ë ¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return []

    async def _execute_single_task_with_cache(self, task: ParallelTask) -> Any:
        """ìºì‹±ì´ í¬í•¨ëœ ë‹¨ì¼ ì‘ì—… ì‹¤í–‰ (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)"""
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._generate_task_cache_key(task)

        # ìºì‹œ í™•ì¸
        cached_result = self._get_from_cache(cache_key)
        if cached_result:
            logger.info(f"âš¡ ìºì‹œ íˆíŠ¸: {task.name}")
            return cached_result

        # ì‹¤ì œ ì‘ì—… ì‹¤í–‰
        task.status = TaskStatus.RUNNING
        task.started_at = datetime.now()

        try:
            # ì‘ì—… ì‹¤í–‰
            if asyncio.iscoroutinefunction(task.function):
                result = await task.function(*task.args, **task.kwargs)
            else:
                # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, task.function, *task.args, **task.kwargs)

            task.status = TaskStatus.COMPLETED
            task.result = result
            task.completed_at = datetime.now()
            task.execution_time = (task.completed_at - task.started_at).total_seconds()

            # ê²°ê³¼ ìºì‹±
            self._cache_result(cache_key, result)

            logger.info(f"âœ… ì‘ì—… ì™„ë£Œ: {task.name} ({task.execution_time:.3f}ì´ˆ)")
            return result

        except Exception as e:
            task.status = TaskStatus.FAILED
            task.error = str(e)
            task.completed_at = datetime.now()
            task.execution_time = (task.completed_at - task.started_at).total_seconds()

            logger.error(f"âŒ ì‘ì—… ì‹¤íŒ¨: {task.name} - {e}")
            return None

    async def _execute_enhanced_parallel_processing(
        self, tasks: List[Callable], task_type: str = "general"
    ) -> List[Any]:
        """í–¥ìƒëœ ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰ (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)"""
        logger.info(f"ğŸš€ í–¥ìƒëœ ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰: {len(tasks)}ê°œ {task_type} ì‘ì—…")

        start_time = time.time()

        try:
            # í–¥ìƒëœ ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‚¬ìš©
            if self.enhanced_parallel_processor:
                parallel_tasks = []
                for i, task_func in enumerate(tasks):
                    task = ParallelTask(
                        id=f"{task_type}_{i}",
                        name=f"{task_type} ì‘ì—… {i+1}",
                        function=task_func,
                        priority=(
                            TaskPriority.HIGH
                            if task_type in ["judgment", "critical"]
                            else TaskPriority.MEDIUM
                        ),
                    )
                    parallel_tasks.append(task)

                results = await self.enhanced_parallel_processor.execute_parallel_tasks(
                    parallel_tasks
                )
            else:
                # ê¸°ë³¸ ë³‘ë ¬ ì²˜ë¦¬
                coroutines = []
                for task_func in tasks:
                    if asyncio.iscoroutinefunction(task_func):
                        coroutines.append(task_func())
                    else:
                        loop = asyncio.get_event_loop()
                        coroutines.append(loop.run_in_executor(None, task_func))

                results = await asyncio.gather(*coroutines, return_exceptions=True)

            execution_time = time.time() - start_time
            logger.info(f"âœ… í–¥ìƒëœ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {execution_time:.3f}ì´ˆ")

            return results

        except Exception as e:
            logger.error(f"âŒ í–¥ìƒëœ ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []

    async def _optimize_performance(
        self, input_data: Dict[str, Any], context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìµœì í™” ì‹¤í–‰ (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)"""
        try:
            if self.performance_optimizer:
                # ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ì‚¬ìš©
                optimized_result = await self.performance_optimizer.optimize_request(
                    str(input_data), str(context), {"timestamp": time.time()}
                )
                return optimized_result
            else:
                # ê¸°ë³¸ ìµœì í™”
                return {
                    "optimized_input": input_data,
                    "optimized_context": context,
                    "optimization_time": time.time(),
                }
        except Exception as e:
            logger.error(f"âŒ ì„±ëŠ¥ ìµœì í™” ì‹¤íŒ¨: {e}")
            return input_data

    async def process_stimulus(
        self, input_data: Dict[str, Any], context: Optional[Dict[str, Any]] = None
    ) -> IntegratedEvolutionResult:
        """ìê·¹ ì²˜ë¦¬ ë° í†µí•© ì§„í™” ì‹¤í–‰ (ì„±ëŠ¥ ìµœì í™” í†µí•© ë²„ì „)"""
        start_time = time.time()

        try:
            # ì„±ëŠ¥ ìµœì í™” ì ìš©
            optimized_data = await self._optimize_performance(input_data, context or {})

            # ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(optimized_data, context or {})
            cached_result = self._get_from_cache(cache_key)
            if cached_result:
                logger.info("âš¡ ìºì‹œ íˆíŠ¸: ê¸°ì¡´ ì§„í™” ê²°ê³¼ ì‚¬ìš©")
                return cached_result

            # ìê·¹ ì´ë²¤íŠ¸ ìƒì„± (í–¥ìƒëœ ë²„ì „)
            stimulus_event = await self._create_enhanced_stimulus_event(
                optimized_data, context or {}
            )

            # ì§„í™” ì„¸ì…˜ ì‹œì‘
            session = await self._start_evolution_session(stimulus_event)

            # í–¥ìƒëœ í†µí•© ì§„í™” ì‹¤í–‰
            evolution_results = await self._execute_enhanced_integrated_evolution(session)

            # ê²°ê³¼ í†µí•©
            integrated_result = await self._integrate_evolution_results(session, evolution_results)

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            execution_time = time.time() - start_time
            await self._update_performance_metrics(
                execution_time, integrated_result.overall_improvement_score
            )

            # ê²°ê³¼ ìºì‹±
            self._cache_result(cache_key, integrated_result)

            # í•™ìŠµ íŒ¨í„´ ë° ì ì‘í˜• íŠ¸ë¦¬ê±° ì—…ë°ì´íŠ¸
            await self._update_learning_patterns(stimulus_event, integrated_result)
            await self._update_adaptive_triggers(stimulus_event, integrated_result)

            logger.info(
                f"âœ… í†µí•© ì§„í™” ì™„ë£Œ: {execution_time:.3f}ì´ˆ, ê°œì„ ì ìˆ˜: {integrated_result.overall_improvement_score:.3f}"
            )

            return integrated_result

        except Exception as e:
            error_message = f"í†µí•© ì§„í™” ì²˜ë¦¬ ì‹¤íŒ¨: {e}"
            logger.error(f"âŒ {error_message}")

            # ì‹¤íŒ¨í•œ ê²°ê³¼ ìƒì„±
            stimulus_event = StimulusEvent(
                event_id=f"error_{int(time.time())}",
                trigger_type=EvolutionTrigger.EXTERNAL_STIMULUS,
                input_data=input_data,
                context=context or {},
                description=f"ì˜¤ë¥˜ ë°œìƒ: {e}",
            )

            return await self._create_failed_result(stimulus_event, error_message)

    async def _execute_enhanced_integrated_evolution(
        self, session: EvolutionSession
    ) -> Dict[str, Any]:
        """í–¥ìƒëœ í†µí•© ì§„í™” ì‹¤í–‰ (ì„±ëŠ¥ ìµœì í™” í†µí•© ë²„ì „)"""
        logger.info(f"ğŸš€ í–¥ìƒëœ í†µí•© ì§„í™” ì‹¤í–‰ ì‹œì‘: {session.session_id}")

        start_time = time.time()
        results = {}

        try:
            # ë³‘ë ¬ ì‹¤í–‰ì„ ìœ„í•œ ì‘ì—… ëª©ë¡ ìƒì„±
            parallel_tasks = []

            # 1. Thought Flow ì‹¤í–‰ (ë³‘ë ¬)
            thought_flow_task = ParallelTask(
                id="thought_flow",
                name="Thought Flow ì‹¤í–‰",
                function=self._execute_thought_flow,
                args=(session,),
                priority=TaskPriority.CRITICAL,
            )
            parallel_tasks.append(thought_flow_task)

            # 2. Phase Î© ì‹¤í–‰ (ë³‘ë ¬)
            if self.phase_omega:
                phase_omega_task = ParallelTask(
                    id="phase_omega",
                    name="Phase Î© ì‹¤í–‰",
                    function=self._execute_phase_omega,
                    args=(session,),
                    priority=TaskPriority.HIGH,
                )
                parallel_tasks.append(phase_omega_task)

            # 3. Self-Rewriting ì‹¤í–‰ (ë³‘ë ¬)
            self_rewriting_task = ParallelTask(
                id="self_rewriting",
                name="Self-Rewriting ì‹¤í–‰",
                function=self._execute_self_rewriting,
                args=(session,),
                priority=TaskPriority.HIGH,
            )
            parallel_tasks.append(self_rewriting_task)

            # 4. Genetic Evolution ì‹¤í–‰ (ë³‘ë ¬)
            genetic_evolution_task = ParallelTask(
                id="genetic_evolution",
                name="Genetic Evolution ì‹¤í–‰",
                function=self._execute_genetic_evolution,
                args=(session,),
                priority=TaskPriority.MEDIUM,
            )
            parallel_tasks.append(genetic_evolution_task)

            # 5. Meta-Coding ì‹¤í–‰ (ë³‘ë ¬)
            meta_coding_task = ParallelTask(
                id="meta_coding",
                name="Meta-Coding ì‹¤í–‰",
                function=self._execute_meta_coding,
                args=(session,),
                priority=TaskPriority.MEDIUM,
            )
            parallel_tasks.append(meta_coding_task)

            # 6. Learning Analysis ì‹¤í–‰ (ë³‘ë ¬)
            learning_analysis_task = ParallelTask(
                id="learning_analysis",
                name="Learning Analysis ì‹¤í–‰",
                function=self._execute_learning_analysis,
                args=(session,),
                priority=TaskPriority.LOW,
            )
            parallel_tasks.append(learning_analysis_task)

            # 7. Adaptive Optimization ì‹¤í–‰ (ë³‘ë ¬)
            adaptive_optimization_task = ParallelTask(
                id="adaptive_optimization",
                name="Adaptive Optimization ì‹¤í–‰",
                function=self._execute_adaptive_optimization,
                args=(session,),
                priority=TaskPriority.LOW,
            )
            parallel_tasks.append(adaptive_optimization_task)

            # í–¥ìƒëœ ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
            parallel_results = await self._execute_parallel_tasks_with_optimization(parallel_tasks)

            # ê²°ê³¼ ë§¤í•‘
            for i, task in enumerate(parallel_tasks):
                if i < len(parallel_results) and parallel_results[i] is not None:
                    results[task.id] = parallel_results[i]
                else:
                    results[task.id] = None

            # ì„±ëŠ¥ ìµœì í™” ì ìš©
            if self.performance_optimizer:
                optimized_results = await self.performance_optimizer.optimize_request(
                    str(results),
                    str(session.stimulus_event),
                    {"session_id": session.session_id},
                )
                results["optimization"] = optimized_results

            execution_time = time.time() - start_time
            logger.info(f"âœ… í–¥ìƒëœ í†µí•© ì§„í™” ì™„ë£Œ: {execution_time:.3f}ì´ˆ")

            return results

        except Exception as e:
            logger.error(f"âŒ í–¥ìƒëœ í†µí•© ì§„í™” ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    async def _execute_thought_flow(self, session: EvolutionSession) -> Optional[ThoughtFlowResult]:
        """ì‚¬ê³  íë¦„ ì‹¤í–‰"""
        try:
            thought_context = {
                "evolution_session": session.session_id,
                "stimulus_event": session.stimulus_event.event_id,
                "trigger_type": session.stimulus_event.trigger_type.value,
            }

            thought_result = await self.thought_flow.process()
            logger.info("ì‚¬ê³  íë¦„ ì‹¤í–‰ ì™„ë£Œ")

            return thought_result

        except Exception as e:
            logger.error(f"ì‚¬ê³  íë¦„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None

    async def _execute_phase_omega(self, session: EvolutionSession) -> Optional[Any]:
        """Phase Î© ì‹¤í–‰"""
        try:
            if self.phase_omega is None:
                # ë™ì  import ì‹œë„
                self._try_initialize_phase_omega()

            if self.phase_omega is None:
                logger.warning("Phase Î© ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ Phase Î© ì‹¤í–‰ì„ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤.")
                return None

            phase_omega_result = await self.phase_omega.process_with_survival_instinct(
                session.stimulus_event.input_data, session.stimulus_event.context
            )

            logger.info("Phase Î© ì‹¤í–‰ ì™„ë£Œ")

            return phase_omega_result

        except Exception as e:
            logger.error(f"Phase Î© ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None

    async def _execute_self_rewriting(self, session: EvolutionSession) -> Optional[Any]:
        """ìê°€ ìˆ˜ì • ì‹¤í–‰"""
        try:
            # ë³µì¡ë„ê°€ ë†’ì€ ëª¨ë“ˆë“¤ ì‹ë³„
            target_modules = await self._identify_target_modules()

            improvements_made = []

            for module_path in target_modules[:3]:  # ìƒìœ„ 3ê°œë§Œ ì²˜ë¦¬
                # ì½”ë“œ í‰ê°€
                assessment = await self.self_rewriter.assess_self_code(module_path)

                if assessment.complexity_score > 0.6:
                    # ê°œì„  ì œì•ˆ ìƒì„±
                    with open(module_path, "r", encoding="utf-8") as f:
                        current_code = f.read()

                    proposal = await self.self_rewriter.generate_alternative(
                        current_code, assessment
                    )

                    if proposal.expected_impact > 0.3:
                        # ì•ˆì „í•œ ì¬ì‘ì„± ì‹¤í–‰
                        rewrite_result = await self.self_rewriter.safely_rewrite(
                            module_path, proposal.proposed_code
                        )

                        if rewrite_result.success:
                            improvements_made.append(
                                f"{module_path}: {proposal.improvement_description}"
                            )

            logger.info(f"ìê°€ ìˆ˜ì • ì™„ë£Œ: {len(improvements_made)}ê°œ ê°œì„ ")

            return {
                "improvements_made": improvements_made,
                "modules_processed": len(target_modules[:3]),
            }

        except Exception as e:
            logger.error(f"ìê°€ ìˆ˜ì • ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None

    async def _execute_genetic_evolution(
        self, session: EvolutionSession
    ) -> Optional[EvolutionResult]:
        """ìœ ì „ì ì§„í™” ì‹¤í–‰"""
        try:
            # ì‹œë“œ ì½”ë“œ ìƒì„±
            seed_code = await self._generate_seed_code()

            # ì§„í™” ì‹¤í–‰
            evolution_result = await self.genetic_engine.evolve_capabilities(
                seed_code, target_goal="performance_optimization"
            )

            logger.info(f"ìœ ì „ì ì§„í™” ì™„ë£Œ: {evolution_result.final_fitness:.2f} ì í•©ë„")

            return evolution_result

        except Exception as e:
            logger.error(f"ìœ ì „ì ì§„í™” ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None

    async def _execute_meta_coding(self, session: EvolutionSession) -> Optional[Any]:
        """ë©”íƒ€ ì½”ë”© ì‹¤í–‰"""
        try:
            # ëŒ€ìƒ ëª¨ë“ˆ ë¶„ì„
            target_modules = await self._identify_target_modules()

            refactoring_results = []

            for module_path in target_modules[:2]:  # ìƒìœ„ 2ê°œë§Œ ì²˜ë¦¬
                # ì½”ë“œ ë¶„ì„
                analysis = await self.meta_coder.parse_module(module_path)

                if analysis.complexity_score > 0.5:
                    # ë¦¬íŒ©í† ë§ ì œì•ˆ ìƒì„±
                    refactor_proposal = await self.meta_coder.refactor_code(
                        analysis.ast_tree, goal="reduce_complexity"
                    )

                    if refactor_proposal.expected_impact > 0.2:
                        # ê²€ì¦ í›„ ì ìš©
                        refactor_result = await self.meta_coder.validate_and_apply(
                            refactor_proposal.proposed_code, test_suite=[]
                        )

                        if refactor_result.success:
                            refactoring_results.append(
                                f"{module_path}: {refactor_proposal.improvement_description}"
                            )

            logger.info(f"ë©”íƒ€ ì½”ë”© ì™„ë£Œ: {len(refactoring_results)}ê°œ ë¦¬íŒ©í† ë§")

            return {
                "refactoring_results": refactoring_results,
                "modules_processed": len(target_modules[:2]),
            }

        except Exception as e:
            logger.error(f"ë©”íƒ€ ì½”ë”© ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None

    async def _execute_learning_analysis(self, session: EvolutionSession) -> Optional[Any]:
        """í•™ìŠµ ë¶„ì„ ì‹¤í–‰"""
        try:
            # ì´ì „ ì„¸ì…˜ì˜ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ íŒ¨í„´ ë¶„ì„
            previous_session = self.evolution_sessions[-1] if self.evolution_sessions else None

            if previous_session and previous_session.success:
                # ì„±ê³µì ì¸ ì„¸ì…˜ì˜ í‰ê·  ê°œì„  ì ìˆ˜ì™€ ì‹¤í–‰ ì‹œê°„ì„ ì‚¬ìš©
                avg_improvement = previous_session.results.get("overall_improvement_score", 0.0)
                avg_time = previous_session.performance_metrics.get("parallel_execution_time", 0.0)

                # ìƒˆë¡œìš´ íŒ¨í„´ ìƒì„±
                pattern_id = f"pattern_{int(time.time() * 1000)}"
                new_pattern = LearningPattern(
                    pattern_id=pattern_id,
                    trigger_type=previous_session.stimulus_event.trigger_type,
                    success_rate=1.0,  # ì„±ê³µì ì¸ ì„¸ì…˜ì€ 100% ì„±ê³µ
                    average_improvement=avg_improvement,
                    execution_time=avg_time,
                    frequency=1,  # í˜„ì¬ ì„¸ì…˜ì€ 1íšŒ ì‹¤í–‰
                    last_used=datetime.now(),
                )
                self.learning_patterns[pattern_id] = new_pattern
                logger.info(f"ìƒˆë¡œìš´ í•™ìŠµ íŒ¨í„´ ìƒì„±: {pattern_id}")

                # ì ì‘í˜• íŠ¸ë¦¬ê±° ì—…ë°ì´íŠ¸ (ìƒˆë¡œìš´ íŒ¨í„´ì— ëŒ€í•œ ì ì‘í˜• ì„ê³„ê°’ ì„¤ì •)
                if self.performance_config["enable_adaptive_triggers"]:
                    for trigger_id, trigger in self.adaptive_triggers.items():
                        if trigger.base_trigger == previous_session.stimulus_event.trigger_type:
                            trigger.adaptive_threshold = (
                                new_pattern.average_improvement * 1.5
                            )  # í‰ê·  ê°œì„  ì ìˆ˜ì˜ 1.5ë°°ë¡œ ì„¤ì •
                            logger.info(
                                f"ì ì‘í˜• íŠ¸ë¦¬ê±° '{trigger_id}' ì—…ë°ì´íŠ¸: ì„ê³„ê°’ {trigger.adaptive_threshold}"
                            )

            return {
                "learning_patterns_updated": len(self.learning_patterns),
                "adaptive_triggers_updated": len(self.adaptive_triggers),
            }

        except Exception as e:
            logger.error(f"í•™ìŠµ ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None

    async def _execute_adaptive_optimization(self, session: EvolutionSession) -> Optional[Any]:
        """ì ì‘í˜• ìµœì í™” ì‹¤í–‰"""
        try:
            # ì´ì „ ì„¸ì…˜ì˜ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ì ì‘í˜• íŠ¸ë¦¬ê±° ì—…ë°ì´íŠ¸
            previous_session = self.evolution_sessions[-1] if self.evolution_sessions else None

            if previous_session and previous_session.success:
                # ì„±ê³µì ì¸ ì„¸ì…˜ì˜ í‰ê·  ê°œì„  ì ìˆ˜ì™€ ì‹¤í–‰ ì‹œê°„ì„ ì‚¬ìš©
                avg_improvement = previous_session.results.get("overall_improvement_score", 0.0)
                avg_time = previous_session.performance_metrics.get("parallel_execution_time", 0.0)

                # ì ì‘í˜• íŠ¸ë¦¬ê±° ì—…ë°ì´íŠ¸
                for trigger_id, trigger in self.adaptive_triggers.items():
                    if trigger.base_trigger == previous_session.stimulus_event.trigger_type:
                        # ì„±ê³µì ì¸ ì„¸ì…˜ì—ì„œ í‰ê·  ê°œì„  ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ì ì‘í˜• ì„ê³„ê°’ ì¦ê°€
                        trigger.adaptive_threshold = min(
                            trigger.adaptive_threshold * 1.1, 1.0
                        )  # ìµœëŒ€ 1.0ê¹Œì§€ ì¦ê°€
                        logger.info(
                            f"ì ì‘í˜• íŠ¸ë¦¬ê±° '{trigger_id}' ì—…ë°ì´íŠ¸: ì„ê³„ê°’ {trigger.adaptive_threshold}"
                        )

            return {"adaptive_triggers_updated": len(self.adaptive_triggers)}

        except Exception as e:
            logger.error(f"ì ì‘í˜• ìµœì í™” ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None

    async def _execute_sequential_evolution(self, session: EvolutionSession) -> Dict[str, Any]:
        """ìˆœì°¨ ì§„í™” ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬ ë¹„í™œì„±í™” ì‹œ)"""
        results = {}

        logger.info("ğŸ”„ ìˆœì°¨ ì§„í™” ì‹¤í–‰ ì‹œì‘")
        start_time = time.time()

        # 1. Phase Z: ì‚¬ê³  íë¦„ ì‹¤í–‰
        session.phases.append(EvolutionPhase.REFLECTION_ANALYSIS)
        results["thought_flow"] = await self._execute_thought_flow(session)

        # 2. Phase Î©: ìƒì¡´ ë³¸ëŠ¥ ê¸°ë°˜ ëª©í‘œ ìƒì„±
        session.phases.append(EvolutionPhase.GOAL_GENERATION)
        results["phase_omega"] = await self._execute_phase_omega(session)

        # 3. ìê°€ ìˆ˜ì • ì‹¤í–‰
        if self.evolution_config["enable_self_rewriting"]:
            session.phases.append(EvolutionPhase.SELF_MODIFICATION)
            results["self_rewriting"] = await self._execute_self_rewriting(session)

        # 4. ìœ ì „ì ì§„í™” ì‹¤í–‰
        if self.evolution_config["enable_genetic_evolution"]:
            session.phases.append(EvolutionPhase.EVOLUTION_EXECUTION)
            results["genetic_evolution"] = await self._execute_genetic_evolution(session)

        # 5. ë©”íƒ€ ì½”ë”© ì‹¤í–‰
        if self.evolution_config["enable_meta_coding"]:
            session.phases.append(EvolutionPhase.EVOLUTION_EXECUTION)
            results["meta_coding"] = await self._execute_meta_coding(session)

        # 6. í•™ìŠµ ë¶„ì„ ì‹¤í–‰
        if self.performance_config["enable_learning_based_evolution"]:
            session.phases.append(EvolutionPhase.LEARNING_ANALYSIS)
            results["learning_analysis"] = await self._execute_learning_analysis(session)

        # 7. ì ì‘í˜• ìµœì í™” ì‹¤í–‰
        if self.performance_config["enable_adaptive_triggers"]:
            session.phases.append(EvolutionPhase.ADAPTIVE_OPTIMIZATION)
            results["adaptive_optimization"] = await self._execute_adaptive_optimization(session)

        execution_time = time.time() - start_time
        session.performance_metrics["sequential_execution_time"] = execution_time
        logger.info(f"ğŸš€ ìˆœì°¨ ì§„í™” ì™„ë£Œ: {execution_time:.3f}ì´ˆ")

        return results

    async def _identify_target_modules(self) -> List[str]:
        """ëŒ€ìƒ ëª¨ë“ˆ ì‹ë³„"""
        try:
            # í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ Python íŒŒì¼ë“¤ ìˆ˜ì§‘
            import os

            target_modules = []

            for root, dirs, files in os.walk("."):
                for file in files:
                    if file.endswith(".py") and not file.startswith("__"):
                        file_path = os.path.join(root, file)
                        target_modules.append(file_path)

            # ë³µì¡ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ê°„ë‹¨í•œ ì¶”ì •)
            def estimate_complexity(file_path):
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read()
                        lines = len(content.split("\n"))
                        functions = content.count("def ")
                        classes = content.count("class ")
                        return lines + functions * 10 + classes * 20
                except:
                    return 0

            target_modules.sort(key=estimate_complexity, reverse=True)

            return target_modules[:10]  # ìƒìœ„ 10ê°œë§Œ ë°˜í™˜

        except Exception as e:
            logger.error(f"ëŒ€ìƒ ëª¨ë“ˆ ì‹ë³„ ì‹¤íŒ¨: {e}")
            return []

    async def _generate_seed_code(self) -> str:
        """ì‹œë“œ ì½”ë“œ ìƒì„±"""
        return """
def optimize_performance():
    \"\"\"ì„±ëŠ¥ ìµœì í™” í•¨ìˆ˜\"\"\"
    # ê¸°ë³¸ ìµœì í™” ë¡œì§
    return "optimized"
"""

    async def _integrate_evolution_results(
        self, session: EvolutionSession, results: Dict[str, Any]
    ) -> IntegratedEvolutionResult:
        """ì§„í™” ê²°ê³¼ í†µí•©"""
        try:
            # ê°œì„  ì ìˆ˜ ê³„ì‚°
            improvement_score = await self._calculate_improvement_score(results)

            # í†µí•© ê²°ê³¼ ìƒì„±
            integrated_result = IntegratedEvolutionResult(
                session_id=session.session_id,
                stimulus_event=session.stimulus_event,
                thought_flow_result=results.get("thought_flow"),
                phase_omega_result=results.get("phase_omega"),
                self_rewriting_result=results.get("self_rewriting"),
                genetic_evolution_result=results.get("genetic_evolution"),
                meta_coding_result=results.get("meta_coding"),
                overall_improvement_score=improvement_score,
                success=True,
            )

            # í•™ìŠµ ì¸ì‚¬ì´íŠ¸ ì¶”ê°€
            if self.performance_config["enable_learning_based_evolution"]:
                integrated_result.learning_insights = await self._extract_learning_insights(results)

            # ì ì‘í˜• ë³€ê²½ì‚¬í•­ ì¶”ê°€
            if self.performance_config["enable_adaptive_triggers"]:
                integrated_result.adaptive_changes = await self._extract_adaptive_changes(results)

            # ì„¸ì…˜ ê²°ê³¼ ì—…ë°ì´íŠ¸
            session.results = results
            session.results["overall_improvement_score"] = improvement_score
            session.success = True
            session.end_time = datetime.now()

            logger.info(f"ì§„í™” ê²°ê³¼ í†µí•© ì™„ë£Œ: ê°œì„ ì ìˆ˜ {improvement_score:.3f}")

            return integrated_result

        except Exception as e:
            logger.error(f"ì§„í™” ê²°ê³¼ í†µí•© ì‹¤íŒ¨: {e}")
            return await self._create_failed_result(session.stimulus_event, str(e))

    async def _calculate_improvement_score(self, results: Dict[str, Any]) -> float:
        """ê°œì„  ì ìˆ˜ ê³„ì‚°"""
        try:
            scores = []

            # Phase Z ê²°ê³¼
            if results.get("thought_flow"):
                thought_score = getattr(results["thought_flow"], "reflection_score", 0.5)
                scores.append(thought_score * 0.2)

            # Phase Î© ê²°ê³¼
            if results.get("phase_omega"):
                phase_omega_score = getattr(results["phase_omega"], "survival_score", 0.5)
                if hasattr(phase_omega_score, "overall_score"):
                    scores.append(phase_omega_score.overall_score * 0.2)
                else:
                    scores.append(0.5 * 0.2)

            # ìê°€ ìˆ˜ì • ê²°ê³¼
            if results.get("self_rewriting"):
                self_rewriting_data = results["self_rewriting"]
                if (
                    isinstance(self_rewriting_data, dict)
                    and "improvements_made" in self_rewriting_data
                ):
                    improvement_count = len(self_rewriting_data["improvements_made"])
                    scores.append(min(improvement_count * 0.1, 0.2))
                else:
                    scores.append(0.1)

            # ìœ ì „ì ì§„í™” ê²°ê³¼
            if results.get("genetic_evolution"):
                genetic_score = getattr(results["genetic_evolution"], "final_fitness", 0.5)
                scores.append(genetic_score * 0.2)

            # ë©”íƒ€ ì½”ë”© ê²°ê³¼
            if results.get("meta_coding"):
                meta_coding_data = results["meta_coding"]
                if isinstance(meta_coding_data, dict) and "refactoring_results" in meta_coding_data:
                    refactoring_count = len(meta_coding_data["refactoring_results"])
                    scores.append(min(refactoring_count * 0.1, 0.2))
                else:
                    scores.append(0.1)

            # í•™ìŠµ ë¶„ì„ ê²°ê³¼
            if results.get("learning_analysis"):
                scores.append(0.1)  # í•™ìŠµ ë¶„ì„ì´ ì‹¤í–‰ë˜ë©´ ê¸°ë³¸ ì ìˆ˜

            # ì ì‘í˜• ìµœì í™” ê²°ê³¼
            if results.get("adaptive_optimization"):
                scores.append(0.1)  # ì ì‘í˜• ìµœì í™”ê°€ ì‹¤í–‰ë˜ë©´ ê¸°ë³¸ ì ìˆ˜

            # í‰ê·  ì ìˆ˜ ê³„ì‚°
            if scores:
                return sum(scores) / len(scores)
            else:
                return 0.5  # ê¸°ë³¸ ì ìˆ˜

        except Exception as e:
            logger.error(f"ê°œì„  ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5

    async def _extract_learning_insights(self, results: Dict[str, Any]) -> List[str]:
        """í•™ìŠµ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        insights = []

        try:
            # Phase Z ì¸ì‚¬ì´íŠ¸
            if results.get("thought_flow"):
                insights.append("ì‚¬ê³  íë¦„ ë¶„ì„ì„ í†µí•œ ìê°€ ë°˜ì„± ê°•í™”")

            # Phase Î© ì¸ì‚¬ì´íŠ¸
            if results.get("phase_omega"):
                insights.append("ìƒì¡´ ë³¸ëŠ¥ ê¸°ë°˜ ëª©í‘œ ìƒì„± ë° ì§„í™”")

            # ìê°€ ìˆ˜ì • ì¸ì‚¬ì´íŠ¸
            if results.get("self_rewriting"):
                self_rewriting_data = results["self_rewriting"]
                if (
                    isinstance(self_rewriting_data, dict)
                    and "improvements_made" in self_rewriting_data
                ):
                    insights.append(
                        f"ìê°€ ìˆ˜ì •ì„ í†µí•œ {len(self_rewriting_data['improvements_made'])}ê°œ ê°œì„  ì™„ë£Œ"
                    )

            # ìœ ì „ì ì§„í™” ì¸ì‚¬ì´íŠ¸
            if results.get("genetic_evolution"):
                insights.append("ìœ ì „ì ì§„í™”ë¥¼ í†µí•œ ì„±ëŠ¥ ìµœì í™”")

            # ë©”íƒ€ ì½”ë”© ì¸ì‚¬ì´íŠ¸
            if results.get("meta_coding"):
                meta_coding_data = results["meta_coding"]
                if isinstance(meta_coding_data, dict) and "refactoring_results" in meta_coding_data:
                    insights.append(
                        f"ë©”íƒ€ ì½”ë”©ì„ í†µí•œ {len(meta_coding_data['refactoring_results'])}ê°œ ë¦¬íŒ©í† ë§ ì™„ë£Œ"
                    )

        except Exception as e:
            logger.error(f"í•™ìŠµ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        return insights

    async def _extract_adaptive_changes(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ì ì‘í˜• ë³€ê²½ì‚¬í•­ ì¶”ì¶œ"""
        changes = {}

        try:
            # ì ì‘í˜• íŠ¸ë¦¬ê±° ì—…ë°ì´íŠ¸ ì •ë³´
            if results.get("adaptive_optimization"):
                adaptive_data = results["adaptive_optimization"]
                if isinstance(adaptive_data, dict) and "adaptive_triggers_updated" in adaptive_data:
                    changes["adaptive_triggers_updated"] = adaptive_data[
                        "adaptive_triggers_updated"
                    ]

            # í•™ìŠµ íŒ¨í„´ ì—…ë°ì´íŠ¸ ì •ë³´
            if results.get("learning_analysis"):
                learning_data = results["learning_analysis"]
                if isinstance(learning_data, dict) and "learning_patterns_updated" in learning_data:
                    changes["learning_patterns_updated"] = learning_data[
                        "learning_patterns_updated"
                    ]

        except Exception as e:
            logger.error(f"ì ì‘í˜• ë³€ê²½ì‚¬í•­ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        return changes

    async def _create_minimal_result(
        self, stimulus_event: StimulusEvent, session: EvolutionSession
    ) -> IntegratedEvolutionResult:
        """ìµœì†Œ ê²°ê³¼ ìƒì„±"""
        session.success = True
        session.end_time = datetime.now()

        return IntegratedEvolutionResult(
            session_id=session.session_id,
            stimulus_event=stimulus_event,
            overall_improvement_score=0.0,
            success=True,
        )

    async def _create_failed_result(
        self, stimulus_event: StimulusEvent, error_message: str
    ) -> IntegratedEvolutionResult:
        """ì‹¤íŒ¨ ê²°ê³¼ ìƒì„±"""
        return IntegratedEvolutionResult(
            session_id=f"failed_{int(time.time() * 1000)}",
            stimulus_event=stimulus_event,
            overall_improvement_score=0.0,
            success=False,
            error_message=error_message,
        )

    async def _generate_stimulus_description(
        self, trigger_type: EvolutionTrigger, input_data: Dict[str, Any]
    ) -> str:
        """ìê·¹ ì„¤ëª… ìƒì„±"""
        descriptions = {
            EvolutionTrigger.REFLECTION_SCORE_LOW: "ë‚®ì€ ë°˜ì„± ì ìˆ˜ë¡œ ì¸í•œ ì§„í™” í•„ìš”",
            EvolutionTrigger.SURVIVAL_THREAT: "ìƒì¡´ ìœ„í˜‘ìœ¼ë¡œ ì¸í•œ ê¸´ê¸‰ ì§„í™”",
            EvolutionTrigger.PERFORMANCE_DEGRADATION: "ì„±ëŠ¥ ì €í•˜ë¡œ ì¸í•œ ìµœì í™” í•„ìš”",
            EvolutionTrigger.GOAL_MISALIGNMENT: "ëª©í‘œ ë¶ˆì¼ì¹˜ë¡œ ì¸í•œ ì¬ì •ë ¬ í•„ìš”",
            EvolutionTrigger.EXTERNAL_STIMULUS: "ì™¸ë¶€ ìê·¹ìœ¼ë¡œ ì¸í•œ ì§„í™”",
            EvolutionTrigger.SELF_IMPROVEMENT_OPPORTUNITY: "ìê¸° ê°œì„  ê¸°íšŒ ë°œê²¬",
            EvolutionTrigger.LEARNING_BASED_EVOLUTION: "í•™ìŠµ ê¸°ë°˜ ì§„í™” íŠ¸ë¦¬ê±°",
            EvolutionTrigger.ADAPTIVE_TRIGGER: "ì ì‘í˜• ì§„í™” íŠ¸ë¦¬ê±°",
        }

        return descriptions.get(trigger_type, "ì•Œ ìˆ˜ ì—†ëŠ” ìê·¹")

    async def _get_adaptive_threshold(self, trigger_type: EvolutionTrigger) -> float:
        """ì ì‘í˜• íŠ¸ë¦¬ê±°ì˜ ì„ê³„ê°’ì„ ë°˜í™˜"""
        for trigger_id, trigger in self.adaptive_triggers.items():
            if trigger.base_trigger == trigger_type:
                return trigger.adaptive_threshold
        return self.evolution_config["adaptive_threshold"]  # ê¸°ë³¸ ì„ê³„ê°’

    async def _calculate_learning_based_intensity(
        self, trigger_type: EvolutionTrigger, context: Dict[str, Any]
    ) -> float:
        """í•™ìŠµ ê¸°ë°˜ ê°•ë„ ê³„ì‚°"""
        try:
            # í•™ìŠµ íŒ¨í„´ ê¸°ë°˜ ê°•ë„ ê³„ì‚°
            if trigger_type in self.learning_patterns:
                pattern = self.learning_patterns[trigger_type]
                return pattern.average_improvement * pattern.success_rate
            return 0.0
        except Exception as e:
            logger.error(f"í•™ìŠµ ê¸°ë°˜ ê°•ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    async def _calculate_adaptive_intensity(
        self, trigger_type: EvolutionTrigger, context: Dict[str, Any]
    ) -> float:
        """ì ì‘í˜• ê°•ë„ ê³„ì‚°"""
        try:
            # ì ì‘í˜• íŠ¸ë¦¬ê±° ê¸°ë°˜ ê°•ë„ ê³„ì‚°
            if trigger_type in self.adaptive_triggers:
                trigger = self.adaptive_triggers[trigger_type]
                return trigger.adaptive_threshold * (1.0 + len(trigger.success_history) * 0.1)
            return 0.0
        except Exception as e:
            logger.error(f"ì ì‘í˜• ê°•ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    async def _create_enhanced_stimulus_event(
        self, input_data: Dict[str, Any], context: Dict[str, Any]
    ) -> StimulusEvent:
        """í–¥ìƒëœ ìê·¹ ì´ë²¤íŠ¸ ìƒì„± (í•™ìŠµ íŒ¨í„´ ë¶„ì„ í¬í•¨)"""
        event_id = f"stimulus_{int(time.time() * 1000)}"

        # ìê·¹ ìœ í˜• ë° ê°•ë„ ë¶„ì„
        trigger_type, intensity = await self._analyze_enhanced_stimulus(input_data, context)

        # í•™ìŠµ íŒ¨í„´ ë¶„ì„
        learning_pattern = await self._analyze_learning_pattern(trigger_type, input_data, context)

        description = await self._generate_stimulus_description(trigger_type, input_data)

        return StimulusEvent(
            event_id=event_id,
            trigger_type=trigger_type,
            input_data=input_data,
            context=context,
            intensity=intensity,
            description=description,
            learning_pattern=learning_pattern,
        )

    async def _analyze_enhanced_stimulus(
        self, input_data: Dict[str, Any], context: Dict[str, Any]
    ) -> Tuple[EvolutionTrigger, float]:
        """í–¥ìƒëœ ìê·¹ ë¶„ì„ (ì ì‘í˜• íŠ¸ë¦¬ê±° í¬í•¨)"""
        intensity = 0.0
        trigger_type = EvolutionTrigger.EXTERNAL_STIMULUS

        # ê¸°ì¡´ ë¶„ì„ ë¡œì§
        if "reflection_score" in context:
            reflection_score = context["reflection_score"]
            if reflection_score < 0.3:  # ì„ê³„ê°’
                trigger_type = EvolutionTrigger.REFLECTION_SCORE_LOW
                intensity = 1.0 - reflection_score

        if "survival_status" in context:
            survival_status = context["survival_status"]
            if hasattr(survival_status, "threat_level") and survival_status.threat_level > 0.5:
                trigger_type = EvolutionTrigger.SURVIVAL_THREAT
                intensity = max(intensity, survival_status.threat_level)

        if "performance_metrics" in context:
            performance_metrics = context["performance_metrics"]
            if "degradation_score" in performance_metrics:
                degradation_score = performance_metrics["degradation_score"]
                if degradation_score > 0.1:
                    trigger_type = EvolutionTrigger.PERFORMANCE_DEGRADATION
                    intensity = max(intensity, degradation_score)

        # í•™ìŠµ ê¸°ë°˜ ì§„í™” íŠ¸ë¦¬ê±°
        learning_intensity = await self._calculate_learning_based_intensity(trigger_type, context)
        if learning_intensity > 0.3:
            trigger_type = EvolutionTrigger.LEARNING_BASED_EVOLUTION
            intensity = max(intensity, learning_intensity)

        # ì ì‘í˜• íŠ¸ë¦¬ê±°
        adaptive_intensity = await self._calculate_adaptive_intensity(trigger_type, context)
        if adaptive_intensity > 0.5:
            trigger_type = EvolutionTrigger.ADAPTIVE_TRIGGER
            intensity = max(intensity, adaptive_intensity)

        return trigger_type, intensity

    async def _analyze_learning_pattern(
        self,
        trigger_type: EvolutionTrigger,
        input_data: Dict[str, Any],
        context: Dict[str, Any],
    ) -> Optional[Dict[str, Any]]:
        """í•™ìŠµ íŒ¨í„´ ë¶„ì„"""
        try:
            # ê¸°ì¡´ í•™ìŠµ íŒ¨í„´ ì¤‘ ìœ ì‚¬í•œ íŒ¨í„´ ì°¾ê¸°
            similar_patterns = []
            for pattern in self.learning_patterns.values():
                if pattern.trigger_type == trigger_type:
                    # ì‹œê°„ ê°€ì¤‘ì¹˜ ì ìš© (ìµœê·¼ íŒ¨í„´ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)
                    time_weight = 1.0 / (1.0 + (datetime.now() - pattern.last_used).days)
                    weighted_score = pattern.average_improvement * time_weight
                    similar_patterns.append(
                        {
                            "pattern_id": pattern.pattern_id,
                            "weighted_score": weighted_score,
                            "frequency": pattern.frequency,
                            "success_rate": pattern.success_rate,
                        }
                    )

            if similar_patterns:
                # ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜ ì ìˆ˜ë¥¼ ê°€ì§„ íŒ¨í„´ ì„ íƒ
                best_pattern = max(similar_patterns, key=lambda x: x["weighted_score"])
                return {
                    "pattern_id": best_pattern["pattern_id"],
                    "expected_improvement": best_pattern["weighted_score"],
                    "confidence": best_pattern["success_rate"],
                    "frequency": best_pattern["frequency"],
                }

            return None

        except Exception as e:
            logger.error(f"í•™ìŠµ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None

    async def _start_evolution_session(self, stimulus_event: StimulusEvent) -> EvolutionSession:
        """ì§„í™” ì„¸ì…˜ ì‹œì‘"""
        session_id = f"evolution_{int(time.time() * 1000)}"
        session = EvolutionSession(session_id=session_id, stimulus_event=stimulus_event)

        self.evolution_sessions.append(session)

        logger.info(f"ì§„í™” ì„¸ì…˜ ì‹œì‘: {session_id} - {stimulus_event.trigger_type.value}")

        return session

    async def _generate_stimulus_description(
        self, trigger_type: EvolutionTrigger, input_data: Dict[str, Any]
    ) -> str:
        """ìê·¹ ì„¤ëª… ìƒì„±"""
        descriptions = {
            EvolutionTrigger.REFLECTION_SCORE_LOW: "ë‚®ì€ ë°˜ì„± ì ìˆ˜ë¡œ ì¸í•œ ì§„í™” í•„ìš”",
            EvolutionTrigger.SURVIVAL_THREAT: "ìƒì¡´ ìœ„í˜‘ìœ¼ë¡œ ì¸í•œ ê¸´ê¸‰ ì§„í™”",
            EvolutionTrigger.PERFORMANCE_DEGRADATION: "ì„±ëŠ¥ ì €í•˜ë¡œ ì¸í•œ ìµœì í™” í•„ìš”",
            EvolutionTrigger.GOAL_MISALIGNMENT: "ëª©í‘œ ë¶ˆì¼ì¹˜ë¡œ ì¸í•œ ì¬ì •ë ¬ í•„ìš”",
            EvolutionTrigger.EXTERNAL_STIMULUS: "ì™¸ë¶€ ìê·¹ìœ¼ë¡œ ì¸í•œ ì§„í™”",
            EvolutionTrigger.SELF_IMPROVEMENT_OPPORTUNITY: "ìê¸° ê°œì„  ê¸°íšŒ ë°œê²¬",
            EvolutionTrigger.LEARNING_BASED_EVOLUTION: "í•™ìŠµ ê¸°ë°˜ ì§„í™” íŠ¸ë¦¬ê±°",
            EvolutionTrigger.ADAPTIVE_TRIGGER: "ì ì‘í˜• ì§„í™” íŠ¸ë¦¬ê±°",
        }

        return descriptions.get(trigger_type, "ì•Œ ìˆ˜ ì—†ëŠ” ìê·¹")

    async def _calculate_learning_based_intensity(
        self, trigger_type: EvolutionTrigger, context: Dict[str, Any]
    ) -> float:
        """í•™ìŠµ ê¸°ë°˜ ê°•ë„ ê³„ì‚°"""
        try:
            # í•™ìŠµ íŒ¨í„´ ê¸°ë°˜ ê°•ë„ ê³„ì‚°
            if trigger_type in self.learning_patterns:
                pattern = self.learning_patterns[trigger_type]
                return pattern.average_improvement * pattern.success_rate
            return 0.0
        except Exception as e:
            logger.error(f"í•™ìŠµ ê¸°ë°˜ ê°•ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    async def _calculate_adaptive_intensity(
        self, trigger_type: EvolutionTrigger, context: Dict[str, Any]
    ) -> float:
        """ì ì‘í˜• ê°•ë„ ê³„ì‚°"""
        try:
            # ì ì‘í˜• íŠ¸ë¦¬ê±° ê¸°ë°˜ ê°•ë„ ê³„ì‚°
            if trigger_type in self.adaptive_triggers:
                trigger = self.adaptive_triggers[trigger_type]
                return trigger.adaptive_threshold * (1.0 + len(trigger.success_history) * 0.1)
            return 0.0
        except Exception as e:
            logger.error(f"ì ì‘í˜• ê°•ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    async def _update_learning_patterns(
        self,
        stimulus_event: StimulusEvent,
        integrated_result: IntegratedEvolutionResult,
    ):
        """í•™ìŠµ íŒ¨í„´ ì—…ë°ì´íŠ¸"""
        if not self.performance_config["enable_learning_based_evolution"]:
            return

        try:
            # ìƒˆë¡œìš´ íŒ¨í„´ ìƒì„±
            pattern_id = f"pattern_{int(time.time() * 1000)}"
            new_pattern = LearningPattern(
                pattern_id=pattern_id,
                trigger_type=stimulus_event.trigger_type,
                success_rate=1.0 if integrated_result.success else 0.0,
                average_improvement=integrated_result.overall_improvement_score,
                execution_time=integrated_result.evolution_time,
                frequency=1,
                last_used=datetime.now(),
            )
            self.learning_patterns[pattern_id] = new_pattern
            logger.info(f"ìƒˆë¡œìš´ í•™ìŠµ íŒ¨í„´ ìƒì„±: {pattern_id}")

            # ê¸°ì¡´ íŒ¨í„´ ì—…ë°ì´íŠ¸
            for pattern in self.learning_patterns.values():
                if pattern.trigger_type == stimulus_event.trigger_type:
                    pattern.frequency += 1
                    pattern.last_used = datetime.now()
                    pattern.average_improvement = (
                        pattern.average_improvement + integrated_result.overall_improvement_score
                    ) / 2

        except Exception as e:
            logger.error(f"í•™ìŠµ íŒ¨í„´ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    async def _update_adaptive_triggers(
        self,
        stimulus_event: StimulusEvent,
        integrated_result: IntegratedEvolutionResult,
    ):
        """ì ì‘í˜• íŠ¸ë¦¬ê±° ì—…ë°ì´íŠ¸"""
        if not self.performance_config["enable_adaptive_triggers"]:
            return

        try:
            # ì ì‘í˜• íŠ¸ë¦¬ê±°ê°€ ì—†ìœ¼ë©´ ìƒì„±
            trigger_id = f"adaptive_{stimulus_event.trigger_type.value}_{int(time.time() * 1000)}"
            if trigger_id not in self.adaptive_triggers:
                self.adaptive_triggers[trigger_id] = AdaptiveTrigger(
                    trigger_id=trigger_id,
                    base_trigger=stimulus_event.trigger_type,
                    adaptive_threshold=self.evolution_config["adaptive_threshold"],
                    environmental_factors={},
                    success_history=[],
                )

            # ì„±ê³µ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            trigger = self.adaptive_triggers[trigger_id]
            trigger.success_history.append(integrated_result.overall_improvement_score)

            # ìµœê·¼ 10ê°œ ê²°ê³¼ë§Œ ìœ ì§€
            if len(trigger.success_history) > 10:
                trigger.success_history = trigger.success_history[-10:]

            # ì ì‘í˜• ì„ê³„ê°’ ì—…ë°ì´íŠ¸
            if integrated_result.success and integrated_result.overall_improvement_score > 0.5:
                trigger.adaptive_threshold = min(trigger.adaptive_threshold * 1.1, 1.0)
            elif integrated_result.overall_improvement_score < 0.3:
                trigger.adaptive_threshold = max(trigger.adaptive_threshold * 0.9, 0.1)

            logger.info(
                f"ì ì‘í˜• íŠ¸ë¦¬ê±° '{trigger_id}' ì—…ë°ì´íŠ¸: ì„ê³„ê°’ {trigger.adaptive_threshold:.3f}"
            )

        except Exception as e:
            logger.error(f"ì ì‘í˜• íŠ¸ë¦¬ê±° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    async def _update_performance_metrics(self, execution_time: float, improvement_score: float):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)"""
        try:
            # ê¸°ë³¸ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.performance_metrics["total_tasks"] += 1
            self.performance_metrics["average_execution_time"] = (
                self.performance_metrics["average_execution_time"]
                * (self.performance_metrics["total_tasks"] - 1)
                + execution_time
            ) / self.performance_metrics["total_tasks"]

            # ì„±ê³µ/ì‹¤íŒ¨ ì„¸ì…˜ ì—…ë°ì´íŠ¸
            if improvement_score > 0:
                self.performance_metrics["completed_tasks"] += 1
            else:
                self.performance_metrics["failed_tasks"] += 1

            # ìºì‹œ íˆíŠ¸ìœ¨ ì—…ë°ì´íŠ¸
            total_cache_requests = (
                self.performance_metrics["cache_hits"] + self.performance_metrics["cache_misses"]
            )
            if total_cache_requests > 0:
                self.performance_metrics["cache_hit_rate"] = (
                    self.performance_metrics["cache_hits"] / total_cache_requests
                )

            # ì„±ëŠ¥ ê°œì„  ì ìˆ˜ ì—…ë°ì´íŠ¸
            if self.baseline_execution_time > 0:
                improvement_ratio = (
                    self.baseline_execution_time - execution_time
                ) / self.baseline_execution_time
                current_improvement = self.performance_metrics["performance_improvement"]
                total_sessions = self.performance_metrics["total_tasks"]
                self.performance_metrics["performance_improvement"] = (
                    current_improvement * (total_sessions - 1) + improvement_ratio
                ) / total_sessions

            # ë³‘ë ¬ íš¨ìœ¨ì„± ì—…ë°ì´íŠ¸
            if execution_time > 0:
                parallel_efficiency = execution_time / (
                    execution_time * len(self.node_status)
                )  # ê°„ë‹¨í•œ íš¨ìœ¨ì„± ê³„ì‚°
                current_efficiency = self.performance_metrics["parallel_efficiency"]
                total_sessions = self.performance_metrics["total_tasks"]
                self.performance_metrics["parallel_efficiency"] = (
                    current_efficiency * (total_sessions - 1) + parallel_efficiency
                ) / total_sessions

            logger.debug(
                f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸: ì‹¤í–‰ì‹œê°„={execution_time:.3f}ì´ˆ, ê°œì„ ì ìˆ˜={improvement_score:.3f}"
            )

        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    async def get_evolution_summary(self) -> Dict[str, Any]:
        """ì§„í™” ì‹œìŠ¤í…œ ìš”ì•½ ì •ë³´ ë°˜í™˜ (ì„±ëŠ¥ ìµœì í™” í†µí•© ë²„ì „)"""
        try:
            # ê¸°ë³¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­
            total_sessions = len(self.evolution_sessions)
            successful_sessions = len([s for s in self.evolution_sessions if s.success])
            failed_sessions = total_sessions - successful_sessions

            # ìºì‹œ í†µê³„
            cache_stats = self.get_cache_stats()

            # ë…¸ë“œ ìƒíƒœ
            node_status = {}
            for node_name, node_info in self.node_status.items():
                node_status[node_name] = {
                    "status": node_info["status"],
                    "response_time": node_info["response_time"],
                    "load": node_info["load"],
                }

            # ì„±ëŠ¥ ê°œì„ ë¥  ê³„ì‚°
            current_execution_time = self.performance_metrics["average_execution_time"]
            improvement_ratio = 0.0
            if self.baseline_execution_time > 0:
                improvement_ratio = (
                    (self.baseline_execution_time - current_execution_time)
                    / self.baseline_execution_time
                    * 100
                )

            summary = {
                "system_status": "active",
                "total_sessions": total_sessions,
                "successful_sessions": successful_sessions,
                "failed_sessions": failed_sessions,
                "success_rate": successful_sessions / max(total_sessions, 1) * 100,
                "average_execution_time": current_execution_time,
                "baseline_execution_time": self.baseline_execution_time,
                "target_execution_time": self.target_execution_time,
                "performance_improvement": improvement_ratio,
                "cache_stats": cache_stats,
                "node_status": node_status,
                "performance_metrics": self.performance_metrics,
                "learning_patterns_count": len(self.learning_patterns),
                "adaptive_triggers_count": len(self.adaptive_triggers),
                "integrated_systems": {
                    "enhanced_parallel_processor": self.enhanced_parallel_processor is not None,
                    "performance_optimizer": self.performance_optimizer is not None,
                    "act_r_parallel_processor": self.act_r_parallel_processor is not None,
                },
            }

            return summary

        except Exception as e:
            logger.error(f"ì§„í™” ì‹œìŠ¤í…œ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜ (ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©)"""
        try:
            total_cache_requests = (
                self.performance_metrics["cache_hits"] + self.performance_metrics["cache_misses"]
            )
            cache_hit_rate = 0.0
            if total_cache_requests > 0:
                cache_hit_rate = self.performance_metrics["cache_hits"] / total_cache_requests * 100

            return {
                "cache_size": len(self.cache),
                "cache_hits": self.performance_metrics["cache_hits"],
                "cache_misses": self.performance_metrics["cache_misses"],
                "cache_hit_rate": cache_hit_rate,
                "cache_ttl": self.cache_ttl,
                "cache_max_size": self.cache_max_size,
            }
        except Exception as e:
            logger.error(f"ìºì‹œ í†µê³„ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    async def test_integrated_performance(self) -> Dict[str, Any]:
        """í†µí•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸ§ª í†µí•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")

        start_time = time.time()
        test_results = {}

        try:
            # 1. ê¸°ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            basic_test_start = time.time()
            test_input = {"test_type": "performance", "data": "test_data"}
            test_context = {"test_mode": True}

            result = await self.process_stimulus(test_input, test_context)
            basic_test_time = time.time() - basic_test_start

            test_results["basic_performance"] = {
                "execution_time": basic_test_time,
                "success": result.success,
                "improvement_score": result.overall_improvement_score,
            }

            # 2. ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            parallel_test_start = time.time()
            parallel_tasks = []
            for i in range(5):
                task = ParallelTask(
                    id=f"test_task_{i}",
                    name=f"í…ŒìŠ¤íŠ¸ ì‘ì—… {i+1}",
                    function=lambda x: {"result": f"test_result_{x}"},
                    args=(i,),
                    priority=TaskPriority.MEDIUM,
                )
                parallel_tasks.append(task)

            parallel_results = await self._execute_parallel_tasks_with_optimization(parallel_tasks)
            parallel_test_time = time.time() - parallel_test_start

            test_results["parallel_processing"] = {
                "execution_time": parallel_test_time,
                "tasks_count": len(parallel_tasks),
                "successful_tasks": len([r for r in parallel_results if r is not None]),
            }

            # 3. ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            cache_test_start = time.time()
            cache_key = self._generate_cache_key(test_input, test_context)
            cached_result = self._get_from_cache(cache_key)
            cache_test_time = time.time() - cache_test_start

            test_results["cache_performance"] = {
                "cache_lookup_time": cache_test_time,
                "cache_hit": cached_result is not None,
                "cache_stats": self.get_cache_stats(),
            }

            # 4. ë¡œë“œ ë°¸ëŸ°ì‹± í…ŒìŠ¤íŠ¸
            load_balancing_test_start = time.time()
            optimal_nodes = []
            for _ in range(5):
                optimal_node = self._get_optimal_node("test")
                optimal_nodes.append(optimal_node)
            load_balancing_test_time = time.time() - load_balancing_test_start

            test_results["load_balancing"] = {
                "node_selection_time": load_balancing_test_time,
                "selected_nodes": optimal_nodes,
                "node_status": self.node_status,
            }

            # 5. í†µí•© ì„±ëŠ¥ ë¶„ì„
            total_test_time = time.time() - start_time
            test_results["overall_performance"] = {
                "total_test_time": total_test_time,
                "average_execution_time": self.performance_metrics["average_execution_time"],
                "performance_improvement": self.performance_metrics["performance_improvement"],
                "cache_hit_rate": self.performance_metrics["cache_hit_rate"],
            }

            logger.info(f"âœ… í†µí•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {total_test_time:.3f}ì´ˆ")
            return test_results

        except Exception as e:
            logger.error(f"âŒ í†µí•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    async def optimize_system_performance(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™” ì‹¤í–‰"""
        logger.info("ğŸ”§ ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™” ì‹œì‘")

        try:
            optimization_results = {}

            # 1. ìºì‹œ ìµœì í™”
            if len(self.cache) > self.cache_max_size * 0.8:
                # ìºì‹œ ì •ë¦¬
                current_time = time.time()
                expired_keys = [
                    key
                    for key, data in self.cache.items()
                    if current_time - data["timestamp"] > self.cache_ttl
                ]
                for key in expired_keys:
                    del self.cache[key]

                optimization_results["cache_optimization"] = {
                    "cleaned_items": len(expired_keys),
                    "current_cache_size": len(self.cache),
                }

            # 2. ë…¸ë“œ ìƒíƒœ ìµœì í™”
            for node_name, node_info in self.node_status.items():
                if node_info["load"] > 0.8:
                    # ë¶€í•˜ê°€ ë†’ì€ ë…¸ë“œì˜ ë¶€í•˜ ê°ì†Œ
                    self.node_status[node_name]["load"] = max(0.0, node_info["load"] - 0.2)
                    optimization_results[f"{node_name}_optimization"] = {
                        "previous_load": node_info["load"],
                        "current_load": self.node_status[node_name]["load"],
                    }

            # 3. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìµœì í™”
            if self.performance_metrics["error_count"] > 10:
                # ì˜¤ë¥˜ê°€ ë§ì€ ê²½ìš° ì„ê³„ê°’ ì¡°ì •
                self.performance_metrics["error_count"] = max(
                    0, self.performance_metrics["error_count"] - 5
                )
                optimization_results["error_optimization"] = {"error_count_reduced": True}

            logger.info("âœ… ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ")
            return optimization_results

        except Exception as e:
            logger.error(f"âŒ ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.executor.shutdown(wait=True)
            logger.info("í†µí•© ì§„í™” ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # í†µí•© ì§„í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    evolution_system = DuRiIntegratedEvolutionSystem()

    try:
        # í…ŒìŠ¤íŠ¸ ìê·¹ ìƒì„±
        test_stimulus = {
            "test_type": "performance_optimization",
            "target_modules": ["integrated_evolution_system.py"],
            "optimization_goals": ["speed", "efficiency", "accuracy"],
        }

        test_context = {
            "reflection_score": 0.4,  # ë‚®ì€ ë°˜ì„± ì ìˆ˜
            "performance_metrics": {"degradation_score": 0.7},  # ì„±ëŠ¥ ì €í•˜
            "survival_status": {"threat_level": 0.3},
        }

        print("ğŸš€ í–¥ìƒëœ í†µí•© ì§„í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")

        # ì§„í™” ì‹¤í–‰
        result = await evolution_system.process_stimulus(test_stimulus, test_context)

        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š í–¥ìƒëœ í†µí•© ì§„í™” ê²°ê³¼:")
        print(f"âœ… ì„±ê³µ: {result.success}")
        print(f"â±ï¸  ì‹¤í–‰ ì‹œê°„: {result.evolution_time:.3f}ì´ˆ")
        print(f"ğŸ¯ ê°œì„  ì ìˆ˜: {result.overall_improvement_score:.3f}")
        print(f"ğŸ§  í•™ìŠµ ì¸ì‚¬ì´íŠ¸: {len(result.learning_insights)}ê°œ")
        print(f"ğŸ”„ ì ì‘í˜• ë³€ê²½ì‚¬í•­: {len(result.adaptive_changes)}ê°œ")

        if result.learning_insights:
            print(f"\nğŸ’¡ í•™ìŠµ ì¸ì‚¬ì´íŠ¸:")
            for insight in result.learning_insights:
                print(f"  - {insight}")

        if result.adaptive_changes:
            print(f"\nğŸ”„ ì ì‘í˜• ë³€ê²½ì‚¬í•­:")
            for change_type, change_value in result.adaptive_changes.items():
                print(f"  - {change_type}: {change_value}")

        # ì§„í™” ìš”ì•½ ì •ë³´
        summary = await evolution_system.get_evolution_summary()
        print(f"\nğŸ“‹ ì§„í™” ìš”ì•½:")
        print(f"ì´ ì„¸ì…˜: {summary['total_sessions']}")
        print(f"ì„±ê³µí•œ ì„¸ì…˜: {summary['successful_sessions']}")
        print(f"ì‹¤íŒ¨í•œ ì„¸ì…˜: {summary['failed_sessions']}")
        print(f"ì„±ê³µë¥ : {summary['success_rate']:.1%}")
        print(f"í‰ê·  ì‹¤í–‰ ì‹œê°„: {summary['average_execution_time']:.3f}ì´ˆ")
        print(f"í‰ê·  ê°œì„  ì ìˆ˜: {summary['average_improvement_score']:.3f}")
        print(f"ë³‘ë ¬ ì²˜ë¦¬ íš¨ìœ¨ì„±: {summary['parallel_processing_efficiency']:.3f}")
        print(f"ì„±ëŠ¥ ê°œì„  ì ìˆ˜: {summary['performance_improvement']:.3f}")
        print(f"ìºì‹œ íˆíŠ¸ìœ¨: {summary['cache_hit_rate']:.1%}")
        print(f"í•™ìŠµ íŒ¨í„´ ìˆ˜: {summary['learning_patterns_count']}")
        print(f"ì ì‘í˜• íŠ¸ë¦¬ê±° ìˆ˜: {summary['adaptive_triggers_count']}")

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        await evolution_system.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
