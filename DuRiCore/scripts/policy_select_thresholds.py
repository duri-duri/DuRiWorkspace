from DuRiCore.trace import emit_trace
"""
Phase 3 - ì •ì±… íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
CSV/JSON ì…ë ¥ìœ¼ë¡œë¶€í„° ROC/PR ê¸°ë°˜ ìµœì  ì„ê³„ê°’ ì‚°ì¶œ ë° ë¦¬í¬íŠ¸ ìƒì„±

@preserve_identity: ê¸°ì¡´ ê²€ì¦ ê·œì¹™ê³¼ì˜ í˜¸í™˜ì„± ë³´ì¥
@evolution_protection: ì •ì±… ì§„í™” ê³¼ì •ì—ì„œì˜ ì•ˆì „ì„± í™•ë³´
@execution_guarantee: ìµœì í™”ëœ ì„ê³„ê°’ìœ¼ë¡œ ê²€ì¦ ë³´ì¥
@existence_ai: ì•ˆì „í•œ ì •ì±… ì§„í™”ë¥¼ ìœ„í•œ ìë™í™”ëœ íŠœë‹
@final_execution: ìµœì í™”ëœ ì •ì±…ìœ¼ë¡œ ì•ˆì „í•œ ê²€ì¦ ì‹¤í–‰
"""
import argparse
import json
import csv
import yaml
import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, asdict
import numpy as np
import pandas as pd
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
try:
    from tests.policy.test_policy_tuning import PolicyTuningAnalyzer, PolicyTuningResult
except ImportError:

    @dataclass
    class ThresholdCandidate:
        threshold: float
        tpr: float
        fpr: float
        precision: float
        f1_score: float
        balanced_accuracy: float

    @dataclass
    class PolicyTuningResult:
        rule_name: str
        best_threshold: float
        best_f1_score: float
        best_balanced_accuracy: float
        candidates: List[ThresholdCandidate]
        roc_data: Dict[str, List[float]]
        pr_data: Dict[str, List[float]]

    class PolicyTuningAnalyzer:

        def __init__(self):
            self.results: List[PolicyTuningResult] = []

        def generate_sample_data(self, n_samples=1000, anomaly_ratio=0.2):
            np.random.seed(42)
            n_anomalies = int(n_samples * anomaly_ratio)
            n_normal = n_samples - n_anomalies
            normal_metrics = np.random.exponential(0.5, n_normal)
            anomaly_metrics = np.random.exponential(2.0, n_anomalies) + 3.0
            all_metrics = np.concatenate([normal_metrics, anomaly_metrics])
            labels = [False] * n_normal + [True] * n_anomalies
            indices = np.random.permutation(len(all_metrics))
            return (all_metrics[indices].tolist(), [labels[i] for i in indices])

        def calculate_metrics(self, predictions, labels):
            tp = sum((1 for (p, l) in zip(predictions, labels) if p and l))
            fp = sum((1 for (p, l) in zip(predictions, labels) if p and (not l)))
            tn = sum((1 for (p, l) in zip(predictions, labels) if not p and (not l)))
            fn = sum((1 for (p, l) in zip(predictions, labels) if not p and l))
            tpr = tp / (tp + fn) if tp + fn > 0 else 0.0
            fpr = fp / (fp + tn) if fp + tn > 0 else 0.0
            precision = tp / (tp + fp) if tp + fp > 0 else 0.0
            f1_score = 2 * precision * tpr / (precision + tpr) if precision + tpr > 0 else 0.0
            balanced_accuracy = (tpr + (1 - fpr)) / 2
            return {'tpr': tpr, 'fpr': fpr, 'precision': precision, 'f1_score': f1_score, 'balanced_accuracy': balanced_accuracy, 'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn}

        def evaluate_thresholds(self, metrics, labels, threshold_range=(0.0, 10.0), n_steps=100):
            candidates = []
            thresholds = np.linspace(threshold_range[0], threshold_range[1], n_steps)
            for threshold in thresholds:
                predictions = [m > threshold for m in metrics]
                metrics_dict = self.calculate_metrics(predictions, labels)
                candidate = ThresholdCandidate(threshold=threshold, tpr=metrics_dict['tpr'], fpr=metrics_dict['fpr'], precision=metrics_dict['precision'], f1_score=metrics_dict['f1_score'], balanced_accuracy=metrics_dict['balanced_accuracy'])
                candidates.append(candidate)
            return candidates

        def find_optimal_threshold(self, candidates, optimization_target='f1_score'):
            if optimization_target == 'f1_score':
                return max(candidates, key=lambda x: x.f1_score)
            elif optimization_target == 'balanced_accuracy':
                return max(candidates, key=lambda x: x.balanced_accuracy)
            elif optimization_target == 'tpr':
                return max(candidates, key=lambda x: x.tpr)
            else:
                raise ValueError(f'ì§€ì›í•˜ì§€ ì•ŠëŠ” ìµœì í™” ëŒ€ìƒ: {optimization_target}')

        def analyze_rule(self, rule_name, metrics, labels):
            candidates = self.evaluate_thresholds(metrics, labels)
            best_f1 = self.find_optimal_threshold(candidates, 'f1_score')
            best_balanced = self.find_optimal_threshold(candidates, 'balanced_accuracy')
            roc_data = {'fpr': [c.fpr for c in candidates], 'tpr': [c.tpr for c in candidates]}
            pr_data = {'precision': [c.precision for c in candidates], 'recall': [c.tpr for c in candidates]}
            result = PolicyTuningResult(rule_name=rule_name, best_threshold=best_f1.threshold, best_f1_score=best_f1.f1_score, best_balanced_accuracy=best_balanced.balanced_accuracy, candidates=candidates, roc_data=roc_data, pr_data=pr_data)
            self.results.append(result)
            return result

@dataclass
class ThresholdRecommendation:
    """ì„ê³„ê°’ ì¶”ì²œ ë°ì´í„° í´ë˜ìŠ¤"""
    rule_name: str
    current_threshold: float
    recommended_threshold: float
    improvement_f1: float
    improvement_balanced: float
    fpr_reduction: float
    fnr_reduction: float

class PolicyThresholdSelector:
    """ì •ì±… ì„ê³„ê°’ ì„ íƒê¸°"""

    def __init__(self, config_path: Optional[str]=None):
        """
        ì´ˆê¸°í™”
        
        Args:
            config_path: thresholds.yaml íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)
        """
        self.config_path = config_path or os.path.join(os.path.dirname(__file__), '..', 'config', 'thresholds.yaml')
        self.analyzer = PolicyTuningAnalyzer()
        self.current_thresholds = self._load_current_thresholds()

    def _load_current_thresholds(self) -> Dict[str, Any]:
        """í˜„ì¬ ì„ê³„ê°’ ì„¤ì • ë¡œë“œ"""
        try:
            if os.path.exists(self.config_path):
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                profile = os.getenv('DURI_PROFILE', 'dev')
                if profile in config.get('profiles', {}):
                    return config['profiles'][profile]
                else:
                    return config.get('defaults', {})
            else:
                emit_trace('info', ' '.join(map(str, [f'âš ï¸ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {self.config_path}'])))
                return {}
        except Exception as e:
            emit_trace('info', ' '.join(map(str, [f'âŒ ì„ê³„ê°’ ë¡œë“œ ì‹¤íŒ¨: {e}'])))
            return {}

    def load_data_from_csv(self, file_path: str, metric_column: str='metric', label_column: str='label') -> Tuple[List[float], List[bool]]:
        """CSV íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            df = pd.read_csv(file_path)
            if metric_column not in df.columns:
                raise ValueError(f"ë©”íŠ¸ë¦­ ì»¬ëŸ¼ '{metric_column}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            if label_column not in df.columns:
                raise ValueError(f"ë¼ë²¨ ì»¬ëŸ¼ '{label_column}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            metrics = df[metric_column].tolist()
            labels = df[label_column].astype(bool).tolist()
            emit_trace('info', ' '.join(map(str, [f'ğŸ“Š CSV ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(metrics)}ê°œ ìƒ˜í”Œ'])))
            return (metrics, labels)
        except Exception as e:
            emit_trace('info', ' '.join(map(str, [f'âŒ CSV ë¡œë“œ ì‹¤íŒ¨: {e}'])))
            raise

    def load_data_from_json(self, file_path: str, metric_key: str='metric', label_key: str='label') -> Tuple[List[float], List[bool]]:
        """JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if isinstance(data, list):
                metrics = [item[metric_key] for item in data]
                labels = [bool(item[label_key]) for item in data]
            elif isinstance(data, dict):
                metrics = data[metric_key]
                labels = [bool(l) for l in data[label_key]]
            else:
                raise ValueError('ì§€ì›í•˜ì§€ ì•ŠëŠ” JSON í˜•ì‹')
            emit_trace('info', ' '.join(map(str, [f'ğŸ“Š JSON ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(metrics)}ê°œ ìƒ˜í”Œ'])))
            return (metrics, labels)
        except Exception as e:
            emit_trace('info', ' '.join(map(str, [f'âŒ JSON ë¡œë“œ ì‹¤íŒ¨: {e}'])))
            raise

    def analyze_all_rules(self, metrics: List[float], labels: List[bool], rule_names: Optional[List[str]]=None) -> List[PolicyTuningResult]:
        """ëª¨ë“  ê·œì¹™ì— ëŒ€í•œ ë¶„ì„ ìˆ˜í–‰"""
        if rule_names is None:
            rule_names = ['performance_degradation', 'error_spike_detection', 'resource_exhaustion', 'latency_threshold', 'memory_usage', 'cpu_usage']
        results = []
        for rule_name in rule_names:
            emit_trace('info', ' '.join(map(str, [f'ğŸ” {rule_name} ê·œì¹™ ë¶„ì„ ì¤‘...'])))
            if 'performance' in rule_name or 'latency' in rule_name:
                rule_metrics = [m * 0.8 for m in metrics]
            elif 'error' in rule_name:
                rule_metrics = [m * 1.2 for m in metrics]
            elif 'resource' in rule_name or 'memory' in rule_name or 'cpu' in rule_name:
                rule_metrics = [m * 1.0 for m in metrics]
            else:
                rule_metrics = metrics
            result = self.analyzer.analyze_rule(rule_name, rule_metrics, labels)
            results.append(result)
            emit_trace('info', ' '.join(map(str, [f'âœ… {rule_name}: ìµœì  ì„ê³„ê°’ {result.best_threshold:.3f} (F1: {result.best_f1_score:.3f}, BA: {result.best_balanced_accuracy:.3f})'])))
        return results

    def generate_recommendations(self, results: List[PolicyTuningResult]) -> List[ThresholdRecommendation]:
        """ì„ê³„ê°’ ì¶”ì²œ ìƒì„±"""
        recommendations = []
        for result in results:
            current_threshold = self._get_current_threshold_for_rule(result.rule_name)
            improvement_f1 = result.best_f1_score - 0.5
            improvement_balanced = result.best_balanced_accuracy - 0.7
            best_candidate = max(result.candidates, key=lambda x: x.f1_score)
            fpr_reduction = 0.1 - best_candidate.fpr if best_candidate.fpr < 0.1 else 0.0
            fnr_reduction = 0.05 - (1 - best_candidate.tpr) if best_candidate.tpr > 0.95 else 0.0
            recommendation = ThresholdRecommendation(rule_name=result.rule_name, current_threshold=current_threshold, recommended_threshold=result.best_threshold, improvement_f1=improvement_f1, improvement_balanced=improvement_balanced, fpr_reduction=fpr_reduction, fnr_reduction=fnr_reduction)
            recommendations.append(recommendation)
        return recommendations

    def _get_current_threshold_for_rule(self, rule_name: str) -> float:
        """ê·œì¹™ ì´ë¦„ì— ë”°ë¥¸ í˜„ì¬ ì„ê³„ê°’ ë°˜í™˜"""
        threshold_mapping = {'performance_degradation': self.current_thresholds.get('performance_events_max', 5.0), 'error_spike_detection': self.current_thresholds.get('error_events_max', 5.0), 'resource_exhaustion': self.current_thresholds.get('resource_events_max', 5.0), 'latency_threshold': self.current_thresholds.get('p95_latency_inc_pct', 5.0), 'memory_usage': self.current_thresholds.get('memory_inc_pct', 3.0), 'cpu_usage': self.current_thresholds.get('cpu_inc_pct', 5.0)}
        return threshold_mapping.get(rule_name, 5.0)

    def save_recommendations_yaml(self, recommendations: List[ThresholdRecommendation], output_path: str) -> None:
        """ì¶”ì²œ ì„ê³„ê°’ì„ YAML í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        try:
            recommended_config = {'profiles': {'dev': {}, 'staging': {}, 'prod': {}}, 'defaults': {}, 'recommendations': {}}
            for profile in ['dev', 'staging', 'prod']:
                profile_config = {}
                for rec in recommendations:
                    if 'performance' in rec.rule_name:
                        profile_config['performance_events_max'] = int(rec.recommended_threshold)
                    elif 'error' in rec.rule_name:
                        profile_config['error_events_max'] = int(rec.recommended_threshold)
                    elif 'resource' in rec.rule_name:
                        profile_config['resource_events_max'] = int(rec.recommended_threshold)
                    elif 'latency' in rec.rule_name:
                        profile_config['p95_latency_inc_pct'] = round(rec.recommended_threshold, 1)
                    elif 'memory' in rec.rule_name:
                        profile_config['memory_inc_pct'] = round(rec.recommended_threshold, 1)
                    elif 'cpu' in rec.rule_name:
                        profile_config['cpu_inc_pct'] = round(rec.recommended_threshold, 1)
                profile_config.update(self.current_thresholds)
                recommended_config['profiles'][profile] = profile_config
            recommended_config['recommendations'] = {rec.rule_name: {'current_threshold': rec.current_threshold, 'recommended_threshold': rec.recommended_threshold, 'improvement_f1': round(rec.improvement_f1, 3), 'improvement_balanced': round(rec.improvement_balanced, 3), 'fpr_reduction': round(rec.fpr_reduction, 3), 'fnr_reduction': round(rec.fnr_reduction, 3)} for rec in recommendations}
            with open(output_path, 'w', encoding='utf-8') as f:
                yaml.dump(recommended_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
            emit_trace('info', ' '.join(map(str, [f'ğŸ’¾ ì¶”ì²œ ì„ê³„ê°’ YAML ì €ì¥ ì™„ë£Œ: {output_path}'])))
        except Exception as e:
            emit_trace('info', ' '.join(map(str, [f'âŒ YAML ì €ì¥ ì‹¤íŒ¨: {e}'])))
            raise

    def generate_report_markdown(self, results: List[PolicyTuningResult], recommendations: List[ThresholdRecommendation], output_path: str) -> None:
        """ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            report_content = f"# Phase 3 - ì •ì±… íŠœë‹ ë¦¬í¬íŠ¸\n\n## ğŸ“Š ë¶„ì„ ìš”ì•½\n\n- **ë¶„ì„ ê·œì¹™ ìˆ˜**: {len(results)}\n- **ë°ì´í„° ìƒ˜í”Œ ìˆ˜**: {(len(results[0].candidates) if results else 0)}\n- **ìƒì„± ì‹œê°„**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n## ğŸ¯ ê·œì¹™ë³„ ìµœì  ì„ê³„ê°’\n\n"
            for result in results:
                report_content += f"### {result.rule_name.replace('_', ' ').title()}\n\n- **ìµœì  ì„ê³„ê°’**: {result.best_threshold:.3f}\n- **F1 ìŠ¤ì½”ì–´**: {result.best_f1_score:.3f}\n- **ê· í˜• ì •í™•ë„**: {result.best_balanced_accuracy:.3f}\n- **TPR (Recall)**: {max((c.tpr for c in result.candidates)):.3f}\n- **ìµœì†Œ FPR**: {min((c.fpr for c in result.candidates)):.3f}\n\n"
            report_content += '## ğŸ“ˆ ì„±ëŠ¥ ê°œì„  ì¶”ì²œ\n\n| ê·œì¹™ | í˜„ì¬ ì„ê³„ê°’ | ì¶”ì²œ ì„ê³„ê°’ | F1 ê°œì„  | ê· í˜•ì •í™•ë„ ê°œì„  | FPR ê°ì†Œ | FNR ê°ì†Œ |\n|------|-------------|-------------|---------|----------------|----------|----------|\n'
            for rec in recommendations:
                report_content += f'| {rec.rule_name} | {rec.current_threshold:.3f} | {rec.recommended_threshold:.3f} | {rec.improvement_f1:+.3f} | {rec.improvement_balanced:+.3f} | {rec.fpr_reduction:+.3f} | {rec.fnr_reduction:+.3f} |\n'
            report_content += f'\n\n## ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n\n1. **ì¶”ì²œ ì„ê³„ê°’ ì ìš©**: `config/thresholds.yaml` íŒŒì¼ ì—…ë°ì´íŠ¸\n2. **í…ŒìŠ¤íŠ¸ ì‹¤í–‰**: `pytest -k policy_tuning` ìœ¼ë¡œ ê²€ì¦\n3. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: FP â‰¤ 2%, FN â‰¤ 5% ë‹¬ì„± í™•ì¸\n4. **ìš´ì˜ ì ìš©**: ë‹¨ê³„ë³„ë¡œ í”„ë¡œë•ì…˜ í™˜ê²½ì— ì ìš©\n\n## ğŸ“‹ ì„±ëŠ¥ ê¸°ì¤€\n\n- **FP (False Positive)**: â‰¤ 2%\n- **FN (False Negative)**: â‰¤ 5%\n- **F1 ìŠ¤ì½”ì–´**: â‰¥ 0.8\n- **ê· í˜• ì •í™•ë„**: â‰¥ 0.85\n\n---\n*ì´ ë¦¬í¬íŠ¸ëŠ” Phase 3 ì •ì±… íŠœë‹ ìë™í™” ë„êµ¬ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*\n'
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            emit_trace('info', ' '.join(map(str, [f'ğŸ“ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {output_path}'])))
        except Exception as e:
            emit_trace('info', ' '.join(map(str, [f'âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}'])))
            raise

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Phase 3 - ì •ì±… íŠœë‹ ìŠ¤í¬ë¦½íŠ¸', formatter_class=argparse.RawDescriptionHelpFormatter, epilog='\nì‚¬ìš© ì˜ˆì‹œ:\n  # CSV íŒŒì¼ë¡œë¶€í„° ë¶„ì„\n  python policy_select_thresholds.py --input data.csv --format csv\n  \n  # JSON íŒŒì¼ë¡œë¶€í„° ë¶„ì„  \n  python policy_select_thresholds.py --input data.json --format json\n  \n  # ì¶œë ¥ íŒŒì¼ ì§€ì •\n  python policy_select_thresholds.py --input data.csv --format csv \\\n    --output-yaml recommended_thresholds.yaml \\\n    --output-report tuning_report.md\n        ')
    parser.add_argument('--input', '-i', required=True, help='ì…ë ¥ ë°ì´í„° íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--format', '-f', choices=['csv', 'json'], required=True, help='ì…ë ¥ íŒŒì¼ í˜•ì‹')
    parser.add_argument('--output-yaml', '-y', default='recommended_thresholds.yaml', help='ì¶”ì²œ ì„ê³„ê°’ YAML ì¶œë ¥ íŒŒì¼')
    parser.add_argument('--output-report', '-r', default='policy_tuning_report.md', help='ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ì¶œë ¥ íŒŒì¼')
    parser.add_argument('--config', '-c', help='ê¸°ì¡´ thresholds.yaml ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--metric-column', default='metric', help='CSV ë©”íŠ¸ë¦­ ì»¬ëŸ¼ëª… (ê¸°ë³¸ê°’: metric)')
    parser.add_argument('--label-column', default='label', help='CSV ë¼ë²¨ ì»¬ëŸ¼ëª… (ê¸°ë³¸ê°’: label)')
    parser.add_argument('--metric-key', default='metric', help='JSON ë©”íŠ¸ë¦­ í‚¤ (ê¸°ë³¸ê°’: metric)')
    parser.add_argument('--label-key', default='label', help='JSON ë¼ë²¨ í‚¤ (ê¸°ë³¸ê°’: label)')
    args = parser.parse_args()
    try:
        emit_trace('info', ' '.join(map(str, ['ğŸš€ Phase 3 - ì •ì±… íŠœë‹ ì‹œì‘'])))
        emit_trace('info', ' '.join(map(str, ['=' * 50])))
        selector = PolicyThresholdSelector(args.config)
        if args.format == 'csv':
            (metrics, labels) = selector.load_data_from_csv(args.input, args.metric_column, args.label_column)
        else:
            (metrics, labels) = selector.load_data_from_json(args.input, args.metric_key, args.label_key)
        emit_trace('info', ' '.join(map(str, ['\nğŸ” ê·œì¹™ë³„ ì„ê³„ê°’ ë¶„ì„ ì¤‘...'])))
        results = selector.analyze_all_rules(metrics, labels)
        emit_trace('info', ' '.join(map(str, ['\nğŸ’¡ ì„ê³„ê°’ ì¶”ì²œ ìƒì„± ì¤‘...'])))
        recommendations = selector.generate_recommendations(results)
        emit_trace('info', ' '.join(map(str, ['\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...'])))
        selector.save_recommendations_yaml(recommendations, args.output_yaml)
        selector.generate_report_markdown(results, recommendations, args.output_report)
        emit_trace('info', ' '.join(map(str, ['\n' + '=' * 50])))
        emit_trace('info', ' '.join(map(str, ['ğŸ‰ ì •ì±… íŠœë‹ ì™„ë£Œ!'])))
        emit_trace('info', ' '.join(map(str, [f'ğŸ“Š ë¶„ì„ëœ ê·œì¹™: {len(results)}ê°œ'])))
        emit_trace('info', ' '.join(map(str, [f'ğŸ’¾ YAML íŒŒì¼: {args.output_yaml}'])))
        emit_trace('info', ' '.join(map(str, [f'ğŸ“ ë¦¬í¬íŠ¸: {args.output_report}'])))
        all_f1_scores = [r.best_f1_score for r in results]
        all_balanced_acc = [r.best_balanced_accuracy for r in results]
        avg_f1 = np.mean(all_f1_scores)
        avg_balanced = np.mean(all_balanced_acc)
        emit_trace('info', ' '.join(map(str, [f'\nğŸ“ˆ ì„±ëŠ¥ ìš”ì•½:'])))
        emit_trace('info', ' '.join(map(str, [f"  í‰ê·  F1 ìŠ¤ì½”ì–´: {avg_f1:.3f} {('âœ…' if avg_f1 >= 0.8 else 'âš ï¸')}"])))
        emit_trace('info', ' '.join(map(str, [f"  í‰ê·  ê· í˜• ì •í™•ë„: {avg_balanced:.3f} {('âœ…' if avg_balanced >= 0.85 else 'âš ï¸')}"])))
        if avg_f1 >= 0.8 and avg_balanced >= 0.85:
            emit_trace('info', ' '.join(map(str, ['\nğŸ¯ Phase 3 ì„±ëŠ¥ ê¸°ì¤€ ë‹¬ì„±! ìš´ì˜ ì ìš© ì¤€ë¹„ ì™„ë£Œ!'])))
        else:
            emit_trace('info', ' '.join(map(str, ['\nâš ï¸ ì¼ë¶€ ì„±ëŠ¥ ê¸°ì¤€ ë¯¸ë‹¬ì„±. ì¶”ê°€ íŠœë‹ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'])))
    except Exception as e:
        emit_trace('info', ' '.join(map(str, [f'\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}'])))
        sys.exit(1)
if __name__ == '__main__':
    main()