#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DuRi í†µí•© ì§„í™” ì‹œìŠ¤í…œ - ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ í…ŒìŠ¤íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒì„ ìœ„í•œ ìµœì í™” ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import asyncio
from datetime import datetime
import json
import logging
import time

from integrated_evolution_system import DuRiIntegratedEvolutionSystem

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


async def test_cache_optimization():
    """ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ§ª ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ í…ŒìŠ¤íŠ¸ ì‹œì‘")

    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    system = DuRiIntegratedEvolutionSystem()

    try:
        # 1. ê¸°ë³¸ ìºì‹œ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ“Š 1. ê¸°ë³¸ ìºì‹œ í…ŒìŠ¤íŠ¸ ì‹œì‘")

        test_inputs = [
            {"task": "test1", "data": "data1"},  # íƒ€ì„ìŠ¤íƒ¬í”„ ì œê±°
            {"task": "test2", "data": "data2"},  # íƒ€ì„ìŠ¤íƒ¬í”„ ì œê±°
            {"task": "test3", "data": "data3"},  # íƒ€ì„ìŠ¤íƒ¬í”„ ì œê±°
            {"task": "test1", "data": "data1"},  # ì¤‘ë³µ (ìºì‹œ íˆíŠ¸ ì˜ˆìƒ)
            {"task": "test2", "data": "data2"},  # ì¤‘ë³µ (ìºì‹œ íˆíŠ¸ ì˜ˆìƒ)
        ]

        test_context = {"test_mode": True, "performance_optimization": True}

        cache_results = []
        for i, test_input in enumerate(test_inputs):
            start_time = time.time()
            result = await system.process_stimulus(test_input, test_context)
            execution_time = time.time() - start_time

            # ìºì‹œ íˆíŠ¸ ì—¬ë¶€ í™•ì¸
            cache_key = system._generate_cache_key(test_input, test_context)
            cached_result = system._get_from_cache(cache_key)
            cache_hit = cached_result is not None

            cache_results.append(
                {
                    "test_id": i + 1,
                    "input": test_input,
                    "execution_time": execution_time,
                    "success": result.success,
                    "cache_hit": cache_hit,
                    "cache_key": cache_key,
                }
            )

            logger.info(
                f"   í…ŒìŠ¤íŠ¸ {i+1}: {execution_time:.3f}ì´ˆ, ì„±ê³µ: {result.success}, ìºì‹œíˆíŠ¸: {cache_hit}"
            )

        # 2. ìºì‹œ ì „ëµ ê°œì„  í…ŒìŠ¤íŠ¸
        logger.info("ğŸ”§ 2. ìºì‹œ ì „ëµ ê°œì„  í…ŒìŠ¤íŠ¸ ì‹œì‘")

        # ìºì‹œ ì „ëµ ê°œì„  ì‹¤í–‰
        system._improve_cache_strategy()

        # ìºì‹œ í†µê³„ í™•ì¸
        cache_stats = system.get_cache_stats()
        logger.info(f"   ìºì‹œ í¬ê¸°: {cache_stats.get('cache_size', 0)}")
        logger.info(f"   ìºì‹œ íˆíŠ¸ìœ¨: {cache_stats.get('cache_hit_rate', 0):.1%}")
        logger.info(f"   ìºì‹œ íˆíŠ¸: {cache_stats.get('cache_hits', 0)}")
        logger.info(f"   ìºì‹œ ë¯¸ìŠ¤: {cache_stats.get('cache_misses', 0)}")

        # 3. ìºì‹œ í‚¤ ìµœì í™” í…ŒìŠ¤íŠ¸
        logger.info("ğŸ”‘ 3. ìºì‹œ í‚¤ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹œì‘")

        test_cases = [
            {
                "name": "ê¸°ë³¸ í…ŒìŠ¤íŠ¸",
                "input": {"task": "basic", "data": "test_data"},
                "context": {"mode": "test"},
            },
            {
                "name": "ë³µì¡í•œ ë°ì´í„°",
                "input": {
                    "task": "complex",
                    "data": "test_data",
                    "extra": "additional_info",
                    "nested": {"key": "value"},
                },
                "context": {"mode": "test", "performance": True},
            },
            {
                "name": "ì¤‘ìš”ë„ ê¸°ë°˜",
                "input": {
                    "task": "important",
                    "data": "test_data",
                    "id": "123",
                    "timestamp": "2025-08-06",
                },
                "context": {"goal": "optimization", "mode": "test"},
            },
        ]

        optimization_results = []
        for test_case in test_cases:
            start_time = time.time()
            cache_key = system._optimize_cache_key(
                test_case["input"], test_case["context"]
            )
            optimization_time = time.time() - start_time

            optimization_results.append(
                {
                    "name": test_case["name"],
                    "cache_key": cache_key,
                    "optimization_time": optimization_time,
                    "key_length": len(cache_key),
                }
            )

            logger.info(
                f"   {test_case['name']}: {optimization_time:.3f}ì´ˆ, í‚¤ ê¸¸ì´: {len(cache_key)}"
            )

        # 4. ìºì‹œ í¬ê¸° ì¡°ì • í…ŒìŠ¤íŠ¸
        logger.info("ğŸ“ 4. ìºì‹œ í¬ê¸° ì¡°ì • í…ŒìŠ¤íŠ¸ ì‹œì‘")

        # í˜„ì¬ ìºì‹œ í¬ê¸° í™•ì¸
        current_size = len(system.cache)
        current_max_size = system.cache_max_size

        # ìºì‹œ í¬ê¸° ì¡°ì • ì‹¤í–‰
        system._adjust_cache_size()

        new_max_size = system.cache_max_size
        logger.info(f"   í˜„ì¬ ìºì‹œ í¬ê¸°: {current_size}")
        logger.info(f"   ìºì‹œ ìµœëŒ€ í¬ê¸°: {current_max_size} -> {new_max_size}")

        # 5. ìºì‹œ ì •ë¦¬ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ§¹ 5. ìºì‹œ ì •ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")

        # ìºì‹œ ì •ë¦¬ ì‹¤í–‰
        system._improve_cache_cleanup()

        cleaned_size = len(system.cache)
        logger.info(f"   ì •ë¦¬ í›„ ìºì‹œ í¬ê¸°: {cleaned_size}")

        # 6. ì„±ëŠ¥ ë¹„êµ ë¶„ì„
        logger.info("ğŸ“ˆ 6. ì„±ëŠ¥ ë¹„êµ ë¶„ì„")

        # ìºì‹œ íˆíŠ¸ìœ¨ ê³„ì‚°
        total_requests = cache_stats.get("cache_hits", 0) + cache_stats.get(
            "cache_misses", 0
        )
        hit_rate = cache_stats.get("cache_hit_rate", 0.0)

        logger.info(f"   ì´ ìš”ì²­ ìˆ˜: {total_requests}")
        logger.info(f"   ìºì‹œ íˆíŠ¸ìœ¨: {hit_rate:.1%}")

        # ì„±ëŠ¥ ê°œì„  íš¨ê³¼ ë¶„ì„
        if hit_rate > 0.5:
            logger.info("   âœ… ìºì‹œ íˆíŠ¸ìœ¨ì´ 50% ì´ìƒìœ¼ë¡œ ì–‘í˜¸í•©ë‹ˆë‹¤!")
        elif hit_rate > 0.2:
            logger.info("   âš ï¸ ìºì‹œ íˆíŠ¸ìœ¨ì´ 20% ì´ìƒì´ì§€ë§Œ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
        else:
            logger.info("   âŒ ìºì‹œ íˆíŠ¸ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤. ì¶”ê°€ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        # ê²°ê³¼ ì €ì¥
        test_results = {
            "test_timestamp": datetime.now().isoformat(),
            "cache_results": cache_results,
            "cache_stats": cache_stats,
            "optimization_results": optimization_results,
            "cache_size_adjustment": {
                "previous_size": current_max_size,
                "new_size": new_max_size,
                "current_items": current_size,
                "cleaned_items": cleaned_size,
            },
            "performance_analysis": {
                "total_requests": total_requests,
                "hit_rate": hit_rate,
                "improvement_needed": hit_rate < 0.5,
            },
        }

        # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        with open("cache_optimization_test_results.json", "w", encoding="utf-8") as f:
            json.dump(test_results, f, indent=2, ensure_ascii=False, default=str)

        logger.info(
            "ğŸ’¾ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ 'cache_optimization_test_results.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
        )

        return test_results

    except Exception as e:
        logger.error(f"âŒ ìºì‹œ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        await system.cleanup()


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ DuRi í†µí•© ì§„í™” ì‹œìŠ¤í…œ - ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ í…ŒìŠ¤íŠ¸ ì‹œì‘")

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = await test_cache_optimization()

    if "error" in results:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {results['error']}")
        return 1
    else:
        logger.info("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)
