#!/usr/bin/env python3
"""
DuRi ì˜ë¯¸ ë²¡í„° ì—”ì§„ (Phase 1-1 Day 1)
ë¬¸ìì—´ ê¸°ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­ â†’ ì˜ë¯¸ ë²¡í„° ê¸°ë°˜ ì´í•´ë¡œ ì „í™˜
"""

import asyncio
import json
import re
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SemanticFrame(Enum):
    """ì˜ë¯¸ í”„ë ˆì„"""
    ETHICAL_DILEMMA = "ethical_dilemma"
    PRACTICAL_DECISION = "practical_decision"
    CONFLICT_RESOLUTION = "conflict_resolution"
    COMPLEX_PROBLEM = "complex_problem"
    GENERAL_SITUATION = "general_situation"

@dataclass
class SemanticVector:
    """ì˜ë¯¸ ë²¡í„°"""
    vector: np.ndarray
    dimension: int
    confidence: float
    metadata: Dict[str, Any]

@dataclass
class SemanticAnalysis:
    """ì˜ë¯¸ ë¶„ì„ ê²°ê³¼"""
    situation_vector: SemanticVector
    matched_frame: SemanticFrame
    confidence: float
    semantic_similarity: float
    context_elements: Dict[str, Any]

class SemanticVectorEngine:
    """ì˜ë¯¸ ë²¡í„° ì—”ì§„"""
    
    def __init__(self, vector_dimension: int = 100):
        self.vector_dimension = vector_dimension
        self.semantic_frames = self._initialize_semantic_frames()
        self.semantic_patterns = self._initialize_semantic_patterns()
        self.vector_cache = {}
        
    def _initialize_semantic_frames(self) -> Dict[SemanticFrame, np.ndarray]:
        """ì˜ë¯¸ í”„ë ˆì„ ì´ˆê¸°í™” - ì •êµí™”ëœ ë²¡í„° êµ¬ì¡°"""
        frames = {}
        
        # ìœ¤ë¦¬ì  ë”œë ˆë§ˆ í”„ë ˆì„ (ê°œì¸ì •ë³´, ë¹„ë°€, ìœ¤ë¦¬ì  ê°ˆë“±)
        ethical_dilemma_vector = np.zeros(self.vector_dimension)
        ethical_dilemma_vector[0:15] = 1.0    # ìœ¤ë¦¬ì  ìš”ì†Œ (ê°•í•¨)
        ethical_dilemma_vector[15:30] = 0.9   # ê°œì¸ì •ë³´/ë¹„ë°€ ìš”ì†Œ
        ethical_dilemma_vector[30:45] = 0.8   # ê°ˆë“± ìš”ì†Œ
        ethical_dilemma_vector[45:60] = 0.7   # ì˜ì‚¬ê²°ì • ìš”ì†Œ
        ethical_dilemma_vector[60:75] = 0.6   # ì‹¤ìš©ì  ìš”ì†Œ
        ethical_dilemma_vector[75:90] = 0.8   # ë³µì¡ì„± ìš”ì†Œ
        ethical_dilemma_vector[90:100] = 0.9  # ë”œë ˆë§ˆ íŠ¹ì„±
        frames[SemanticFrame.ETHICAL_DILEMMA] = ethical_dilemma_vector
        
        # ì‹¤ìš©ì  ê²°ì • í”„ë ˆì„ (ë¹„ì¦ˆë‹ˆìŠ¤, íš¨ìœ¨ì„±, ì„±ê³¼)
        practical_decision_vector = np.zeros(self.vector_dimension)
        practical_decision_vector[0:15] = 0.3  # ìœ¤ë¦¬ì  ìš”ì†Œ (ì•½í•¨)
        practical_decision_vector[15:30] = 0.2 # ê°œì¸ì •ë³´/ë¹„ë°€ ìš”ì†Œ
        practical_decision_vector[30:45] = 0.6 # ê°ˆë“± ìš”ì†Œ
        practical_decision_vector[45:60] = 1.0 # ì˜ì‚¬ê²°ì • ìš”ì†Œ (ê°•í•¨)
        practical_decision_vector[60:75] = 1.0 # ì‹¤ìš©ì  ìš”ì†Œ (ê°•í•¨)
        practical_decision_vector[75:90] = 0.7 # ë³µì¡ì„± ìš”ì†Œ
        practical_decision_vector[90:100] = 0.6 # ë”œë ˆë§ˆ íŠ¹ì„±
        frames[SemanticFrame.PRACTICAL_DECISION] = practical_decision_vector
        
        # ê°ˆë“± í•´ê²° í”„ë ˆì„ (ê°ˆë“±, ì¶©ëŒ, í•´ê²°)
        conflict_resolution_vector = np.zeros(self.vector_dimension)
        conflict_resolution_vector[0:15] = 0.5  # ìœ¤ë¦¬ì  ìš”ì†Œ
        conflict_resolution_vector[15:30] = 0.3 # ê°œì¸ì •ë³´/ë¹„ë°€ ìš”ì†Œ
        conflict_resolution_vector[30:45] = 1.0 # ê°ˆë“± ìš”ì†Œ (ê°•í•¨)
        conflict_resolution_vector[45:60] = 0.9 # ì˜ì‚¬ê²°ì • ìš”ì†Œ
        conflict_resolution_vector[60:75] = 0.6 # ì‹¤ìš©ì  ìš”ì†Œ
        conflict_resolution_vector[75:90] = 0.8 # ë³µì¡ì„± ìš”ì†Œ
        conflict_resolution_vector[90:100] = 0.7 # ë”œë ˆë§ˆ íŠ¹ì„±
        frames[SemanticFrame.CONFLICT_RESOLUTION] = conflict_resolution_vector
        
        # ë³µì¡í•œ ë¬¸ì œ í”„ë ˆì„ (ë‹¤ë©´ì , ë³µí•©ì )
        complex_problem_vector = np.zeros(self.vector_dimension)
        complex_problem_vector[0:15] = 0.7   # ìœ¤ë¦¬ì  ìš”ì†Œ
        complex_problem_vector[15:30] = 0.6  # ê°œì¸ì •ë³´/ë¹„ë°€ ìš”ì†Œ
        complex_problem_vector[30:45] = 0.8  # ê°ˆë“± ìš”ì†Œ
        complex_problem_vector[45:60] = 0.9  # ì˜ì‚¬ê²°ì • ìš”ì†Œ
        complex_problem_vector[60:75] = 0.7  # ì‹¤ìš©ì  ìš”ì†Œ
        complex_problem_vector[75:90] = 1.0  # ë³µì¡ì„± ìš”ì†Œ (ê°•í•¨)
        complex_problem_vector[90:100] = 0.8 # ë”œë ˆë§ˆ íŠ¹ì„±
        frames[SemanticFrame.COMPLEX_PROBLEM] = complex_problem_vector
        
        # ì¼ë°˜ì  ìƒí™© í”„ë ˆì„ (ê¸°ë³¸, ë‹¨ìˆœ)
        general_situation_vector = np.zeros(self.vector_dimension)
        general_situation_vector[0:15] = 0.3  # ìœ¤ë¦¬ì  ìš”ì†Œ
        general_situation_vector[15:30] = 0.2 # ê°œì¸ì •ë³´/ë¹„ë°€ ìš”ì†Œ
        general_situation_vector[30:45] = 0.3 # ê°ˆë“± ìš”ì†Œ
        general_situation_vector[45:60] = 0.5 # ì˜ì‚¬ê²°ì • ìš”ì†Œ
        general_situation_vector[60:75] = 0.5 # ì‹¤ìš©ì  ìš”ì†Œ
        general_situation_vector[75:90] = 0.3 # ë³µì¡ì„± ìš”ì†Œ
        general_situation_vector[90:100] = 0.2 # ë”œë ˆë§ˆ íŠ¹ì„±
        frames[SemanticFrame.GENERAL_SITUATION] = general_situation_vector
        
        return frames
    
    def _initialize_semantic_patterns(self) -> Dict[str, List[str]]:
        """ì˜ë¯¸ íŒ¨í„´ ì´ˆê¸°í™” - í•œêµ­ì–´ íŠ¹í™” í‚¤ì›Œë“œ ëŒ€í­ í™•ì¥"""
        return {
            "ethical_keywords": [
                # ê¸°ë³¸ ìœ¤ë¦¬ í‚¤ì›Œë“œ
                "ìœ¤ë¦¬", "ë„ë•", "ì •ì˜", "ê³µì •", "ì •ì§", "ì‹ ë¢°", "ì±…ì„", "ì˜ë¬´",
                "ê¶Œë¦¬", "ììœ ", "í‰ë“±", "ì¸ê¶Œ", "ì¡´ì—„", "ê°€ì¹˜", "ì›ì¹™", "ê·œì¹™",
                # ê°œì¸ì •ë³´ ë° í”„ë¼ì´ë²„ì‹œ
                "ê°œì¸ì •ë³´", "í”„ë¼ì´ë²„ì‹œ", "ì‚¬ìƒí™œ", "ë¹„ë°€", "ê¸°ë°€", "ë³´ì•ˆ", "ìœ ì¶œ", "ì¹¨í•´",
                "ê°œì¸ì •ë³´ë³´í˜¸", "ì •ë³´ë³´í˜¸", "ë°ì´í„°ë³´í˜¸", "ê°œì¸ì •ë³´ì¹¨í•´",
                # ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ê´€ë ¨
                "ë”œë ˆë§ˆ", "ê°ˆë“±", "ëª¨ìˆœ", "ìƒì¶©", "ì–‘ìíƒì¼", "ì„ íƒì˜ ê¸°ë¡œ",
                "ìœ¤ë¦¬ì ", "ë„ë•ì ", "ì •ì˜ë¡œìš´", "ê³µì •í•œ", "ì •ì§í•œ",
                # ë¹„ì¦ˆë‹ˆìŠ¤ ìœ¤ë¦¬
                "ê¸°ì—…ìœ¤ë¦¬", "ì‚¬íšŒì ì±…ì„", "ì´í•´ê´€ê³„", "íˆ¬ëª…ì„±", "ê³µì •ê±°ë˜",
                "ë¶€ì •ë¶€íŒ¨", "ë‡Œë¬¼", "íš¡ë ¹", "ë°°ì„", "ì‚¬ê¸°", "ê¸°ë§",
                # ê±°ì§“ë§ ë° ì§„ì‹¤
                "ê±°ì§“ë§", "í—ˆìœ„", "ì‚¬ì‹¤", "ì§„ì‹¤", "ì •ì§", "ê±°ì§“", "ì†ì„ìˆ˜",
                "ê¸°ë§Œ", "ì†ì´ê¸°", "ì†ì„", "ê¸°ë§", "ì‚¬ê¸°", "ê¸°ë§Œí–‰ìœ„"
            ],
            "conflict_keywords": [
                # ê¸°ë³¸ ê°ˆë“± í‚¤ì›Œë“œ
                "ê°ˆë“±", "ì¶©ëŒ", "ëŒ€ë¦½", "ë°˜ëŒ€", "ëª¨ìˆœ", "ìƒì¶©", "ê²½ìŸ", "íˆ¬ìŸ",
                "ë¶„ìŸ", "ì˜ê²¬ì°¨", "ì´í•´ê´€ê³„", "ëŒ€ë¦½", "ë°˜ë°œ", "ì €í•­", "ë°˜ëŒ€",
                # ê°ˆë“± ìƒí™© í‘œí˜„
                "ì‹¸ì›€", "ë‹¤íˆ¼", "ì–¸ìŸ", "ë§ë‹¤íˆ¼", "ê°ˆë“±ìƒí™©", "ì¶©ëŒìƒí™©",
                "ëŒ€ë¦½ê´€ê³„", "ì ëŒ€ê´€ê³„", "ë°˜ëª©", "ë¶ˆí™”", "ì•Œë ¥", "ë‹¤íˆ¼",
                # ê°ˆë“± í•´ê²°
                "í•´ê²°", "ì¡°ì •", "ì¤‘ì¬", "í™”í•´", "íƒ€í˜‘", "í•©ì˜", "ì¡°ìœ¨",
                "ê°ˆë“±í•´ê²°", "ì¶©ëŒí•´ê²°", "ë¶„ìŸí•´ê²°", "ì¡°ì •", "ì¤‘ì¬",
                # ê°ˆë“±ì˜ ì›ì¸
                "ì´í•´ê´€ê³„", "ì´ìµ", "ì†ì‹¤", "ë“ì‹¤", "ì°¨ìµ", "ì´ë“", "ì†í•´",
                "í”¼í•´", "ì†ì‹¤", "ì´ìµì¶©ëŒ", "ì´í•´ì¶©ëŒ", "ì´ìµê°ˆë“±"
            ],
            "decision_keywords": [
                # ê¸°ë³¸ ê²°ì • í‚¤ì›Œë“œ
                "ê²°ì •", "ì„ íƒ", "íŒë‹¨", "ê²°ë¡ ", "ì˜ì‚¬ê²°ì •", "íŒë‹¨", "ì„ íƒ",
                "ê²°ì •í•˜ë‹¤", "ì„ íƒí•˜ë‹¤", "íŒë‹¨í•˜ë‹¤", "ê²°ë¡ ë‚´ë¦¬ë‹¤", "ê²°ì •í•˜ë‹¤",
                # ê²°ì • ìƒí™©
                "ê²°ì •í•´ì•¼", "ì„ íƒí•´ì•¼", "íŒë‹¨í•´ì•¼", "ê²°ë¡ ë‚´ë ¤ì•¼",
                "ê²°ì •í•´ì•¼ í•˜ëŠ”", "ì„ íƒí•´ì•¼ í•˜ëŠ”", "íŒë‹¨í•´ì•¼ í•˜ëŠ”",
                "ê²°ì •í•´ì•¼ í•˜ëŠ” ìƒí™©", "ì„ íƒí•´ì•¼ í•˜ëŠ” ìƒí™©", "íŒë‹¨í•´ì•¼ í•˜ëŠ” ìƒí™©",
                # ê³ ë¯¼ê³¼ ê³ ë ¤
                "ê³ ë¯¼", "ê³ ë ¤", "ìƒê°", "ê²€í† ", "ê²€í† í•˜ë‹¤", "ê³ ë¯¼í•˜ë‹¤",
                "ìƒê°í•˜ë‹¤", "ê³ ë ¤í•˜ë‹¤", "ê²€í† í•˜ë‹¤", "ì‹¬ì‚¬ìˆ™ê³ ",
                # ê²°ì •ì˜ ê²°ê³¼
                "ê²°ê³¼", "ì„±ê³¼", "íš¨ê³¼", "ì˜í–¥", "ê²°ê³¼ì ìœ¼ë¡œ", "ê²°ê³¼ì ìœ¼ë¡œëŠ”",
                "ê²°ê³¼ì ìœ¼ë¡œ ë³´ë©´", "ê²°ê³¼ì ìœ¼ë¡œ ìƒê°í•˜ë©´"
            ],
            "practical_keywords": [
                # ê¸°ë³¸ ì‹¤ìš© í‚¤ì›Œë“œ
                "ì‹¤ìš©", "íš¨ìœ¨", "íš¨ê³¼", "ì„±ê³¼", "ê²°ê³¼", "ì„±ê³µ", "ì‹¤íŒ¨",
                "ì´ìµ", "ì†ì‹¤", "ë¹„ìš©", "í¸ìµ", "íš¨ìœ¨ì„±", "ì‹¤ìš©ì„±",
                # ë¹„ì¦ˆë‹ˆìŠ¤ ì‹¤ìš©ì„±
                "ìˆ˜ìµ", "ë§¤ì¶œ", "ë§¤ì¶œì•¡", "ë§¤ì¶œì´ìµ", "ì˜ì—…ì´ìµ", "ìˆœì´ìµ",
                "ë¹„ìš©ì ˆê°", "ì›ê°€ì ˆê°", "íš¨ìœ¨í™”", "ìµœì í™”", "ê°œì„ ",
                # íš¨ìœ¨ì„± ê´€ë ¨
                "íš¨ìœ¨ì ", "íš¨ê³¼ì ", "ì‹¤ìš©ì ", "ê²½ì œì ", "í•©ë¦¬ì ", "ì´ì„±ì ",
                "íš¨ìœ¨ì ìœ¼ë¡œ", "íš¨ê³¼ì ìœ¼ë¡œ", "ì‹¤ìš©ì ìœ¼ë¡œ", "ê²½ì œì ìœ¼ë¡œ",
                # ì„±ê³¼ ë° ê²°ê³¼
                "ì„±ê³¼", "ì‹¤ì ", "ì—…ì ", "ê²°ê³¼", "ì„±ê³¼ì§€í‘œ", "ì‹¤ì ì§€í‘œ",
                "ì„±ê³¼í‰ê°€", "ì‹¤ì í‰ê°€", "ì„±ê³¼ê´€ë¦¬", "ì‹¤ì ê´€ë¦¬"
            ],
            "complexity_keywords": [
                # ê¸°ë³¸ ë³µì¡ì„± í‚¤ì›Œë“œ
                "ë³µì¡", "ì–´ë ¤ìš´", "ë‚œí•´í•œ", "ë³µì¡í•œ", "ë‹¤ì–‘í•œ", "ì—¬ëŸ¬",
                "ë‹¤ì¤‘", "ë‹¤ì–‘", "ë³µí•©", "í†µí•©", "ì¢…í•©", "í¬ê´„",
                # ë³µì¡í•œ ìƒí™©
                "ë³µì¡í•œ ìƒí™©", "ì–´ë ¤ìš´ ìƒí™©", "ë‚œí•´í•œ ìƒí™©", "ë³µì¡í•œ ë¬¸ì œ",
                "ì–´ë ¤ìš´ ë¬¸ì œ", "ë‚œí•´í•œ ë¬¸ì œ", "ë³µì¡í•œ ì´ìŠˆ", "ì–´ë ¤ìš´ ì´ìŠˆ",
                # ë‹¤ë©´ì  ìš”ì†Œ
                "ë‹¤ë©´ì ", "ë‹¤ê°ì ", "ë‹¤ì°¨ì›ì ", "ë³µí•©ì ", "í†µí•©ì ", "ì¢…í•©ì ",
                "í¬ê´„ì ", "ì „ë©´ì ", "ì „ì²´ì ", "ì „ë°˜ì ", "ì „ì²´ì ìœ¼ë¡œ",
                # ë³µì¡ì„± í‘œí˜„
                "ë³µì¡í•˜ê²Œ", "ì–´ë µê²Œ", "ë‚œí•´í•˜ê²Œ", "ë³µì¡í•˜ê²Œ ë˜ì–´",
                "ì–´ë µê²Œ ë˜ì–´", "ë‚œí•´í•˜ê²Œ ë˜ì–´", "ë³µì¡í•˜ê²Œ ë§Œë“¤ë‹¤",
                "ì–´ë µê²Œ ë§Œë“¤ë‹¤", "ë‚œí•´í•˜ê²Œ ë§Œë“¤ë‹¤",
                # Day 3 ì¶”ê°€: ë³µì¡ì„± í‚¤ì›Œë“œ ëŒ€í­ í™•ì¥
                "ë‹¤ì–‘í•œ ì´í•´ê´€ê³„ì", "ì—¬ëŸ¬ ì´í•´ê´€ê³„ì", "ë‹¤ì¤‘ ì´í•´ê´€ê³„ì",
                "ë‹¤ì–‘í•œ ê´€ì ", "ì—¬ëŸ¬ ê´€ì ", "ë‹¤ì¤‘ ê´€ì ", "ë‹¤ë©´ì  ê´€ì ",
                "ë³µí•©ì  ìš”ì†Œ", "ë‹¤ì–‘í•œ ìš”ì†Œ", "ì—¬ëŸ¬ ìš”ì†Œ", "ë‹¤ì¤‘ ìš”ì†Œ",
                "í†µí•©ì  ì ‘ê·¼", "ì¢…í•©ì  ì ‘ê·¼", "í¬ê´„ì  ì ‘ê·¼", "ì „ë©´ì  ì ‘ê·¼",
                "ë‹¤ì°¨ì›ì  ë¶„ì„", "ë‹¤ê°ì  ë¶„ì„", "ë³µí•©ì  ë¶„ì„", "í†µí•©ì  ë¶„ì„",
                "ì¢…í•©ì  ë¶„ì„", "í¬ê´„ì  ë¶„ì„", "ì „ë©´ì  ë¶„ì„", "ì „ì²´ì  ë¶„ì„",
                "ë‹¤ì–‘í•œ ì¸¡ë©´", "ì—¬ëŸ¬ ì¸¡ë©´", "ë‹¤ì¤‘ ì¸¡ë©´", "ë‹¤ë©´ì  ì¸¡ë©´",
                "ë³µí•©ì  ì¸¡ë©´", "í†µí•©ì  ì¸¡ë©´", "ì¢…í•©ì  ì¸¡ë©´", "í¬ê´„ì  ì¸¡ë©´",
                "ë‹¤ì–‘í•œ ì°¨ì›", "ì—¬ëŸ¬ ì°¨ì›", "ë‹¤ì¤‘ ì°¨ì›", "ë‹¤ë©´ì  ì°¨ì›",
                "ë³µí•©ì  ì°¨ì›", "í†µí•©ì  ì°¨ì›", "ì¢…í•©ì  ì°¨ì›", "í¬ê´„ì  ì°¨ì›",
                "ë‹¤ì–‘í•œ ì˜ì—­", "ì—¬ëŸ¬ ì˜ì—­", "ë‹¤ì¤‘ ì˜ì—­", "ë‹¤ë©´ì  ì˜ì—­",
                "ë³µí•©ì  ì˜ì—­", "í†µí•©ì  ì˜ì—­", "ì¢…í•©ì  ì˜ì—­", "í¬ê´„ì  ì˜ì—­",
                "ë‹¤ì–‘í•œ ë¶„ì•¼", "ì—¬ëŸ¬ ë¶„ì•¼", "ë‹¤ì¤‘ ë¶„ì•¼", "ë‹¤ë©´ì  ë¶„ì•¼",
                "ë³µí•©ì  ë¶„ì•¼", "í†µí•©ì  ë¶„ì•¼", "ì¢…í•©ì  ë¶„ì•¼", "í¬ê´„ì  ë¶„ì•¼",
                "ë‹¤ì–‘í•œ ì£¼ì œ", "ì—¬ëŸ¬ ì£¼ì œ", "ë‹¤ì¤‘ ì£¼ì œ", "ë‹¤ë©´ì  ì£¼ì œ",
                "ë³µí•©ì  ì£¼ì œ", "í†µí•©ì  ì£¼ì œ", "ì¢…í•©ì  ì£¼ì œ", "í¬ê´„ì  ì£¼ì œ",
                "ë‹¤ì–‘í•œ ì´ìŠˆ", "ì—¬ëŸ¬ ì´ìŠˆ", "ë‹¤ì¤‘ ì´ìŠˆ", "ë‹¤ë©´ì  ì´ìŠˆ",
                "ë³µí•©ì  ì´ìŠˆ", "í†µí•©ì  ì´ìŠˆ", "ì¢…í•©ì  ì´ìŠˆ", "í¬ê´„ì  ì´ìŠˆ",
                "ë‹¤ì–‘í•œ ë¬¸ì œ", "ì—¬ëŸ¬ ë¬¸ì œ", "ë‹¤ì¤‘ ë¬¸ì œ", "ë‹¤ë©´ì  ë¬¸ì œ",
                "ë³µí•©ì  ë¬¸ì œ", "í†µí•©ì  ë¬¸ì œ", "ì¢…í•©ì  ë¬¸ì œ", "í¬ê´„ì  ë¬¸ì œ",
                "ë‹¤ì–‘í•œ ìƒí™©", "ì—¬ëŸ¬ ìƒí™©", "ë‹¤ì¤‘ ìƒí™©", "ë‹¤ë©´ì  ìƒí™©",
                "ë³µí•©ì  ìƒí™©", "í†µí•©ì  ìƒí™©", "ì¢…í•©ì  ìƒí™©", "í¬ê´„ì  ìƒí™©",
                "ë‹¤ì–‘í•œ ì¡°ê±´", "ì—¬ëŸ¬ ì¡°ê±´", "ë‹¤ì¤‘ ì¡°ê±´", "ë‹¤ë©´ì  ì¡°ê±´",
                "ë³µí•©ì  ì¡°ê±´", "í†µí•©ì  ì¡°ê±´", "ì¢…í•©ì  ì¡°ê±´", "í¬ê´„ì  ì¡°ê±´",
                "ë‹¤ì–‘í•œ ìš”ì¸", "ì—¬ëŸ¬ ìš”ì¸", "ë‹¤ì¤‘ ìš”ì¸", "ë‹¤ë©´ì  ìš”ì¸",
                "ë³µí•©ì  ìš”ì¸", "í†µí•©ì  ìš”ì¸", "ì¢…í•©ì  ìš”ì¸", "í¬ê´„ì  ìš”ì¸",
                "ë‹¤ì–‘í•œ ë³€ìˆ˜", "ì—¬ëŸ¬ ë³€ìˆ˜", "ë‹¤ì¤‘ ë³€ìˆ˜", "ë‹¤ë©´ì  ë³€ìˆ˜",
                "ë³µí•©ì  ë³€ìˆ˜", "í†µí•©ì  ë³€ìˆ˜", "ì¢…í•©ì  ë³€ìˆ˜", "í¬ê´„ì  ë³€ìˆ˜"
            ],
            "general_keywords": [
                # Day 3 ì¶”ê°€: ì¼ë°˜ì  ìƒí™© í‚¤ì›Œë“œ
                "ì¼ë°˜", "ì¼ìƒ", "ë³´í†µ", "í‰ìƒì‹œ", "í‰ì†Œ", "ì¼ë°˜ì ",
                "ì¼ìƒì ", "ë³´í†µì˜", "í‰ìƒì‹œì˜", "í‰ì†Œì˜", "ì¼ë°˜ì ì¸",
                "ì¼ìƒì ì¸", "ë³´í†µì¸", "í‰ìƒì‹œì¸", "í‰ì†Œì¸", "ì¼ë°˜ì ìœ¼ë¡œ",
                "ì¼ìƒì ìœ¼ë¡œ", "ë³´í†µìœ¼ë¡œ", "í‰ìƒì‹œë¡œ", "í‰ì†Œë¡œ", "ì¼ë°˜ì ìœ¼ë¡œëŠ”",
                "ì¼ìƒì ìœ¼ë¡œëŠ”", "ë³´í†µìœ¼ë¡œëŠ”", "í‰ìƒì‹œë¡œëŠ”", "í‰ì†Œë¡œëŠ”",
                "ì¼ë°˜ì ì¸ ìƒí™©", "ì¼ìƒì ì¸ ìƒí™©", "ë³´í†µì˜ ìƒí™©", "í‰ìƒì‹œì˜ ìƒí™©",
                "í‰ì†Œì˜ ìƒí™©", "ì¼ë°˜ì ì¸ ê²½ìš°", "ì¼ìƒì ì¸ ê²½ìš°", "ë³´í†µì˜ ê²½ìš°",
                "í‰ìƒì‹œì˜ ê²½ìš°", "í‰ì†Œì˜ ê²½ìš°", "ì¼ë°˜ì ì¸ ì—…ë¬´", "ì¼ìƒì ì¸ ì—…ë¬´",
                "ë³´í†µì˜ ì—…ë¬´", "í‰ìƒì‹œì˜ ì—…ë¬´", "í‰ì†Œì˜ ì—…ë¬´", "ì¼ë°˜ì ì¸ ì‘ì—…",
                "ì¼ìƒì ì¸ ì‘ì—…", "ë³´í†µì˜ ì‘ì—…", "í‰ìƒì‹œì˜ ì‘ì—…", "í‰ì†Œì˜ ì‘ì—…",
                "ì¼ë°˜ì ì¸ ì²˜ë¦¬", "ì¼ìƒì ì¸ ì²˜ë¦¬", "ë³´í†µì˜ ì²˜ë¦¬", "í‰ìƒì‹œì˜ ì²˜ë¦¬",
                "í‰ì†Œì˜ ì²˜ë¦¬", "ì¼ë°˜ì ì¸ ê´€ë¦¬", "ì¼ìƒì ì¸ ê´€ë¦¬", "ë³´í†µì˜ ê´€ë¦¬",
                "í‰ìƒì‹œì˜ ê´€ë¦¬", "í‰ì†Œì˜ ê´€ë¦¬", "ì¼ë°˜ì ì¸ ìš´ì˜", "ì¼ìƒì ì¸ ìš´ì˜",
                "ë³´í†µì˜ ìš´ì˜", "í‰ìƒì‹œì˜ ìš´ì˜", "í‰ì†Œì˜ ìš´ì˜", "ì¼ë°˜ì ì¸ ì„œë¹„ìŠ¤",
                "ì¼ìƒì ì¸ ì„œë¹„ìŠ¤", "ë³´í†µì˜ ì„œë¹„ìŠ¤", "í‰ìƒì‹œì˜ ì„œë¹„ìŠ¤", "í‰ì†Œì˜ ì„œë¹„ìŠ¤",
                "ì¼ë°˜ì ì¸ ì •ë³´", "ì¼ìƒì ì¸ ì •ë³´", "ë³´í†µì˜ ì •ë³´", "í‰ìƒì‹œì˜ ì •ë³´",
                "í‰ì†Œì˜ ì •ë³´", "ì¼ë°˜ì ì¸ ë°ì´í„°", "ì¼ìƒì ì¸ ë°ì´í„°", "ë³´í†µì˜ ë°ì´í„°",
                "í‰ìƒì‹œì˜ ë°ì´í„°", "í‰ì†Œì˜ ë°ì´í„°", "ì¼ë°˜ì ì¸ ë‚´ìš©", "ì¼ìƒì ì¸ ë‚´ìš©",
                "ë³´í†µì˜ ë‚´ìš©", "í‰ìƒì‹œì˜ ë‚´ìš©", "í‰ì†Œì˜ ë‚´ìš©", "ì¼ë°˜ì ì¸ ì‚¬í•­",
                "ì¼ìƒì ì¸ ì‚¬í•­", "ë³´í†µì˜ ì‚¬í•­", "í‰ìƒì‹œì˜ ì‚¬í•­", "í‰ì†Œì˜ ì‚¬í•­"
            ]
        }
    
    def encode_semantics(self, situation: str) -> SemanticVector:
        """ìƒí™©ì„ ì˜ë¯¸ ë²¡í„°ë¡œ ì¸ì½”ë”©"""
        logger.info(f"ì˜ë¯¸ ë²¡í„° ì¸ì½”ë”© ì‹œì‘: {situation[:50]}...")
        
        # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_text = self._preprocess_text(situation)
        
        # 2. ì˜ë¯¸ì  íŠ¹ì„± ì¶”ì¶œ
        semantic_features = self._extract_semantic_features(processed_text)
        
        # 3. ë²¡í„° ìƒì„±
        vector = self._create_semantic_vector(semantic_features)
        
        # 4. ë²¡í„° ì •ê·œí™”
        normalized_vector = self._normalize_vector(vector)
        
        # 5. ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self._calculate_encoding_confidence(semantic_features)
        
        semantic_vector = SemanticVector(
            vector=normalized_vector,
            dimension=self.vector_dimension,
            confidence=confidence,
            metadata={
                "semantic_features": semantic_features,
                "original_text": situation,
                "processed_text": processed_text
            }
        )
        
        logger.info(f"ì˜ë¯¸ ë²¡í„° ì¸ì½”ë”© ì™„ë£Œ: ì°¨ì›={self.vector_dimension}, ì‹ ë¢°ë„={confidence:.2f}")
        return semantic_vector
    
    def _preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # ì†Œë¬¸ì ë³€í™˜
        text = text.lower()
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì˜ë¯¸ ìˆëŠ” êµ¬ë‘ì ì€ ë³´ì¡´)
        text = re.sub(r'[^\w\s\.\,\!\?]', '', text)
        
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _extract_semantic_features(self, text: str) -> Dict[str, float]:
        """ì˜ë¯¸ì  íŠ¹ì„± ì¶”ì¶œ - Day 4 ê°œì„ ëœ í‚¤ì›Œë“œ ë§¤ì¹­"""
        features = {
            "ethical_score": 0.0,
            "privacy_score": 0.0,
            "conflict_score": 0.0,
            "decision_score": 0.0,
            "practical_score": 0.0,
            "complexity_score": 0.0,
            "dilemma_score": 0.0,
            "general_score": 0.0
        }
        
        # Day 4: ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì •ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        context = self._extract_context_elements(text)
        
        # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë§¤ì¹­
        category_weights = {
            "ethical_keywords": 1.0,
            "conflict_keywords": 1.0,
            "decision_keywords": 1.0,
            "practical_keywords": 1.0,
            "complexity_keywords": 1.2,
            "general_keywords": 0.8
        }
        
        matched_keywords = {}
        
        for category, keywords in self.semantic_patterns.items():
            score, matched_in_category = self._optimized_keyword_matching(text, keywords)
            matched_keywords[category] = matched_in_category
            
            # ì •ê·œí™” (0.0-1.0)
            if keywords:
                normalized_score = min(score / len(keywords), 1.0) * category_weights.get(category, 1.0)
                
                # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ í• ë‹¹
                if category == "ethical_keywords":
                    features["ethical_score"] = normalized_score
                    # ê°œì¸ì •ë³´/ë¹„ë°€ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ privacy_scoreë„ ì¦ê°€
                    privacy_keywords = ["ê°œì¸ì •ë³´", "í”„ë¼ì´ë²„ì‹œ", "ë¹„ë°€", "ê¸°ë°€", "ìœ ì¶œ", "ì¹¨í•´"]
                    privacy_score = sum(1.0 for kw in privacy_keywords if kw in text) / len(privacy_keywords)
                    features["privacy_score"] = min(privacy_score, 1.0)
                elif category == "conflict_keywords":
                    features["conflict_score"] = normalized_score
                elif category == "decision_keywords":
                    features["decision_score"] = normalized_score
                elif category == "practical_keywords":
                    features["practical_score"] = normalized_score
                elif category == "complexity_keywords":
                    features["complexity_score"] = normalized_score
                elif category == "general_keywords":
                    features["general_score"] = normalized_score
        
        # ë”œë ˆë§ˆ ì ìˆ˜ ê³„ì‚° (ìœ¤ë¦¬ì  ìš”ì†Œì™€ ê°ˆë“± ìš”ì†Œì˜ ì¡°í•©)
        dilemma_score = (features["ethical_score"] + features["conflict_score"]) / 2.0
        features["dilemma_score"] = dilemma_score
        
        # Day 4: ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì • ì ìš©
        dynamic_weights = self._adjust_weights_dynamically(features, context)
        
        # ë™ì  ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ì ìˆ˜ ì¬ê³„ì‚°
        for feature_name, weight in dynamic_weights.items():
            if feature_name in features:
                features[feature_name] *= weight * 5  # ê°€ì¤‘ì¹˜ ì •ê·œí™” ë³´ì •
        
        # ë³µì¡ì„± ì ìˆ˜ ë³´ì • (ë³µì¡ì„± í‚¤ì›Œë“œê°€ ë§ì„ ë•Œ ê°€ì¤‘ì¹˜ ì¦ê°€)
        if features["complexity_score"] > 0.3:
            features["complexity_score"] = min(features["complexity_score"] * 1.3, 1.0)
        
        # ì¼ë°˜ì  ìƒí™© ì ìˆ˜ ë³´ì • (ì¼ë°˜ì  í‚¤ì›Œë“œê°€ ë§ì„ ë•Œ ê°€ì¤‘ì¹˜ ì¦ê°€)
        if features["general_score"] > 0.2:
            features["general_score"] = min(features["general_score"] * 1.2, 1.0)
        
        # ë³µì¡ì„±ê³¼ ì¼ë°˜ì„±ì˜ ìƒí˜¸ ë°°ì œ ë¡œì§
        if features["complexity_score"] > 0.5 and features["general_score"] > 0.3:
            features["general_score"] *= 0.5
        
        # íŠ¹ë³„í•œ ë³µì¡ì„± í‚¤ì›Œë“œ ë§¤ì¹­
        special_complexity_keywords = ["ë‹¤ì–‘í•œ", "ì—¬ëŸ¬", "ë‹¤ì¤‘", "ë‹¤ë©´ì ", "ë³µí•©ì ", "í†µí•©ì ", "ì¢…í•©ì ", "í¬ê´„ì "]
        special_complexity_score = sum(1.0 for kw in special_complexity_keywords if kw in text) / len(special_complexity_keywords)
        if special_complexity_score > 0:
            features["complexity_score"] = max(features["complexity_score"], special_complexity_score * 0.8)
        
        # íŠ¹ë³„í•œ ì¼ë°˜ì„± í‚¤ì›Œë“œ ë§¤ì¹­
        special_general_keywords = ["ì¼ë°˜", "ì¼ìƒ", "ë³´í†µ", "í‰ìƒì‹œ", "í‰ì†Œ", "ì¼ë°˜ì ", "ì¼ìƒì "]
        special_general_score = sum(1.0 for kw in special_general_keywords if kw in text) / len(special_general_keywords)
        if special_general_score > 0:
            features["general_score"] = max(features["general_score"], special_general_score * 0.8)
        
        # ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
        features["_debug_matched_keywords"] = matched_keywords
        features["_debug_dynamic_weights"] = dynamic_weights
        
        return features
    
    def _optimized_keyword_matching(self, text: str, keywords: List[str]) -> Tuple[float, List[str]]:
        """Day 4: ìµœì í™”ëœ í‚¤ì›Œë“œ ë§¤ì¹­"""
        score = 0.0
        matched = []
        
        # í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ë§¤ì¹­ ì„±ëŠ¥ í–¥ìƒ
        text_lower = text.lower()
        
        # í‚¤ì›Œë“œë¥¼ ê¸¸ì´ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê¸´ í‚¤ì›Œë“œë¥¼ ë¨¼ì € ë§¤ì¹­
        sorted_keywords = sorted(keywords, key=len, reverse=True)
        
        for keyword in sorted_keywords:
            keyword_lower = keyword.lower()
            if keyword_lower in text_lower:
                # í‚¤ì›Œë“œ ê¸¸ì´ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ (ê¸´ í‚¤ì›Œë“œê°€ ë” ì¤‘ìš”)
                keyword_weight = len(keyword) / 10.0
                
                # ë³µì¡ì„± í‚¤ì›Œë“œì— ëŒ€í•œ ì¶”ê°€ ê°€ì¤‘ì¹˜
                if len(keyword) > 5:
                    keyword_weight *= 1.5
                
                score += keyword_weight
                matched.append(keyword)
        
        return score, matched
    
    def _create_semantic_vector(self, features: Dict[str, float]) -> np.ndarray:
        """ì˜ë¯¸ ë²¡í„° ìƒì„± - Day 3 ì •êµí™”ëœ êµ¬ì¡°"""
        vector = np.zeros(self.vector_dimension)
        
        # ìœ¤ë¦¬ì  ìš”ì†Œ (0-15)
        vector[0:15] = features["ethical_score"]
        
        # ê°œì¸ì •ë³´/ë¹„ë°€ ìš”ì†Œ (15-30)
        vector[15:30] = features["privacy_score"]
        
        # ê°ˆë“± ìš”ì†Œ (30-45)
        vector[30:45] = features["conflict_score"]
        
        # ì˜ì‚¬ê²°ì • ìš”ì†Œ (45-60)
        vector[45:60] = features["decision_score"]
        
        # ì‹¤ìš©ì  ìš”ì†Œ (60-75)
        vector[60:75] = features["practical_score"]
        
        # ë³µì¡ì„± ìš”ì†Œ (75-90)
        vector[75:90] = features["complexity_score"]
        
        # ë”œë ˆë§ˆ íŠ¹ì„± (90-100)
        vector[90:100] = features["dilemma_score"]
        
        # Day 3: ì¼ë°˜ì  ìƒí™© ì ìˆ˜ëŠ” ì „ì²´ ë²¡í„°ì— ë¶„ì‚° ì ìš©
        if features["general_score"] > 0.0:
            # ì¼ë°˜ì  ìƒí™©ì¼ ë•ŒëŠ” ëª¨ë“  ìš”ì†Œì— ì•½ê°„ì˜ ê°€ì¤‘ì¹˜ ì ìš©
            general_weight = features["general_score"] * 0.3
            vector = vector * (1.0 - general_weight) + general_weight * 0.5
        
        return vector
    
    def _normalize_vector(self, vector: np.ndarray) -> np.ndarray:
        """ë²¡í„° ì •ê·œí™”"""
        norm = np.linalg.norm(vector)
        if norm > 0:
            return vector / norm
        return vector
    
    def _calculate_encoding_confidence(self, features: Dict[str, float]) -> float:
        """ì¸ì½”ë”© ì‹ ë¢°ë„ ê³„ì‚° - Day 4 ê°œì„ ëœ ë°©ì‹"""
        # íŠ¹ì„± ì ìˆ˜ì˜ ê°€ì¤‘ í‰ê· ì„ ì‹ ë¢°ë„ë¡œ ì‚¬ìš©
        weights = {
            "ethical_score": 0.2,
            "privacy_score": 0.15,
            "conflict_score": 0.2,
            "decision_score": 0.2,
            "practical_score": 0.15,
            "complexity_score": 0.1,
            "general_score": 0.1
        }
        
        weighted_sum = 0.0
        total_weight = 0.0
        
        for feature, score in features.items():
            if feature in weights and isinstance(score, (int, float)):
                weighted_sum += score * weights[feature]
                total_weight += weights[feature]
        
        if total_weight > 0:
            base_confidence = weighted_sum / total_weight
        else:
            base_confidence = 0.0
        
        # Day 4: í‚¤ì›Œë“œ ë§¤ì¹­ ê°•ë„ì— ë”°ë¥¸ ë³´ì • ê°•í™”
        keyword_matches = sum(1 for score in features.values() if isinstance(score, (int, float)) and score > 0)
        keyword_bonus = min(keyword_matches * 0.15, 0.4)  # Day 4: ë³´ë„ˆìŠ¤ ì¦ê°€
        
        # Day 4: ë³µì¡ì„± í‚¤ì›Œë“œ ë§¤ì¹­ ì‹œ ì¶”ê°€ ë³´ë„ˆìŠ¤ ê°•í™”
        complexity_bonus = 0.0
        if features.get("complexity_score", 0.0) > 0.3:
            complexity_bonus = min(features["complexity_score"] * 0.3, 0.3)  # Day 4: ë³´ë„ˆìŠ¤ ì¦ê°€
        
        # Day 4: ì¼ë°˜ì  ìƒí™© í‚¤ì›Œë“œ ë§¤ì¹­ ì‹œ ê¸°ë³¸ ë³´ë„ˆìŠ¤ ê°•í™”
        general_bonus = 0.0
        if features.get("general_score", 0.0) > 0.0:
            general_bonus = min(features["general_score"] * 0.2, 0.2)  # Day 4: ë³´ë„ˆìŠ¤ ì¦ê°€
        
        # Day 4: ìœ¤ë¦¬ì  ìš”ì†Œ ê°•ë„ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
        ethical_bonus = 0.0
        if features.get("ethical_score", 0.0) > 0.5:
            ethical_bonus = min(features["ethical_score"] * 0.2, 0.2)
        
        # Day 4: ê°ˆë“± ìš”ì†Œ ê°•ë„ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
        conflict_bonus = 0.0
        if features.get("conflict_score", 0.0) > 0.5:
            conflict_bonus = min(features["conflict_score"] * 0.2, 0.2)
        
        # Day 4: ì‹¤ìš©ì  ìš”ì†Œ ê°•ë„ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
        practical_bonus = 0.0
        if features.get("practical_score", 0.0) > 0.5:
            practical_bonus = min(features["practical_score"] * 0.15, 0.15)
        
        # Day 4: ì˜ì‚¬ê²°ì • ìš”ì†Œ ê°•ë„ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
        decision_bonus = 0.0
        if features.get("decision_score", 0.0) > 0.5:
            decision_bonus = min(features["decision_score"] * 0.15, 0.15)
        
        # ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚°
        final_confidence = (base_confidence + keyword_bonus + complexity_bonus + 
                          general_bonus + ethical_bonus + conflict_bonus + 
                          practical_bonus + decision_bonus)
        
        # Day 4: ìµœì†Œ ì‹ ë¢°ë„ ë³´ì¥ (0.3ìœ¼ë¡œ ì¦ê°€)
        final_confidence = max(final_confidence, 0.3)
        
        return min(final_confidence, 0.8)  # Day 4: ìƒí•œ 0.8ë¡œ ì œí•œ
    
    def _adjust_weights_dynamically(self, features: Dict[str, float], context: Dict[str, Any]) -> Dict[str, float]:
        """Day 4: ìƒí™©ë³„ ê°€ì¤‘ì¹˜ ë™ì  ì¡°ì •"""
        base_weights = {
            "ethical_score": 0.2,
            "privacy_score": 0.15,
            "conflict_score": 0.2,
            "decision_score": 0.2,
            "practical_score": 0.15,
            "complexity_score": 0.1,
            "general_score": 0.1
        }
        
        # ì´í•´ê´€ê³„ì ìˆ˜ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì¡°ì •
        stakeholder_count = len(context.get("actors", []))
        if stakeholder_count > 3:
            base_weights["complexity_score"] *= 1.3
            base_weights["conflict_score"] *= 1.2
        elif stakeholder_count > 1:
            base_weights["complexity_score"] *= 1.1
            base_weights["conflict_score"] *= 1.1
        
        # ìœ¤ë¦¬ì  ìš”ì†Œ ê°•ë„ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì¡°ì •
        if features.get("ethical_score", 0.0) > 0.5:
            base_weights["ethical_score"] *= 1.2
            base_weights["privacy_score"] *= 1.1
        
        # ë³µì¡ì„± ìš”ì†Œ ê°•ë„ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì¡°ì •
        if features.get("complexity_score", 0.0) > 0.4:
            base_weights["complexity_score"] *= 1.3
            base_weights["decision_score"] *= 1.1
        
        # ê°ˆë“± ìš”ì†Œ ê°•ë„ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì¡°ì •
        if features.get("conflict_score", 0.0) > 0.5:
            base_weights["conflict_score"] *= 1.2
            base_weights["decision_score"] *= 1.1
        
        # ì¼ë°˜ì  ìƒí™©ì—ì„œì˜ ê°€ì¤‘ì¹˜ ì¡°ì •
        if features.get("general_score", 0.0) > 0.3:
            base_weights["general_score"] *= 1.2
            # ë‹¤ë¥¸ ìš”ì†Œë“¤ì˜ ê°€ì¤‘ì¹˜ ê°ì†Œ
            for key in base_weights:
                if key != "general_score":
                    base_weights[key] *= 0.9
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™” (í•©ì´ 1.0ì´ ë˜ë„ë¡)
        total_weight = sum(base_weights.values())
        if total_weight > 0:
            for key in base_weights:
                base_weights[key] /= total_weight
        
        return base_weights
    
    def _calculate_context_based_confidence(self, features: Dict[str, float], context: Dict[str, Any]) -> float:
        """Day 5: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚° (ê³ ë„í™”)"""
        base_confidence = self._calculate_encoding_confidence(features)
        
        # ì»¨í…ìŠ¤íŠ¸ ìš”ì†Œë³„ ë³´ì •
        context_bonus = 0.0
        
        # ì´í•´ê´€ê³„ì ìˆ˜ì— ë”°ë¥¸ ë³´ì • (Day 5: ê°•í™”)
        stakeholder_count = len(context.get("actors", []))
        if stakeholder_count > 3:
            context_bonus += 0.2  # Day 5: ì¦ê°€
        elif stakeholder_count > 2:
            context_bonus += 0.18  # Day 5: ì¦ê°€
        elif stakeholder_count > 1:
            context_bonus += 0.15  # Day 5: ì¦ê°€
        
        # ìƒí™© ë³µì¡ì„±ì— ë”°ë¥¸ ë³´ì • (Day 5: ê°•í™”)
        if features.get("complexity_score", 0.0) > 0.5:
            context_bonus += 0.25  # Day 5: ì¦ê°€
        elif features.get("complexity_score", 0.0) > 0.3:
            context_bonus += 0.2  # Day 5: ì¦ê°€
        
        # ì‹œê°„ì  ì••ë°•ì— ë”°ë¥¸ ë³´ì • (Day 5: ê°•í™”)
        temporal_aspects = context.get("temporal_aspects", [])
        urgency_keywords = ["ê¸´ê¸‰", "ì‹œê¸‰", "ì¦‰ì‹œ", "ë¹ ë¥¸", "ì‹ ì†", "ê¸‰í•œ", "ê¸´ê¸‰í•œ"]
        if any(urgency in str(temporal_aspects) for urgency in urgency_keywords):
            context_bonus += 0.15  # Day 5: ì¦ê°€
        
        # ìœ¤ë¦¬ì  ìš”ì†Œ ê°•ë„ì— ë”°ë¥¸ ë³´ì • (Day 5: ê°•í™”)
        if features.get("ethical_score", 0.0) > 0.6:
            context_bonus += 0.2  # Day 5: ì¦ê°€
        elif features.get("ethical_score", 0.0) > 0.3:
            context_bonus += 0.15  # Day 5: ì¦ê°€
        
        # ê°ˆë“± ìš”ì†Œ ê°•ë„ì— ë”°ë¥¸ ë³´ì • (Day 5: ê°•í™”)
        if features.get("conflict_score", 0.0) > 0.6:
            context_bonus += 0.2  # Day 5: ì¦ê°€
        elif features.get("conflict_score", 0.0) > 0.3:
            context_bonus += 0.15  # Day 5: ì¦ê°€
        
        # ì¼ë°˜ì  ìƒí™©ì—ì„œì˜ ê¸°ë³¸ ë³´ì • (Day 5: ê°•í™”)
        if features.get("general_score", 0.0) > 0.3:
            context_bonus += 0.15  # Day 5: ì¦ê°€
        
        # ì‹¤ìš©ì  ìš”ì†Œ ê°•ë„ì— ë”°ë¥¸ ë³´ì • (Day 5: ê°•í™”)
        if features.get("practical_score", 0.0) > 0.5:
            context_bonus += 0.15  # Day 5: ì¦ê°€
        
        # ì˜ì‚¬ê²°ì • ìš”ì†Œ ê°•ë„ì— ë”°ë¥¸ ë³´ì • (Day 5: ê°•í™”)
        if features.get("decision_score", 0.0) > 0.5:
            context_bonus += 0.15  # Day 5: ì¦ê°€
        
        # Day 5: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ë³´ë„ˆìŠ¤
        # í”„ë¼ì´ë²„ì‹œ ìš”ì†Œ ê°•ë„ì— ë”°ë¥¸ ë³´ì •
        if features.get("privacy_score", 0.0) > 0.5:
            context_bonus += 0.1
        
        # Day 5: ìƒí™©ì˜ ëª…í™•ì„±ì— ë”°ë¥¸ ë³´ì •
        total_feature_score = sum(float(v) for v in features.values() if isinstance(v, (int, float)))
        if total_feature_score > 2.0:
            context_bonus += 0.1  # ëª…í™•í•œ ìƒí™©ì— ëŒ€í•œ ë³´ë„ˆìŠ¤
        
        # ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚° (Day 5: ìƒí•œ 0.85ë¡œ ì¦ê°€)
        final_confidence = min(base_confidence + context_bonus, 0.85)
        
        # Day 5: ìµœì†Œ ì‹ ë¢°ë„ ë³´ì¥ (0.35ë¡œ ì¦ê°€)
        return max(final_confidence, 0.35)
    
    def match_to_known_frames(self, semantic_vector: SemanticVector) -> SemanticFrame:
        """ì•Œë ¤ì§„ í”„ë ˆì„ê³¼ ë§¤ì¹­ - Day 3 ê°œì„ ëœ ë°©ì‹"""
        logger.info("ì˜ë¯¸ í”„ë ˆì„ ë§¤ì¹­ ì‹œì‘")
        
        best_match = SemanticFrame.GENERAL_SITUATION
        best_similarity = 0.0
        
        # Day 3: ë³µì¡ì„± ì ìˆ˜ì™€ ì¼ë°˜ì„± ì ìˆ˜ë¥¼ ë¨¼ì € í™•ì¸
        complexity_score = semantic_vector.vector[75:90].mean()
        general_score = semantic_vector.metadata.get("semantic_features", {}).get("general_score", 0.0)
        
        # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        logger.info(f"ë³µì¡ì„± ì ìˆ˜: {complexity_score:.3f}, ì¼ë°˜ì„± ì ìˆ˜: {general_score:.3f}")
        
        # ë³µì¡ì„± ì ìˆ˜ê°€ ë†’ê³  ì¼ë°˜ì„± ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ë³µì¡í•œ ë¬¸ì œë¡œ ë¶„ë¥˜
        if complexity_score > 0.3 and general_score < 0.2:
            logger.info(f"ë³µì¡ì„± ì ìˆ˜ê°€ ë†’ìŒ ({complexity_score:.3f}), ë³µì¡í•œ ë¬¸ì œë¡œ ë¶„ë¥˜")
            return SemanticFrame.COMPLEX_PROBLEM
        
        # ì¼ë°˜ì„± ì ìˆ˜ê°€ ë†’ê³  ë‹¤ë¥¸ ì ìˆ˜ë“¤ì´ ë‚®ìœ¼ë©´ ì¼ë°˜ì  ìƒí™©ìœ¼ë¡œ ë¶„ë¥˜
        if general_score > 0.3 and complexity_score < 0.2 and semantic_vector.vector[0:75].mean() < 0.2:
            logger.info(f"ì¼ë°˜ì„± ì ìˆ˜ê°€ ë†’ìŒ ({general_score:.3f}), ì¼ë°˜ì  ìƒí™©ìœ¼ë¡œ ë¶„ë¥˜")
            return SemanticFrame.GENERAL_SITUATION
        
        # ê¸°ì¡´ ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­
        for frame, frame_vector in self.semantic_frames.items():
            similarity = self._calculate_cosine_similarity(
                semantic_vector.vector, frame_vector
            )
            
            # Day 3: ë³µì¡í•œ ë¬¸ì œ í”„ë ˆì„ì— ëŒ€í•œ ì¶”ê°€ ë³´ì •
            if frame == SemanticFrame.COMPLEX_PROBLEM and complexity_score > 0.2:
                similarity *= 1.3
            
            # Day 3: ì¼ë°˜ì  ìƒí™© í”„ë ˆì„ì— ëŒ€í•œ ì¶”ê°€ ë³´ì •
            if frame == SemanticFrame.GENERAL_SITUATION and general_score > 0.1:
                similarity *= 1.2
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = frame
        
        logger.info(f"ìµœì  ë§¤ì¹­ í”„ë ˆì„: {best_match.value}, ìœ ì‚¬ë„: {best_similarity:.3f}")
        return best_match
    
    def _calculate_cosine_similarity(self, vector1: np.ndarray, vector2: np.ndarray) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        dot_product = np.dot(vector1, vector2)
        norm1 = np.linalg.norm(vector1)
        norm2 = np.linalg.norm(vector2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def estimate_confidence(self, semantic_vector: SemanticVector, matched_frame: SemanticFrame) -> float:
        """ì‹ ë¢°ë„ ì¶”ì • - Day 5 ê³ ë„í™”ëœ ë°©ì‹ (ì •ëŸ‰ì  ì§ˆì  ì í”„)"""
        # ë²¡í„°ì˜ ì‹ ë¢°ë„ì™€ í”„ë ˆì„ ë§¤ì¹­ ìœ ì‚¬ë„ë¥¼ ê²°í•©
        frame_vector = self.semantic_frames[matched_frame]
        similarity = self._calculate_cosine_similarity(semantic_vector.vector, frame_vector)
        
        # Day 5: í”„ë ˆì„ë³„ ê°€ì¤‘ì¹˜ ì •ê·œí™” (ëª¨ë“  í”„ë ˆì„ì´ ë™ë“±í•œ ê¸°íšŒë¥¼ ê°€ì§€ë„ë¡)
        frame_weights = {
            SemanticFrame.ETHICAL_DILEMMA: 1.1,      # Day 5: ê°€ì¤‘ì¹˜ ì¦ê°€
            SemanticFrame.PRACTICAL_DECISION: 1.1,   # Day 5: ê°€ì¤‘ì¹˜ ì¦ê°€
            SemanticFrame.CONFLICT_RESOLUTION: 1.1,  # Day 5: ê°€ì¤‘ì¹˜ ì¦ê°€
            SemanticFrame.COMPLEX_PROBLEM: 1.1,      # Day 5: ê°€ì¤‘ì¹˜ ì¦ê°€
            SemanticFrame.GENERAL_SITUATION: 1.1     # Day 5: ê°€ì¤‘ì¹˜ ì¦ê°€
        }
        
        weight = frame_weights.get(matched_frame, 1.0)
        
        # Day 5: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚° ì‚¬ìš©
        features = semantic_vector.metadata.get("semantic_features", {})
        context = semantic_vector.metadata.get("context_elements", {})
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°
        context_confidence = self._calculate_context_based_confidence(features, context)
        
        # Day 5: ìœ ì‚¬ë„ ê¸°ë°˜ ë³´ì • ê°•í™” (ì»¨í…ìŠ¤íŠ¸ 70%, ìœ ì‚¬ë„ 30%ë¡œ ì¡°ì •)
        confidence = (context_confidence * 0.7 + similarity * 0.3) * weight
        
        # Day 5: í”„ë ˆì„ë³„ ì •ê·œí™” ë¡œì§ ë„ì…
        frame_normalization = self._calculate_frame_normalization(matched_frame, features, similarity)
        confidence *= frame_normalization
        
        # Day 5: ë³µì¡ì„± í”„ë ˆì„ì¼ ë•Œ ì¶”ê°€ ë³´ì • (ê°•í™”)
        if matched_frame == SemanticFrame.COMPLEX_PROBLEM:
            complexity_score = semantic_vector.vector[75:90].mean()
            if complexity_score > 0.5:
                confidence *= 1.35  # Day 5: ë³´ì • ê°•í™”
            elif complexity_score > 0.3:
                confidence *= 1.25
        
        # Day 5: ì¼ë°˜ì  ìƒí™©ì¼ ë•Œ ê¸°ë³¸ ì‹ ë¢°ë„ ë³´ì¥ (ê°•í™”)
        if matched_frame == SemanticFrame.GENERAL_SITUATION:
            confidence = max(confidence, 0.45)  # Day 5: ìµœì†Œ ì‹ ë¢°ë„ ì¦ê°€
        
        # Day 5: ìœ¤ë¦¬ì  ë”œë ˆë§ˆì¼ ë•Œ ì¶”ê°€ ë³´ì • (ê°•í™”)
        if matched_frame == SemanticFrame.ETHICAL_DILEMMA:
            ethical_score = features.get("ethical_score", 0.0)
            if ethical_score > 0.5:
                confidence *= 1.3  # Day 5: ë³´ì • ê°•í™”
            elif ethical_score > 0.3:
                confidence *= 1.2
        
        # Day 5: ì‹¤ìš©ì  ê²°ì •ì¼ ë•Œ ì¶”ê°€ ë³´ì • (ê°•í™”)
        if matched_frame == SemanticFrame.PRACTICAL_DECISION:
            practical_score = features.get("practical_score", 0.0)
            if practical_score > 0.5:
                confidence *= 1.25  # Day 5: ë³´ì • ê°•í™”
            elif practical_score > 0.3:
                confidence *= 1.15
        
        # Day 5: ê°ˆë“± í•´ê²°ì¼ ë•Œ ì¶”ê°€ ë³´ì • (ê°•í™”)
        if matched_frame == SemanticFrame.CONFLICT_RESOLUTION:
            conflict_score = features.get("conflict_score", 0.0)
            if conflict_score > 0.5:
                confidence *= 1.25  # Day 5: ë³´ì • ê°•í™”
            elif conflict_score > 0.3:
                confidence *= 1.15
        
        # Day 5: ë†’ì€ ìœ ì‚¬ë„ì¼ ë•Œ ì¶”ê°€ ë³´ì • (ê°•í™”)
        if similarity > 0.7:
            confidence *= 1.25  # Day 5: ë³´ì • ê°•í™”
        elif similarity > 0.5:
            confidence *= 1.15
        
        # Day 5: ì»¨í…ìŠ¤íŠ¸ ë³´ë„ˆìŠ¤ ì •ê·œí™” (ê³¼ë„í•œ ë³´ì • ë°©ì§€)
        confidence = self._normalize_context_bonus(confidence, features, context)
        
        # Day 5: ìµœì†Œ ì‹ ë¢°ë„ ë³´ì¥ (0.45ë¡œ ì¦ê°€)
        min_confidence = 0.45
        confidence = max(confidence, min_confidence)
        
        # Day 5: ì‹ ë¢°ë„ ìƒí•œ 0.85ë¡œ ì¦ê°€
        return min(confidence, 0.85)
    
    def _calculate_frame_normalization(self, matched_frame: SemanticFrame, features: Dict[str, float], similarity: float) -> float:
        """Day 5: í”„ë ˆì„ë³„ ì •ê·œí™” ë¡œì§ (ê³ ë„í™”)"""
        # ê¸°ë³¸ ì •ê·œí™” ê³„ìˆ˜
        normalization_factor = 1.0
        
        # í”„ë ˆì„ë³„ íŠ¹ì„±ì— ë”°ë¥¸ ì •ê·œí™” (Day 5: ê°•í™”)
        if matched_frame == SemanticFrame.ETHICAL_DILEMMA:
            # ìœ¤ë¦¬ì  ë”œë ˆë§ˆ: ìœ¤ë¦¬ì  ìš”ì†Œì™€ ê°ˆë“± ìš”ì†Œê°€ ë†’ì„ ë•Œ ì •ê·œí™” ê°•í™”
            ethical_score = features.get("ethical_score", 0.0)
            conflict_score = features.get("conflict_score", 0.0)
            if ethical_score > 0.5 and conflict_score > 0.3:
                normalization_factor = 1.25  # Day 5: ì¦ê°€
            elif ethical_score > 0.3:
                normalization_factor = 1.15  # Day 5: ì¦ê°€
        
        elif matched_frame == SemanticFrame.PRACTICAL_DECISION:
            # ì‹¤ìš©ì  ê²°ì •: ì‹¤ìš©ì  ìš”ì†Œì™€ ì˜ì‚¬ê²°ì • ìš”ì†Œê°€ ë†’ì„ ë•Œ ì •ê·œí™” ê°•í™”
            practical_score = features.get("practical_score", 0.0)
            decision_score = features.get("decision_score", 0.0)
            if practical_score > 0.5 and decision_score > 0.3:
                normalization_factor = 1.25  # Day 5: ì¦ê°€
            elif practical_score > 0.3:
                normalization_factor = 1.15  # Day 5: ì¦ê°€
        
        elif matched_frame == SemanticFrame.CONFLICT_RESOLUTION:
            # ê°ˆë“± í•´ê²°: ê°ˆë“± ìš”ì†Œì™€ ì˜ì‚¬ê²°ì • ìš”ì†Œê°€ ë†’ì„ ë•Œ ì •ê·œí™” ê°•í™”
            conflict_score = features.get("conflict_score", 0.0)
            decision_score = features.get("decision_score", 0.0)
            if conflict_score > 0.5 and decision_score > 0.3:
                normalization_factor = 1.25  # Day 5: ì¦ê°€
            elif conflict_score > 0.3:
                normalization_factor = 1.15  # Day 5: ì¦ê°€
        
        elif matched_frame == SemanticFrame.COMPLEX_PROBLEM:
            # ë³µì¡í•œ ë¬¸ì œ: ë³µì¡ì„± ìš”ì†Œê°€ ë†’ì„ ë•Œ ì •ê·œí™” ê°•í™”
            complexity_score = features.get("complexity_score", 0.0)
            if complexity_score > 0.5:
                normalization_factor = 1.35  # Day 5: ì¦ê°€
            elif complexity_score > 0.3:
                normalization_factor = 1.25  # Day 5: ì¦ê°€
        
        elif matched_frame == SemanticFrame.GENERAL_SITUATION:
            # ì¼ë°˜ì  ìƒí™©: ì¼ë°˜ì„± ìš”ì†Œê°€ ë†’ì„ ë•Œ ì •ê·œí™” ê°•í™”
            general_score = features.get("general_score", 0.0)
            if general_score > 0.5:
                normalization_factor = 1.25  # Day 5: ì¦ê°€
            elif general_score > 0.3:
                normalization_factor = 1.15  # Day 5: ì¦ê°€
        
        # ìœ ì‚¬ë„ì— ë”°ë¥¸ ì¶”ê°€ ì •ê·œí™” (Day 5: ê°•í™”)
        if similarity > 0.7:
            normalization_factor *= 1.15  # Day 5: ì¦ê°€
        elif similarity > 0.5:
            normalization_factor *= 1.1  # Day 5: ì¦ê°€
        
        return normalization_factor
    
    def _normalize_context_bonus(self, confidence: float, features: Dict[str, float], context: Dict[str, Any]) -> float:
        """Day 5: ì»¨í…ìŠ¤íŠ¸ ë³´ë„ˆìŠ¤ ì •ê·œí™” (ê³¼ë„í•œ ë³´ì • ë°©ì§€)"""
        # ì»¨í…ìŠ¤íŠ¸ ë³´ë„ˆìŠ¤ ì ìˆ˜ ê³„ì‚°
        context_bonus = 0.0
        
        # ì´í•´ê´€ê³„ì ìˆ˜ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤ (ìµœëŒ€ 0.15)
        stakeholder_count = len(context.get("actors", []))
        if stakeholder_count > 2:
            context_bonus += min(0.15, stakeholder_count * 0.05)
        elif stakeholder_count > 1:
            context_bonus += min(0.1, stakeholder_count * 0.05)
        
        # ìƒí™© ë³µì¡ì„±ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤ (ìµœëŒ€ 0.2)
        complexity_score = features.get("complexity_score", 0.0)
        if complexity_score > 0.5:
            context_bonus += min(0.2, complexity_score * 0.3)
        elif complexity_score > 0.3:
            context_bonus += min(0.15, complexity_score * 0.3)
        
        # ì‹œê°„ì  ì••ë°•ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤ (ìµœëŒ€ 0.1)
        temporal_aspects = context.get("temporal_aspects", [])
        urgency_keywords = ["ê¸´ê¸‰", "ì‹œê¸‰", "ì¦‰ì‹œ", "ë¹ ë¥¸", "ì‹ ì†", "ê¸‰í•œ", "ê¸´ê¸‰í•œ"]
        if any(urgency in str(temporal_aspects) for urgency in urgency_keywords):
            context_bonus += 0.1
        
        # ìœ¤ë¦¬ì  ìš”ì†Œ ê°•ë„ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤ (ìµœëŒ€ 0.15)
        ethical_score = features.get("ethical_score", 0.0)
        if ethical_score > 0.6:
            context_bonus += min(0.15, ethical_score * 0.2)
        elif ethical_score > 0.3:
            context_bonus += min(0.1, ethical_score * 0.2)
        
        # ê°ˆë“± ìš”ì†Œ ê°•ë„ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤ (ìµœëŒ€ 0.15)
        conflict_score = features.get("conflict_score", 0.0)
        if conflict_score > 0.6:
            context_bonus += min(0.15, conflict_score * 0.2)
        elif conflict_score > 0.3:
            context_bonus += min(0.1, conflict_score * 0.2)
        
        # ì¼ë°˜ì  ìƒí™©ì—ì„œì˜ ê¸°ë³¸ ë³´ë„ˆìŠ¤ (ìµœëŒ€ 0.1)
        general_score = features.get("general_score", 0.0)
        if general_score > 0.3:
            context_bonus += min(0.1, general_score * 0.2)
        
        # ì‹¤ìš©ì  ìš”ì†Œ ê°•ë„ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤ (ìµœëŒ€ 0.1)
        practical_score = features.get("practical_score", 0.0)
        if practical_score > 0.5:
            context_bonus += min(0.1, practical_score * 0.15)
        
        # ì˜ì‚¬ê²°ì • ìš”ì†Œ ê°•ë„ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤ (ìµœëŒ€ 0.1)
        decision_score = features.get("decision_score", 0.0)
        if decision_score > 0.5:
            context_bonus += min(0.1, decision_score * 0.15)
        
        # ì»¨í…ìŠ¤íŠ¸ ë³´ë„ˆìŠ¤ ì •ê·œí™” (ìµœëŒ€ 0.3ìœ¼ë¡œ ì œí•œ)
        normalized_context_bonus = min(context_bonus, 0.3)
        
        # ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚° (ì»¨í…ìŠ¤íŠ¸ ë³´ë„ˆìŠ¤ ë°˜ì˜)
        final_confidence = confidence + normalized_context_bonus
        
        return min(final_confidence, 0.8)  # ìµœëŒ€ 0.8ë¡œ ì œí•œ
    
    def analyze_situation(self, situation: str) -> Dict[str, Any]:
        """ìƒí™© ë¶„ì„ - Day 4 ê°œì„ ëœ ë°©ì‹"""
        logger.info(f"ìƒí™© ë¶„ì„ ì‹œì‘: {situation[:50]}...")
        
        # 1. ìƒí™© ë²¡í„°í™”
        situation_vector = self.encode_semantics(situation)
        
        # 2. ë§¥ë½ ìš”ì†Œ ì¶”ì¶œ
        context_elements = self._extract_context_elements(situation)
        
        # 3. ì˜ë¯¸ì  ìœ ì‚¬ë„ ë¹„êµ
        matched_frame = self.match_to_known_frames(situation_vector)
        
        # 4. Day 4: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‹ ë¢°ë„ ì¶”ì •
        confidence = self.estimate_confidence(situation_vector, matched_frame)
        
        # 5. Day 4: ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
        situation_vector.metadata["context_elements"] = context_elements
        
        result = {
            "situation_vector": situation_vector,
            "matched_frame": matched_frame,
            "confidence": confidence,
            "context_elements": context_elements,
            "semantic_similarity": self._calculate_cosine_similarity(
                situation_vector.vector, 
                self.semantic_frames[matched_frame]
            )
        }
        
        logger.info(f"ìƒí™© ë¶„ì„ ì™„ë£Œ: {matched_frame.value}, ì‹ ë¢°ë„: {confidence:.2f}")
        return result
    
    def _extract_context_elements(self, situation: str) -> Dict[str, Any]:
        """ë§¥ë½ ìš”ì†Œ ì¶”ì¶œ"""
        context = {
            "actors": [],
            "actions": [],
            "motivations": [],
            "circumstances": [],
            "temporal_aspects": [],
            "spatial_aspects": []
        }
        
        # í–‰ìœ„ì ì¶”ì¶œ
        actor_patterns = [
            r"(\w+ê°€|\w+ì€|\w+ëŠ”|\w+ì—ê²Œ|\w+ì™€|\w+ê³¼)",
            r"(\w+ë“¤|\w+ë“¤ê»˜|\w+ë“¤ì—ê²Œ)"
        ]
        
        for pattern in actor_patterns:
            matches = re.findall(pattern, situation)
            context["actors"].extend(matches)
        
        # í–‰ìœ„ ì¶”ì¶œ
        action_patterns = [
            r"(\w+í•´ì•¼|\w+í•´ì•¼ í•˜ëŠ”|\w+í•´ì•¼ í•˜ëŠ” ìƒí™©)",
            r"(\w+í•˜ë ¤ê³ |\w+í•˜ë ¤ëŠ”|\w+í•˜ë ¤ëŠ” ìƒí™©)",
            r"(\w+í•´ì•¼|\w+í•´ì•¼ í•˜ëŠ”|\w+í•´ì•¼ í•˜ëŠ” ìƒí™©)"
        ]
        
        for pattern in action_patterns:
            matches = re.findall(pattern, situation)
            context["actions"].extend(matches)
        
        # ë™ê¸° ì¶”ì¶œ
        motivation_keywords = ["ìœ„í•´", "ë•Œë¬¸ì—", "ì´ìœ ë¡œ", "ëª©ì ìœ¼ë¡œ", "ê²°ê³¼ë¡œ"]
        for keyword in motivation_keywords:
            if keyword in situation:
                context["motivations"].append(keyword)
        
        # ìƒí™© ì¶”ì¶œ
        circumstance_keywords = ["ìƒí™©", "ê²½ìš°", "ë•Œ", "ìƒí™©ì—ì„œ", "ê²½ìš°ì—"]
        for keyword in circumstance_keywords:
            if keyword in situation:
                context["circumstances"].append(keyword)
        
        return context

async def test_semantic_vector_engine():
    """ì˜ë¯¸ ë²¡í„° ì—”ì§„ í…ŒìŠ¤íŠ¸ - í™•ì¥ëœ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""
    print("=" * 80)
    print("ğŸ§  SemanticVectorEngine í…ŒìŠ¤íŠ¸ ì‹œì‘ (Day 2 ê°œì„  ë²„ì „)")
    print("=" * 80)
    
    engine = SemanticVectorEngine()
    
    # í…ŒìŠ¤íŠ¸ ìƒí™©ë“¤ (ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤)
    test_situations = [
        # ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ìƒí™©ë“¤
        {
            "situation": "íšŒì‚¬ì˜ AI ì‹œìŠ¤í…œì´ ê³ ê° ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê°œì¸í™”ëœ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ì§€ë§Œ, ê°œì¸ì •ë³´ ë³´í˜¸ì— ëŒ€í•œ ìš°ë ¤ê°€ ì œê¸°ë˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "expected_frame": "ethical_dilemma",
            "description": "ê°œì¸ì •ë³´ ë³´í˜¸ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ"
        },
        {
            "situation": "ì§ì›ì´ íšŒì‚¬ì˜ ë¹„ë°€ì„ ì™¸ë¶€ì— ìœ ì¶œí•˜ë ¤ê³  í•  ë•Œ, ì´ë¥¼ ë§‰ì•„ì•¼ í•˜ëŠ”ì§€ ê³ ë¯¼í•˜ëŠ” ìƒí™©ì…ë‹ˆë‹¤.",
            "expected_frame": "ethical_dilemma", 
            "description": "ë¹„ë°€ ìœ ì¶œ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ"
        },
        {
            "situation": "ê±°ì§“ë§ì„ í•´ì•¼ í•˜ëŠ” ìƒí™©ì—ì„œ ì§„ì‹¤ì„ ë§í• ì§€ ê±°ì§“ë§ì„ í• ì§€ ê³ ë¯¼í•˜ëŠ” ìƒí™©ì…ë‹ˆë‹¤.",
            "expected_frame": "ethical_dilemma",
            "description": "ì§„ì‹¤ê³¼ ê±°ì§“ë§ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ"
        },
        
        # ì‹¤ìš©ì  ê²°ì • ìƒí™©ë“¤
        {
            "situation": "íš¨ìœ¨ì„±ì„ ìœ„í•´ ì¼ë¶€ ì§ì›ì„ í•´ê³ í•´ì•¼ í•˜ëŠ” ìƒí™©ì—ì„œ, ê³µì •ì„±ê³¼ íš¨ìœ¨ì„± ì‚¬ì´ì—ì„œ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.",
            "expected_frame": "practical_decision",
            "description": "íš¨ìœ¨ì„± vs ê³µì •ì„± ì‹¤ìš©ì  ê²°ì •"
        },
        {
            "situation": "ë¹„ìš©ì ˆê°ì„ ìœ„í•´ í’ˆì§ˆì„ ë‚®ì¶°ì•¼ í•˜ëŠ” ìƒí™©ì—ì„œ ìˆ˜ìµê³¼ í’ˆì§ˆ ì‚¬ì´ì—ì„œ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.",
            "expected_frame": "practical_decision",
            "description": "ìˆ˜ìµ vs í’ˆì§ˆ ì‹¤ìš©ì  ê²°ì •"
        },
        
        # ê°ˆë“± í•´ê²° ìƒí™©ë“¤
        {
            "situation": "íŒ€ì›ë“¤ ê°„ì˜ ì˜ê²¬ ì¶©ëŒì´ ë°œìƒí–ˆì„ ë•Œ, ì´ë¥¼ ì¡°ì •í•˜ê³  í•´ê²°í•´ì•¼ í•˜ëŠ” ìƒí™©ì…ë‹ˆë‹¤.",
            "expected_frame": "conflict_resolution",
            "description": "íŒ€ ê°ˆë“± í•´ê²°"
        },
        {
            "situation": "ê³ ê°ê³¼ì˜ ë¶„ìŸì´ ë°œìƒí–ˆì„ ë•Œ, ì´ë¥¼ ì¤‘ì¬í•˜ê³  í•´ê²°í•´ì•¼ í•˜ëŠ” ìƒí™©ì…ë‹ˆë‹¤.",
            "expected_frame": "conflict_resolution",
            "description": "ê³ ê° ë¶„ìŸ í•´ê²°"
        },
        
        # ë³µì¡í•œ ë¬¸ì œ ìƒí™©ë“¤
        {
            "situation": "ë‹¤ì–‘í•œ ì´í•´ê´€ê³„ìë“¤ì´ ì°¸ì—¬í•˜ëŠ” ë³µì¡í•œ í”„ë¡œì íŠ¸ì—ì„œ ì—¬ëŸ¬ ê´€ì ì„ í†µí•©í•˜ì—¬ í•´ê²°ì±…ì„ ì°¾ì•„ì•¼ í•˜ëŠ” ìƒí™©ì…ë‹ˆë‹¤.",
            "expected_frame": "complex_problem",
            "description": "ë‹¤ë©´ì  ë³µì¡ ë¬¸ì œ"
        },
        {
            "situation": "ìœ¤ë¦¬ì , ì‹¤ìš©ì , ë²•ì  ìš”ì†Œê°€ ëª¨ë‘ ì–½í˜€ìˆëŠ” ë³µí•©ì ì¸ ë¬¸ì œë¥¼ í•´ê²°í•´ì•¼ í•˜ëŠ” ìƒí™©ì…ë‹ˆë‹¤.",
            "expected_frame": "complex_problem",
            "description": "ë³µí•©ì  ìœ¤ë¦¬ ë¬¸ì œ"
        },
        
        # ì¼ë°˜ì  ìƒí™©ë“¤
        {
            "situation": "ì¼ìƒì ì¸ ì—…ë¬´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ìƒí™©ì…ë‹ˆë‹¤.",
            "expected_frame": "general_situation",
            "description": "ì¼ë°˜ì  ì—…ë¬´ ìƒí™©"
        },
        {
            "situation": "ë‹¨ìˆœí•œ ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” ìƒí™©ì…ë‹ˆë‹¤.",
            "expected_frame": "general_situation",
            "description": "ë‹¨ìˆœ ì •ë³´ ì „ë‹¬"
        }
    ]
    
    # ê²°ê³¼ í†µê³„
    total_tests = len(test_situations)
    correct_classifications = 0
    high_confidence_tests = 0
    confidence_scores = []
    
    print(f"\nğŸ“Š ì´ {total_tests}ê°œì˜ í…ŒìŠ¤íŠ¸ ìƒí™©ì„ ë¶„ì„í•©ë‹ˆë‹¤...\n")
    
    for i, test_case in enumerate(test_situations, 1):
        situation = test_case["situation"]
        expected_frame = test_case["expected_frame"]
        description = test_case["description"]
        
        print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ {i}: {description}")
        print(f"   ìƒí™©: {situation[:60]}...")
        
        # ì˜ë¯¸ ë²¡í„° ì—”ì§„ìœ¼ë¡œ ë¶„ì„
        result = engine.analyze_situation(situation)
        
        matched_frame = result['matched_frame'].value
        confidence = result['confidence']
        similarity = result['semantic_similarity']
        
        # ê²°ê³¼ í‰ê°€
        is_correct = matched_frame == expected_frame
        is_high_confidence = confidence >= 0.5
        
        if is_correct:
            correct_classifications += 1
        if is_high_confidence:
            high_confidence_tests += 1
        
        confidence_scores.append(confidence)
        
        # ê²°ê³¼ ì¶œë ¥
        status_icon = "âœ…" if is_correct else "âŒ"
        confidence_icon = "ğŸ”¥" if is_high_confidence else "âš ï¸"
        
        print(f"   {status_icon} ë§¤ì¹­ëœ í”„ë ˆì„: {matched_frame} (ì˜ˆìƒ: {expected_frame})")
        print(f"   {confidence_icon} ì‹ ë¢°ë„: {confidence:.3f}")
        print(f"   ğŸ“ˆ ì˜ë¯¸ì  ìœ ì‚¬ë„: {similarity:.3f}")
        print(f"   ğŸ§  ë²¡í„° ì°¨ì›: {result['situation_vector'].dimension}")
        print(f"   ğŸ‘¥ ì´í•´ê´€ê³„ì: {len(result['context_elements']['actors'])}ëª…")
        print()
    
    # í†µê³„ ìš”ì•½
    accuracy = correct_classifications / total_tests * 100
    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0
    high_confidence_rate = high_confidence_tests / total_tests * 100
    
    print("=" * 80)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)
    print(f"ğŸ¯ ì •í™•ë„: {accuracy:.1f}% ({correct_classifications}/{total_tests})")
    print(f"ğŸ”¥ ë†’ì€ ì‹ ë¢°ë„ ë¹„ìœ¨: {high_confidence_rate:.1f}% ({high_confidence_tests}/{total_tests})")
    print(f"ğŸ“ˆ í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
    print(f"ğŸ“Š ì‹ ë¢°ë„ ë²”ìœ„: {min(confidence_scores):.3f} - {max(confidence_scores):.3f}")
    
    # ì„±ëŠ¥ í‰ê°€
    if accuracy >= 80 and avg_confidence >= 0.4:
        print("\nğŸ‰ ì„±ëŠ¥ í‰ê°€: ìš°ìˆ˜ (Day 2 ëª©í‘œ ë‹¬ì„±)")
    elif accuracy >= 60 and avg_confidence >= 0.3:
        print("\nâœ… ì„±ëŠ¥ í‰ê°€: ì–‘í˜¸ (Day 2 ëª©í‘œ ë¶€ë¶„ ë‹¬ì„±)")
    else:
        print("\nâš ï¸ ì„±ëŠ¥ í‰ê°€: ê°œì„  í•„ìš” (Day 2 ëª©í‘œ ë¯¸ë‹¬ì„±)")
    
    print("\n" + "=" * 80)
    print("âœ… SemanticVectorEngine í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("ğŸ‰ ì˜ë¯¸ ë²¡í„° ê¸°ë°˜ ë¶„ì„ ì‹œìŠ¤í…œ Day 2 ê°œì„  ì™„ë£Œ!")
    print("=" * 80)

if __name__ == "__main__":
    asyncio.run(test_semantic_vector_engine()) 