#!/usr/bin/env python3
"""
ë©”íƒ€ ë£¨í”„ ì‹œìŠ¤í…œ - ê°œì„  ê²°ê³¼ ì¬í‰ê°€ ë° í•™ìŠµ íš¨ê³¼ ì¸¡ì •
"""

from datetime import datetime
from typing import Any, Dict


class MetaLoopSystem:
    """ê°œì„  ê²°ê³¼ë¥¼ ë‹¤ì‹œ í‰ê°€í•˜ëŠ” ë©”íƒ€ ë£¨í”„ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.meta_evaluation_history = []
        self.improvement_tracking = {}

    def evaluate_improvement_effect(
        self,
        original_response: str,
        improved_response: str,
        user_question: str,
        original_evaluation: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ê°œì„  íš¨ê³¼ í‰ê°€"""

        print("ğŸ”„ ë©”íƒ€ ë£¨í”„: ê°œì„  íš¨ê³¼ í‰ê°€ ì‹œì‘")

        # ê°œì„ ëœ ì‘ë‹µì— ëŒ€í•œ ìƒˆë¡œìš´ í‰ê°€
        from ..evaluation.evaluator import chatgpt_evaluator

        new_evaluation = chatgpt_evaluator.evaluate_response(improved_response, user_question)

        # ê°œì„  íš¨ê³¼ ë¶„ì„
        improvement_analysis = self._analyze_improvement_effect(original_evaluation, new_evaluation)

        # ë©”íƒ€ í‰ê°€ ê²°ê³¼
        meta_result = {
            "timestamp": datetime.now().isoformat(),
            "original_evaluation": original_evaluation,
            "new_evaluation": new_evaluation,
            "improvement_analysis": improvement_analysis,
            "meta_score": improvement_analysis.get("overall_improvement", 0.0),
        }

        # ë©”íƒ€ í‰ê°€ ê¸°ë¡ ì €ì¥
        self.meta_evaluation_history.append(meta_result)

        print("âœ… ë©”íƒ€ ë£¨í”„: ê°œì„  íš¨ê³¼ í‰ê°€ ì™„ë£Œ")
        print(f"   ğŸ“ˆ ì „ì²´ ê°œì„ ë„: {improvement_analysis.get('overall_improvement', 0):.3f}")
        print(f"   ğŸ¯ ê°œì„ ëœ ì˜ì—­: {len(improvement_analysis.get('improved_dimensions', []))}ê°œ")

        return meta_result

    def _analyze_improvement_effect(self, original_eval: Dict[str, Any], new_eval: Dict[str, Any]) -> Dict[str, Any]:
        """ê°œì„  íš¨ê³¼ ë¶„ì„"""

        original_scores = original_eval.get("scores", {})
        new_scores = new_eval.get("scores", {})

        # ê° ì°¨ì›ë³„ ê°œì„ ë„ ê³„ì‚°
        dimension_improvements = {}
        improved_dimensions = []
        declined_dimensions = []

        for dimension in original_scores:
            original_score = original_scores.get(dimension, 0)
            new_score = new_scores.get(dimension, 0)
            improvement = new_score - original_score

            dimension_improvements[dimension] = {
                "original": original_score,
                "new": new_score,
                "improvement": improvement,
                "improvement_percentage": ((improvement / original_score * 100) if original_score > 0 else 0),
            }

            if improvement > 0:
                improved_dimensions.append(dimension)
            elif improvement < 0:
                declined_dimensions.append(dimension)

        # ì „ì²´ ê°œì„ ë„ ê³„ì‚°
        total_original = sum(original_scores.values())
        total_new = sum(new_scores.values())
        overall_improvement = (total_new - total_original) / len(original_scores) if original_scores else 0

        # ê°œì„  ì„±ê³µ ì—¬ë¶€ íŒë‹¨
        improvement_success = overall_improvement > 0.1  # 10% ì´ìƒ ê°œì„ 

        return {
            "dimension_improvements": dimension_improvements,
            "improved_dimensions": improved_dimensions,
            "declined_dimensions": declined_dimensions,
            "overall_improvement": overall_improvement,
            "improvement_success": improvement_success,
            "total_original_score": total_original,
            "total_new_score": total_new,
        }

    def generate_improvement_feedback(self, meta_result: Dict[str, Any]) -> Dict[str, Any]:
        """ê°œì„  í”¼ë“œë°± ìƒì„±"""

        analysis = meta_result.get("improvement_analysis", {})

        feedback = {
            "timestamp": datetime.now().isoformat(),
            "improvement_success": analysis.get("improvement_success", False),
            "overall_improvement": analysis.get("overall_improvement", 0),
            "recommendations": [],
            "lessons_learned": [],
        }

        # ê°œì„  ì„±ê³µ ì‹œ
        if analysis.get("improvement_success", False):
            feedback["recommendations"].append("ê°œì„ ì´ ì„±ê³µì ìœ¼ë¡œ ì´ë£¨ì–´ì¡ŒìŠµë‹ˆë‹¤. ì´ íŒ¨í„´ì„ í•™ìŠµì— ë°˜ì˜í•˜ì„¸ìš”.")
            feedback["lessons_learned"].append("ì´ë²ˆ ê°œì„  ë°©ë²•ì´ íš¨ê³¼ì ì´ì—ˆìŠµë‹ˆë‹¤.")
        else:
            feedback["recommendations"].append("ê°œì„ ì´ ê¸°ëŒ€ì— ë¯¸ì¹˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì ‘ê·¼ ë°©ë²•ì„ ì‹œë„í•´ë³´ì„¸ìš”.")
            feedback["lessons_learned"].append("ì´ë²ˆ ê°œì„  ë°©ë²•ì€ íš¨ê³¼ì ì´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ê°œì„ ëœ ì˜ì—­ ë¶„ì„
        improved_dims = analysis.get("improved_dimensions", [])
        if improved_dims:
            feedback["recommendations"].append(f"ë‹¤ìŒ ì˜ì—­ì—ì„œ ê°œì„ ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(improved_dims)}")

        # ì•…í™”ëœ ì˜ì—­ ë¶„ì„
        declined_dims = analysis.get("declined_dimensions", [])
        if declined_dims:
            feedback["recommendations"].append(f"ë‹¤ìŒ ì˜ì—­ì—ì„œ ì•…í™”ê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(declined_dims)}")

        return feedback

    def get_meta_learning_statistics(self) -> Dict[str, Any]:
        """ë©”íƒ€ í•™ìŠµ í†µê³„"""

        if not self.meta_evaluation_history:
            return {"total_meta_evaluations": 0, "success_rate": 0.0}

        total_evaluations = len(self.meta_evaluation_history)
        successful_improvements = sum(
            1
            for result in self.meta_evaluation_history
            if result.get("improvement_analysis", {}).get("improvement_success", False)
        )

        success_rate = successful_improvements / total_evaluations if total_evaluations > 0 else 0

        # í‰ê·  ê°œì„ ë„
        avg_improvement = (
            sum(
                result.get("improvement_analysis", {}).get("overall_improvement", 0)
                for result in self.meta_evaluation_history
            )
            / total_evaluations
            if total_evaluations > 0
            else 0
        )

        return {
            "total_meta_evaluations": total_evaluations,
            "successful_improvements": successful_improvements,
            "success_rate": success_rate,
            "average_improvement": avg_improvement,
            "recent_meta_evaluations": self.meta_evaluation_history[-5:],  # ìµœê·¼ 5ê°œ
        }


# ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
meta_loop_system = MetaLoopSystem()
