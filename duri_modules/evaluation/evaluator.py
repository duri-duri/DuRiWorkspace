#!/usr/bin/env python3
"""
ChatGPT 6ì°¨ì› í‰ê°€ ì‹œìŠ¤í…œ ëª¨ë“ˆ
"""

import re
from datetime import datetime
from typing import Any, Dict, List


class ChatGPTEvaluator:
    """ChatGPTì˜ 6ì°¨ì› ë‹µë³€ í‰ê°€ ì‹œìŠ¤í…œ"""

    EVALUATION_CRITERIA = {
        "correctness": "ì •í™•ì„± - ê¸°ìˆ ì  ì •í™•ë„ì™€ ì‚¬ì‹¤ì„±",
        "relevance": "ê´€ë ¨ì„± - ì§ˆë¬¸ê³¼ì˜ ì—°ê´€ì„±",
        "depth": "ê¹Šì´ - ìƒì„¸í•œ ì„¤ëª…ê³¼ ë¶„ì„",
        "structure": "êµ¬ì¡° - ë…¼ë¦¬ì  êµ¬ì„±ê³¼ íë¦„",
        "clarity": "ëª…í™•ì„± - ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…",
        "actionability": "ì‹¤í–‰ê°€ëŠ¥ì„± - ì‹¤ìš©ì  ì ìš© ê°€ëŠ¥ì„±",
    }

    def evaluate_response(
        self, duri_response: str, user_question: str
    ) -> Dict[str, Any]:
        """DuRi ì‘ë‹µì„ 6ì°¨ì›ìœ¼ë¡œ í‰ê°€"""

        print(f"ğŸ¤– ChatGPT í‰ê°€ ì‹œì‘: {len(duri_response)}ì ì‘ë‹µ")

        # 6ì°¨ì› ì ìˆ˜ ê³„ì‚°
        scores = self._calculate_6d_scores(duri_response, user_question)

        # ê°œì„  ì œì•ˆ ìƒì„±
        suggestions = self._identify_improvements(duri_response)

        # ì¤‘ìš” ì´ìŠˆ ì‹ë³„
        critical_issues = self._find_critical_issues(duri_response)

        # ì „ì²´ í‰ê°€ ìƒì„±
        overall_assessment = self._generate_overall_assessment(duri_response)

        # ì´ì  ê³„ì‚°
        total_score = sum(scores.values()) / len(scores)

        evaluation_result = {
            "scores": scores,
            "suggestions": suggestions,
            "critical_issues": critical_issues,
            "overall_assessment": overall_assessment,
            "timestamp": datetime.now().isoformat(),
            "total_score": total_score,
        }

        print(f"ğŸ¤– ChatGPT í‰ê°€ ì™„ë£Œ: ì´ì  {total_score:.3f}")
        print(f"   ğŸ“Š ì„¸ë¶€ ì ìˆ˜: {scores}")
        print(f"   ğŸ’¡ ê°œì„  ì œì•ˆ: {suggestions}")

        return evaluation_result

    def _calculate_6d_scores(self, response: str, question: str) -> Dict[str, float]:
        """6ì°¨ì› ì ìˆ˜ ê³„ì‚°"""
        scores = {}

        # ì •í™•ì„± (ê¸°ìˆ ì  ì •í™•ë„)
        technical_accuracy = self._assess_technical_accuracy(response)
        scores["correctness"] = technical_accuracy

        # ê´€ë ¨ì„± (ì§ˆë¬¸ê³¼ì˜ ì—°ê´€ì„±)
        relevance_score = self._assess_relevance(response, question)
        scores["relevance"] = relevance_score

        # ê¹Šì´ (ìƒì„¸í•œ ì„¤ëª…)
        depth_score = self._assess_depth(response)
        scores["depth"] = depth_score

        # êµ¬ì¡° (ë…¼ë¦¬ì  êµ¬ì„±)
        structure_score = self._assess_structure(response)
        scores["structure"] = structure_score

        # ëª…í™•ì„± (ì´í•´í•˜ê¸° ì‰¬ì›€)
        clarity_score = self._assess_clarity(response)
        scores["clarity"] = clarity_score

        # ì‹¤í–‰ê°€ëŠ¥ì„± (ì‹¤ìš©ì  ì ìš©)
        actionability_score = self._assess_actionability(response)
        scores["actionability"] = actionability_score

        return scores

    def _assess_technical_accuracy(self, response: str) -> float:
        """ê¸°ìˆ ì  ì •í™•ë„ í‰ê°€"""
        # ê¸°ìˆ ì  í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
        tech_keywords = [
            "API",
            "HTTP",
            "JSON",
            "async",
            "await",
            "FastAPI",
            "Flask",
            "Python",
        ]
        keyword_count = sum(
            1 for keyword in tech_keywords if keyword.lower() in response.lower()
        )
        return min(keyword_count / len(tech_keywords), 1.0)

    def _assess_relevance(self, response: str, question: str) -> float:
        """ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„± í‰ê°€"""
        # ì§ˆë¬¸ í‚¤ì›Œë“œê°€ ì‘ë‹µì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
        question_words = set(re.findall(r"\w+", question.lower()))
        response_words = set(re.findall(r"\w+", response.lower()))

        if not question_words:
            return 0.0

        overlap = len(question_words.intersection(response_words))
        return min(overlap / len(question_words), 1.0)

    def _assess_depth(self, response: str) -> float:
        """ìƒì„¸í•œ ì„¤ëª… í‰ê°€"""
        # ì‘ë‹µ ê¸¸ì´ì™€ ë³µì¡ì„±
        word_count = len(response.split())
        if word_count < 10:
            return 0.0
        elif word_count < 50:
            return 0.3
        elif word_count < 100:
            return 0.6
        else:
            return 1.0

    def _assess_structure(self, response: str) -> float:
        """ë…¼ë¦¬ì  êµ¬ì¡° í‰ê°€"""
        # êµ¬ì¡°ì  ìš”ì†Œ í™•ì¸
        structure_indicators = [
            "1.",
            "2.",
            "3.",
            "ì²«ì§¸",
            "ë‘˜ì§¸",
            "ì…‹ì§¸",
            "ë‹¨ê³„",
            "ë‹¨ê³„ë³„",
        ]
        indicator_count = sum(
            1 for indicator in structure_indicators if indicator in response
        )
        return min(indicator_count / 3, 1.0)

    def _assess_clarity(self, response: str) -> float:
        """ëª…í™•ì„± í‰ê°€"""
        # ëª…í™•í•œ ì„¤ëª… ìš”ì†Œ
        clarity_indicators = ["ì˜ˆë¥¼ ë“¤ì–´", "ì¦‰", "ë‹¤ì‹œ ë§í•´", "êµ¬ì²´ì ìœ¼ë¡œ", "ì˜ˆì‹œ"]
        indicator_count = sum(
            1 for indicator in clarity_indicators if indicator in response
        )
        return min(indicator_count / 2, 1.0)

    def _assess_actionability(self, response: str) -> float:
        """ì‹¤í–‰ê°€ëŠ¥ì„± í‰ê°€"""
        # ì‹¤ìš©ì  ìš”ì†Œ
        action_indicators = ["ì½”ë“œ", "ì˜ˆì œ", "ì‹¤ì œ", "êµ¬í˜„", "ì‚¬ìš©ë²•", "ë°©ë²•"]
        indicator_count = sum(
            1 for indicator in action_indicators if indicator in response
        )
        return min(indicator_count / 3, 1.0)

    def _identify_improvements(self, response: str) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        suggestions = []

        if len(response.split()) < 20:
            suggestions.append("ë” ìƒì„¸í•œ ì„¤ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤")

        if "ì½”ë“œ" not in response and "ì˜ˆì œ" not in response:
            suggestions.append("ì‹¤ì œ ì½”ë“œ ì˜ˆì œë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”")

        if "ì´ìœ " not in response and "ì™œ" not in response:
            suggestions.append("ì´ìœ ì™€ ê·¼ê±°ë¥¼ ë” ëª…í™•íˆ ì„¤ëª…í•´ë³´ì„¸ìš”")

        if not any(indicator in response for indicator in ["1.", "2.", "ë‹¨ê³„"]):
            suggestions.append("ë‹¨ê³„ë³„ë¡œ êµ¬ì¡°í™”ëœ ì„¤ëª…ì„ ì¶”ê°€í•´ë³´ì„¸ìš”")

        return suggestions

    def _find_critical_issues(self, response: str) -> List[str]:
        """ì¤‘ìš” ì´ìŠˆ ì‹ë³„"""
        issues = []

        if len(response.split()) < 10:
            issues.append("ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤")

        if "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤" in response or "ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in response:
            issues.append("ë¶ˆí™•ì‹¤í•œ ë‹µë³€ì…ë‹ˆë‹¤")

        if len(response) > 1000:
            issues.append("ë‹µë³€ì´ ë„ˆë¬´ ê¸¸ì–´ì„œ í•µì‹¬ì„ ë†“ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤")

        return issues

    def _generate_overall_assessment(self, response: str) -> str:
        """ì „ì²´ í‰ê°€ ìƒì„±"""
        word_count = len(response.split())

        if word_count < 10:
            return "ë‹µë³€ì´ ë„ˆë¬´ ê°„ë‹¨í•©ë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        elif word_count < 50:
            return "ê¸°ë³¸ì ì¸ ë‹µë³€ì…ë‹ˆë‹¤. ë” ìƒì„¸í•œ ì„¤ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤."
        elif word_count < 100:
            return "ì ì ˆí•œ ìˆ˜ì¤€ì˜ ë‹µë³€ì…ë‹ˆë‹¤. ì¼ë¶€ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤."
        else:
            return "ìƒì„¸í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì…ë‹ˆë‹¤. ë§¤ìš° ì¢‹ìŠµë‹ˆë‹¤."


# ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
chatgpt_evaluator = ChatGPTEvaluator()
