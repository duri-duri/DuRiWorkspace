#!/usr/bin/env python3
"""
DORA ë©”íŠ¸ë¦­ ìµìŠ¤í¬í„° - ì™„ì „ ì¬ì‘ì„± (ë§‰íŒ ë¯¸ì„¸ ì¡°ì • 4ê°œ ë°˜ì˜)
- Healthcheckì—ì„œ curl ì˜ì¡´ ì œê±°
- í¬íŠ¸ ë‚´ë¶€í™” í›„ì—ë„ ë°°í¬ ì´ë²¤íŠ¸ í‘¸ì‹œ ê°€ëŠ¥í•˜ê²Œ
- dedup ê´€ì¸¡ì„± ë” íƒ„íƒ„í•˜ê²Œ (ë¼ë²¨Â·TTL ë…¸ì¶œ)
- ì½”ë“œ ì²­ì†Œ(ê¶Œì¥)
"""

import os
import json
import time
import threading
import ipaddress
from datetime import datetime
from urllib.parse import parse_qs
from collections import OrderedDict
from http.server import ThreadingHTTPServer as HTTPServer, BaseHTTPRequestHandler
from prometheus_client import Counter, Gauge, generate_latest, CONTENT_TYPE_LATEST
import redis

# í™˜ê²½ ë³€ìˆ˜
PUSH_TOKEN = os.getenv('DORA_PUSH_TOKEN', 'duri-secret-token-2024')
REDIS_URL = os.getenv('REDIS_URL', 'redis://127.0.0.1:6379/0')

# ë³´ì•ˆ ì„¤ì •
ALLOW_LOCAL_BYPASS = os.getenv('ALLOW_LOCAL_BYPASS', '0') == '1'
ALLOW_CIDR = os.getenv('ALLOW_PUSH_CIDR', '')  # ì˜ˆ: "172.18.0.0/16"

# ë ˆì´íŠ¸ë¦¬ë°‹ íŠœë‹
RL_CAP = float(os.getenv('PUSH_RL_CAPACITY', '10'))
RL_REFILL = float(os.getenv('PUSH_RL_REFILL', '10'))

# dedup TTL ì„¤ì •
DEDUP_TTL = int(os.getenv('PUSH_DEDUP_TTL_SEC', '300'))

# Redis ì—°ê²°
try:
    r = redis.from_url(REDIS_URL)
    r.ping()
    REDIS_AVAILABLE = True
    print(f"âœ… Redis ì—°ê²° ì„±ê³µ: {REDIS_URL}")
except Exception as e:
    print(f"âŒ Redis ì—°ê²° ì‹¤íŒ¨: {e}")
    REDIS_AVAILABLE = False
    r = None

# ë©”íŠ¸ë¦­ ì •ì˜
deployment_events = Counter('duri_deployment_events_total', 'Total deployment events')
deployment_events_persisted = Gauge('duri_deployment_events_persisted', 'Persisted deployment events')
deployment_events_labeled = Counter('duri_deployment_events_labeled_total', 'Labeled deployment events', ['env', 'service', 'source', 'commit'])

# í‰ê°€ ì ìˆ˜ ë©”íŠ¸ë¦­
auto_eval_score = Gauge('duri_auto_eval_score', 'Auto evaluation score', ['task_type', 'level'])
auto_eval_recent_avg = Gauge('duri_auto_eval_recent_avg', 'Recent average evaluation score')

# ì§€ëŠ¥í˜• ê²Œì´íŠ¸ ë©”íŠ¸ë¦­ (Gaugeë¡œ ë³€ê²½ - ì´ì¤‘ì§‘ê³„ ë°©ì§€)
promo_decisions = Gauge('duri_promo_decisions', 'Promotion decisions', ['decision'])

# Canary ratio ë©”íŠ¸ë¦­
canary_ratio = Gauge('duri_canary_ratio', 'Current canary traffic ratio')

# dedup/ratelimit ê´€ì¸¡ì„± ë©”íŠ¸ë¦­ (ë¼ë²¨ ì¶”ê°€)
dedup_hits = Counter('duri_push_dedup_total', 'Deduplicated push requests', ['reason'])
rate_limited_hits = Counter('duri_push_rate_limited_total', 'Rate-limited push requests')

# ìš´ì˜ íŠœë‹ê°’ ë©”íŠ¸ë¦­
rl_tokens = Gauge('duri_push_tokens', 'Current tokens per source', ['source'])

# ê°„ë‹¨ CIDR ì²´í¬ ìœ í‹¸
def ip_in_cidr(ip, cidr):
    try:
        return ipaddress.ip_address(ip) in ipaddress.ip_network(cidr, strict=False)
    except:
        return False

# dedup TTLì„ Redisë¡œ (ì¬ê¸°ë™ì—ë„ ìœ ì§€)
def seen_once(qid: str, ttl=DEDUP_TTL):
    if not REDIS_AVAILABLE or not qid:
        return False
    # SETNX + EXë¥¼ í•œ ë²ˆì— (redis-py 6.xëŠ” set(..., nx=True, ex=ttl))
    return not r.set(f"push:seen:{qid}", 1, nx=True, ex=ttl)  # Trueë©´ ì´ë¯¸ ë´„(=ì¤‘ë³µ)

# ë ˆì´íŠ¸ë¦¬ë°‹í„° (ìŠ¤ë ˆë“œ ì„¸ì´í”„í‹° ì¶”ê°€)
class TokenBucket:
    def __init__(self, capacity=RL_CAP, refill_rate=RL_REFILL):
        self.capacity = capacity
        self.tokens = capacity
        self.refill_rate = refill_rate
        self.last_refill = time.time()

class RateLimiter:
    def __init__(self):
        self.buckets = {}
        self._lock = threading.Lock()
    
    def get_bucket(self, key):
        with self._lock:
            return self.buckets.setdefault(key, TokenBucket())
    
    def is_allowed(self, key, tokens=1.0):
        bucket = self.get_bucket(key)
        now = time.time()
        
        # í† í° ë¦¬í•„
        time_passed = now - bucket.last_refill
        tokens_to_add = time_passed * bucket.refill_rate
        bucket.tokens = min(bucket.capacity, bucket.tokens + tokens_to_add)
        bucket.last_refill = now
        
        # í† í° ì†Œë¹„
        if bucket.tokens >= tokens:
            bucket.tokens -= tokens
            return True
        else:
            return False
    
    def get_tokens(self, key):
        bucket = self.get_bucket(key)
        now = time.time()
        
        # í† í° ë¦¬í•„
        time_passed = now - bucket.last_refill
        tokens_to_add = time_passed * bucket.refill_rate
        bucket.tokens = min(bucket.capacity, bucket.tokens + tokens_to_add)
        bucket.last_refill = now
        
        return bucket.tokens

rate_limiter = RateLimiter()

def record_deployment_event_labeled(env, service, source, commit):
    """ë¼ë²¨ì´ ìˆëŠ” ë°°í¬ ì´ë²¤íŠ¸ ê¸°ë¡"""
    deployment_events_labeled.labels(env=env, service=service, source=source, commit=commit).inc()
    deployment_events.inc()
    
    # Redisì— ì˜êµ¬ ì €ì¥ (ì „ìš© ì¹´ìš´í„° + ë¦¬ìŠ¤íŠ¸ ì €ì¥)
    if REDIS_AVAILABLE:
        try:
            # ì „ìš© ì¹´ìš´í„° ì‚¬ìš©
            v = r.incr("duri:deploy_events")
            deployment_events_persisted.set(v)
            
            # ì´ë²¤íŠ¸ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
            data = {
                'env': env,
                'service': service,
                'source': source,
                'commit': commit,
                'timestamp': time.time()
            }
            r.lpush("deploy:events", json.dumps(data))
            r.ltrim("deploy:events", 0, 999)  # ìµœê·¼ 1000ê°œë§Œ ë³´ê´€
            
        except Exception as e:
            print(f"âŒ Redis ì €ì¥ ì‹¤íŒ¨: {e}")

def check_health():
    """í—¬ìŠ¤ì²´í¬ (íŠœë‹ê°’/ì¹´ìš´í„° ìŠ¤ëƒ…ìƒ· í¬í•¨)"""
    health_status = {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "components": {}
    }
    
    # Redis ì—°ê²° ì²´í¬
    if REDIS_AVAILABLE:
        try:
            r.ping()
            health_status["components"]["redis"] = "healthy"
        except:
            health_status["components"]["redis"] = "unhealthy"
            health_status["status"] = "degraded"
    else:
        health_status["components"]["redis"] = "unavailable"
    
    # íŠœë‹ê°’/ì¹´ìš´í„° ìŠ¤ëƒ…ìƒ·
    health_status["components"]["rate_limiter"] = {"capacity": RL_CAP, "refill": RL_REFILL}
    if REDIS_AVAILABLE:
        health_status["components"]["dedup_ttl_sec"] = DEDUP_TTL
        health_status["components"]["deploy_events"] = int(r.get("duri:deploy_events") or 0)
    
    return health_status

def check_ready():
    ok = REDIS_AVAILABLE
    return {"status": "ready" if ok else "not_ready", "redis": "ok" if ok else "fail"}

def mask(tok: str):
    if not tok: return ""
    return ("*" * max(0, len(tok)-4)) + tok[-4:]

def verify_token(headers, client_addr=None):
    auth = headers.get('Authorization', '')
    if auth.startswith('Bearer '):
        token = auth.split(' ',1)[1].strip()
        if token == PUSH_TOKEN:
            print(f"[auth ok] ip={client_addr} token={mask(token)}")
            return True
    print(f"[auth ng] ip={client_addr} token={mask(auth.split(' ',1)[1].strip() if ' ' in auth else auth)}")
    return False

def log_deploy_event(client_ip, source, req_id, service, env):
    """ë°°í¬ ì´ë²¤íŠ¸ ë³´ì•ˆ ë¡œê·¸ (client_ip/source/req_id ì¶”ê°€ ê¸°ë¡)"""
    print(f"[deploy] ip={client_ip} source={source} req_id={req_id} service={service} env={env}")

def update_dora_metrics():
    """DORA ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ (ì „ìš© ì¹´ìš´í„° ì‚¬ìš©)"""
    try:
        if REDIS_AVAILABLE:
            v = int(r.get("duri:deploy_events") or 0)  # ì „ìš© ì¹´ìš´í„°ë§Œ ì‚¬ìš©
            deployment_events_persisted.set(v)
    except Exception as e:
        print(f"âŒ DORA ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def update_evaluation_metrics():
    """í‰ê°€ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
    try:
        # ë”ë¯¸ ë°ì´í„° (ì‹¤ì œ êµ¬í˜„ ì‹œ toolsì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        auto_eval_recent_avg.set(0.85)
        auto_eval_score.labels(task_type='reasoning', level='recent').set(0.90)
        auto_eval_score.labels(task_type='coding', level='recent').set(0.80)
    except Exception as e:
        print(f"âŒ í‰ê°€ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def update_promotion_metrics():
    """ìŠ¹ê²© ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ (Gaugeë¡œ ë³€ê²½ - ì´ì¤‘ì§‘ê³„ ë°©ì§€)"""
    try:
        # ë”ë¯¸ ë°ì´í„° (ì‹¤ì œ êµ¬í˜„ ì‹œ toolsì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        promo_decisions.labels(decision='promote').set(5)
        promo_decisions.labels(decision='hold').set(3)
        promo_decisions.labels(decision='rollback').set(1)
    except Exception as e:
        print(f"âŒ ìŠ¹ê²© ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def update_canary_metric():
    """Canary ratio ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
    try:
        if REDIS_AVAILABLE:
            v = float(r.get("canary:ratio") or 0)
            canary_ratio.set(v)
    except Exception as e:
        print(f"âŒ Canary ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def update_rl_tokens_metric():
    """ë ˆì´íŠ¸ë¦¬ë°‹ í† í° ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
    try:
        # ì£¼ìš” ì†ŒìŠ¤ë³„ í† í° ìˆ˜ìœ„ ì—…ë°ì´íŠ¸
        for source in ['ci', 'manual', 'auto']:
            tokens = rate_limiter.get_tokens(f"push:{source}")
            rl_tokens.labels(source=source).set(tokens)
    except Exception as e:
        print(f"âŒ RL í† í° ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

# ë°±ê·¸ë¼ìš´ë“œ ê°±ì‹  ë£¨í”„ (ë‹¨ì¼ ì •ì˜)
def bg_loop():
    """ë°±ê·¸ë¼ìš´ë“œ ê°±ì‹  ë£¨í”„"""
    while True:
        update_dora_metrics()
        update_evaluation_metrics()
        update_promotion_metrics()
        update_canary_metric()
        update_rl_tokens_metric()
        time.sleep(30)

# HTTP ì„œë²„ í´ë˜ìŠ¤
class MetricsHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        if self.path == '/metrics':
            self.send_response(200)
            self.send_header('Content-Type', CONTENT_TYPE_LATEST)
            self.end_headers()
            self.wfile.write(generate_latest())
            return
        if self.path == '/health':
            self.send_response(200)
            self.end_headers()
            self.wfile.write(json.dumps(check_health()).encode())
            return
        if self.path == '/ready':
            self.send_response(200)
            self.end_headers()
            self.wfile.write(json.dumps(check_ready()).encode())
            return
        if self.path.startswith('/push/deployment'):
            self.send_response(405); self.send_header('Allow','POST'); self.end_headers()
            self.wfile.write(b'only POST\n'); return
        self.send_response(404)
        self.end_headers()
        self.wfile.write(b'not found\n')
    
    def do_POST(self):
        if self.path.startswith('/push/deployment'):
            # í† í° ê²€ì¦ (CIDR í—ˆìš© ëª©ë¡ + í™˜ê²½ë³€ìˆ˜ í† ê¸€)
            client_ip = self.client_address[0]
            if not verify_token(self.headers, client_ip):
                self.send_response(401)
                self.end_headers()
                self.wfile.write(b'unauthorized\n')
                return
            # ì…ë ¥ ê²€ì¦
            q = parse_qs(self.path.split('?')[1] if '?' in self.path else '')
            need = ['env','service','source','commit','id']
            missing = [k for k in need if not q.get(k)]
            if missing:
                self.send_response(400)
                self.end_headers()
                self.wfile.write(f"missing:{','.join(missing)}".encode())
                return

            # ì¤‘ë³µ í‘¸ì‹œ ë°©ì§€ (dedup ë¡œì§) - Redis TTL ì‚¬ìš©
            qid = q.get('id', [None])[0]
            if qid and seen_once(qid):
                dedup_hits.labels(reason='redis_ttl').inc()
                self.send_response(200)
                self.end_headers()
                self.wfile.write(b'dedup\n')
                return
            
            # ë ˆì´íŠ¸ë¦¬ë°‹ ì²´í¬
            source = q.get('source', ['manual'])[0]
            if not rate_limiter.is_allowed(f"push:{source}", tokens=1.0):
                rate_limited_hits.inc()
                self.send_response(429)
                self.end_headers()
                self.wfile.write(b'rate_limited\n')
                return
            
            # ë°°í¬ ì´ë²¤íŠ¸ ê¸°ë¡
            env = q.get('env', ['staging'])[0]
            service = q.get('service', ['duri_control'])[0]
            commit = q.get('commit', ['unknown'])[0]
            
            # ë³´ì•ˆ ë¡œê·¸ (client_ip/source/req_id ì¶”ê°€ ê¸°ë¡)
            log_deploy_event(client_ip, source, qid, service, env)
            
            record_deployment_event_labeled(env, service, source, commit)
            
            # Trace v2: ë°°í¬ ì´ë²¤íŠ¸ë¥¼ íŠ¸ë ˆì´ìŠ¤ ë£¨íŠ¸ ìŠ¤íŒ¬ìœ¼ë¡œ ë³€í™˜
            try:
                import time
                evt = {
                    "kind": "span_upsert",
                    "span": {
                        "span_id": f"deploy-{qid}",
                        "parent_span_id": None,
                        "deploy_req_id": qid,
                        "artifact_id": None,
                        "span_name": "deploy_root",
                        "status": "ok",
                        "start_ts": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                        "end_ts": None,
                        "labels": {"source": source, "env": env, "service": service},
                        "attrs": {"commit": commit, "pipeline": q.get('pipeline', ['unknown'])[0], "node_id": q.get('node_id', ['unknown'])[0]}
                    }
                }
                r.rpush("trace:events", json.dumps(evt))
                print(f"[trace] Deploy root span queued: {qid}")
            except Exception as e:
                print(f"[trace] Failed to queue deploy root span: {e}")
            
            self.send_response(200)
            self.end_headers()
            self.wfile.write(b'OK\n')
        else:
            self.send_response(404)
            self.end_headers()
            self.wfile.write(b'not found\n')
    
    def log_message(self, format, *args):
        # ë¡œê·¸ ì¶œë ¥
        print(f"{self.address_string()} - - [{self.log_date_time_string()}] {format % args}")

if __name__ == "__main__":
    # ë°±ê·¸ë¼ìš´ë“œ ë£¨í”„ ì‹œì‘ (í•œ ë²ˆë§Œ)
    threading.Thread(target=bg_loop, daemon=True).start()
    
    # HTTP ì„œë²„ ì‹œì‘ (ThreadingHTTPServer ì‚¬ìš©)
    httpd = HTTPServer(('0.0.0.0', 8000), MetricsHandler)
    print("ğŸš€ DORA ë©”íŠ¸ë¦­ ìµìŠ¤í¬í„° ì‹œì‘: http://0.0.0.0:8000")
    httpd.serve_forever()
