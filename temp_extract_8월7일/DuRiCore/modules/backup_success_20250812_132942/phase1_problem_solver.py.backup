# phase1_problem_solver.py  â€”  SPINE (minimal, safe-to-run)
# ëª©ì : ì¦‰ì‹œ ê°€ë™ ê°€ëŠ¥í•œ ë¼ˆëŒ€(run í¬í•¨) + D1~D5 ì¸í„°í˜ì´ìŠ¤ ê³ ì •
# ì˜ì¡´ì„±: pandas, numpy, scikit-learn (xgboostëŠ” ì˜µì…˜)

from __future__ import annotations
from typing import Dict, Any, List, Optional, Tuple
import os, json, time, gc
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split, KFold, StratifiedShuffleSplit
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error

# ì˜µì…˜ ì˜ì¡´ì„± (ì—†ì–´ë„ ë™ì‘)
try:
    import xgboost as xgb
    _HAS_XGB = True
except Exception:
    _HAS_XGB = False

# === D2: íšŒê·€ ë³´ì •ê¸° ===
from sklearn.isotonic import IsotonicRegression
from sklearn.ensemble import GradientBoostingRegressor


class _RegCalibrator:
    """íšŒê·€ ë³´ì •ê¸°. fit(X_val, y_val, y_pred_val) í›„, transform(y_pred, X_opt)ë¡œ ì ìš©."""
    def __init__(self, mode:str="none"):
        self.mode = mode
        self.iso: Optional[IsotonicRegression] = None
        self.gbm: Optional[GradientBoostingRegressor] = None

    def fit(self, y_true_val: np.ndarray, y_pred_val: np.ndarray, X_val: Optional[pd.DataFrame] = None):
        if self.mode == "isotonic":
            self.iso = IsotonicRegression(out_of_bounds="clip")
            # Isotonicì€ (y_pred â†’ y_true) ë‹¨ì¡° ë§¤í•‘ì„ ë°°ìš´ë‹¤
            self.iso.fit(y_pred_val, y_true_val)
        elif self.mode == "residual_gbm":
            assert X_val is not None, "residual_gbm ëª¨ë“œëŠ” X_valì´ í•„ìš”í•©ë‹ˆë‹¤."
            resid = y_true_val - y_pred_val
            self.gbm = GradientBoostingRegressor(
                n_estimators=300, learning_rate=0.05, max_depth=3, random_state=0
            )
            self.gbm.fit(X_val.values, resid)
        else:
            pass  # none

    def transform(self, y_pred: np.ndarray, X_opt: Optional[pd.DataFrame] = None) -> np.ndarray:
        if self.mode == "isotonic" and self.iso is not None:
            return self.iso.predict(y_pred)
        elif self.mode == "residual_gbm" and self.gbm is not None:
            assert X_opt is not None, "residual_gbm transformì—ëŠ” Xê°€ í•„ìš”í•©ë‹ˆë‹¤."
            return y_pred + self.gbm.predict(X_opt.values)
        else:
            return y_pred


def _evaluate_reg(y_true, y_pred) -> Dict[str, float]:
    return {
        "r2": float(r2_score(y_true, y_pred)),
        "mse": float(mean_squared_error(y_true, y_pred)),
    }


def _pick_best_calibrator(y_true_val, y_pred_val, X_val) -> Dict[str, Any]:
    """
    ê²€ì¦ì…‹ì—ì„œ ë³´ì •ë²• 3ê°œ ë¹„êµ: none / isotonic / residual_gbm
    MSE ê¸°ì¤€ìœ¼ë¡œ ìµœì„  ì„ íƒ. ë°˜í™˜: {"mode":..., "cal":_RegCalibrator, "metrics":...}
    """
    candidates = []

    # none
    m_none = _evaluate_reg(y_true_val, y_pred_val)
    candidates.append({"mode": "none", "cal": _RegCalibrator("none"), "metrics": m_none})

    # isotonic
    try:
        cal_iso = _RegCalibrator("isotonic")
        cal_iso.fit(y_true_val, y_pred_val, X_val=None)
        yp_iso = cal_iso.transform(y_pred_val)
        m_iso = _evaluate_reg(y_true_val, yp_iso)
        candidates.append({"mode": "isotonic", "cal": cal_iso, "metrics": m_iso})
    except Exception:
        pass

    # residual_gbm
    try:
        cal_gbm = _RegCalibrator("residual_gbm")
        cal_gbm.fit(y_true_val, y_pred_val, X_val=X_val)
        yp_gbm = cal_gbm.transform(y_pred_val, X_opt=X_val)
        m_gbm = _evaluate_reg(y_true_val, yp_gbm)
        candidates.append({"mode": "residual_gbm", "cal": cal_gbm, "metrics": m_gbm})
    except Exception:
        pass

    # select by lowest MSE (tie-break by highest R2)
    best = min(candidates, key=lambda d: (d["metrics"]["mse"], -d["metrics"]["r2"]))
    return best


class Phase1ProblemSolver:
    """
    Phase 1 ë©”ì¸ ì†”ë²„ì˜ 'ì²™ì¶”':
      - run() íŒŒì´í”„ë¼ì¸
      - ë² ì´ìŠ¤ ëª¨ë¸(RF, XGB-ì˜µì…˜)
      - í‰ê°€/ì €ì¥/ì•„í‹°íŒ©íŠ¸ ìƒì„±
      - D1~D5 ìŠ¤í…(í›„ì† í™•ì¥ ì§€ì  ê³ ì •)
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.random_state = int(self.config.get("random_state", 42))
        self.test_size = float(self.config.get("test_size", 0.2))
        self.artifacts_dir = self.config.get("artifacts_dir", "./artifacts_phase1")
        self.light_params = bool(self.config.get("light_params", True))  # ê²½ëŸ‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° í† ê¸€
        self.use_stratified_split = bool(self.config.get("use_stratified_split", True))
        self.force_topk_features = int(self.config.get("force_topk_features", 3))   # 2 or 3
        self.strong_reg = bool(self.config.get("strong_reg", True))                  # ê°•ê·œì œ ì¼œê¸°
        self.select_by_test = bool(self.config.get("select_by_test", False))         # ì‹¤í—˜ìš©: testë¡œ ìµœì¢… ì„ íƒ
        self.auto_apply_basic_preproc = bool(self.config.get("auto_apply_basic_preproc", False))  # ìë™ ì „ì²˜ë¦¬ ì ìš© í† ê¸€
        os.makedirs(self.artifacts_dir, exist_ok=True)

    # ---------------- 0) Data / Features ----------------
    def load_data(self) -> pd.DataFrame:
        """
        ì‹¤ì œ ì—°ê²° ì „ê¹Œì§€ëŠ” ë”ë¯¸ ë°ì´í„° ìƒì„±.
        ì‹¤ì œ ì‚¬ìš© ì‹œ self.config["data_path"]ë¡œ êµì²´.
        """
        if "data_path" in self.config:
            path = self.config["data_path"]
            if path.endswith(".parquet"):
                return pd.read_parquet(path)
            elif path.endswith(".csv"):
                return pd.read_csv(path)
            else:
                raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° í¬ë§·")
        rng = np.random.RandomState(self.random_state)
        X = rng.randn(1200, 8)
        y = (
            0.5 * X[:, 0]
            - 0.3 * X[:, 1]
            + 0.2 * (X[:, 2] ** 2)
            + 0.1 * rng.randn(1200)
        )
        df = pd.DataFrame(X, columns=[f"f{i}" for i in range(X.shape[1])])
        df["target"] = y
        return df

    def _validate_data_quality(self, df: pd.DataFrame) -> None:
        assert "target" in df.columns, "target ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."
        X = df.drop(columns=["target"])
        assert X.shape[1] > 0, "ì…ë ¥ íŠ¹ì„±ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
        if X.isnull().any().any() or df["target"].isnull().any():
            raise ValueError("ê²°ì¸¡ì¹˜ ì¡´ì¬")

    def generate_base_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        X = df.drop(columns=["target"]).copy()
        y = df["target"].copy()
        return X, y

    def generate_advanced_features(self, X: pd.DataFrame) -> pd.DataFrame:
        # ìµœì†Œ ì•ˆì „ ì˜ˆì‹œ 2ê°œ
        if "f0" in X.columns and "f1" in X.columns:
            X["f0_f1_cross"] = X["f0"] * X["f1"]
        if "f2" in X.columns:
            X["f2_sq"] = X["f2"] ** 2
        return X

    def _split_train_valid(self, X: pd.DataFrame, y: pd.Series):
        return train_test_split(
            X, y, test_size=self.test_size, random_state=self.random_state
        )

    def _split_train_valid_test(self, X, y, valid_size=0.2, test_size=0.2, seed=None):
        """ê¸°ì¡´ ë¶„í•  ë°©ì‹"""
        rs = self.random_state if seed is None else seed
        X_trv, X_te, y_trv, y_te = train_test_split(X, y, test_size=test_size, random_state=rs)
        X_tr, X_va, y_tr, y_va = train_test_split(X_trv, y_trv, test_size=valid_size, random_state=rs)
        return (X_tr, X_va, X_te, y_tr, y_va, y_te)
    
    def _make_strat_labels(self, y: pd.Series, q: int = 10) -> np.ndarray:
        """
        íšŒê·€ìš© ì¸µí™” ë¼ë²¨: yë¥¼ që¶„ìœ„ë¡œ binning.
        ìƒ˜í”Œ ìˆ˜ê°€ ì ê±°ë‚˜ ì¤‘ë³µì´ ë§ì•„ qê°œë¥¼ ëª» ë§Œë“¤ë©´ ê°€ëŠ¥í•œ ë§Œí¼ë§Œ ìƒì„±.
        """
        y_np = y.to_numpy() if isinstance(y, pd.Series) else np.asarray(y)
        # ì¤‘ë³µ/ì—£ì§€ ì¼€ì´ìŠ¤ ëŒ€ë¹„: qcut ì‹¤íŒ¨ ì‹œ rank ê¸°ë°˜ ë“±ë¶„ìœ„ ê·¼ì‚¬
        try:
            bins = pd.qcut(y_np, q=q, labels=False, duplicates="drop")
        except Exception:
            ranks = pd.Series(y_np).rank(method="average", pct=True).to_numpy()
            bins = np.floor(ranks * q).clip(0, q - 1).astype(int)
        return np.asarray(bins, dtype=int)
    
    def _split_train_valid_test_stratified(
        self, X: pd.DataFrame, y: pd.Series, valid_size: float = 0.2, test_size: float = 0.2, q: int = 10, seed: Optional[int] = None
    ):
        """
        1) ì „ì²´ì—ì„œ test_sizeë§Œí¼ test stratified split
        2) ë‚˜ë¨¸ì§€ì—ì„œ valid_sizeë§Œí¼ valid stratified split
        - që¶„ìœ„ ê¸°ë°˜ ì¸µí™”ë¡œ valid/testì˜ yë¶„í¬ë¥¼ í•™ìŠµê³¼ ìœ ì‚¬í•˜ê²Œ ìœ ì§€
        """
        rs = self.random_state if seed is None else seed
        y_bins = self._make_strat_labels(y, q=q)

        # 1) Train+Valid vs Test
        sss1 = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=rs)
        idx = np.arange(len(y))
        trv_idx, te_idx = next(sss1.split(idx, y_bins))
        X_trv, y_trv = X.iloc[trv_idx], y.iloc[trv_idx]
        X_te,  y_te  = X.iloc[te_idx],  y.iloc[te_idx]

        # 2) Train vs Valid (ë‚¨ì€ trvì—ì„œ ë‹¤ì‹œ ì¸µí™”)
        y_trv_bins = self._make_strat_labels(y_trv, q=q)
        sss2 = StratifiedShuffleSplit(n_splits=1, test_size=valid_size, random_state=rs)
        tr_idx, va_idx = next(sss2.split(np.arange(len(y_trv)), y_trv_bins))
        X_tr, y_tr = X_trv.iloc[tr_idx], y_trv.iloc[tr_idx]
        X_va, y_va = X_trv.iloc[va_idx], y_trv.iloc[va_idx]
        return (X_tr, X_va, X_te, y_tr, y_va, y_te)

    def _save_json(self, obj, name):
        """JSON ì €ì¥ í¸ì˜ í•¨ìˆ˜"""
        p = os.path.join(self.artifacts_dir, name)
        with open(p, "w") as f:
            json.dump(obj, f, ensure_ascii=False, indent=2)
        return p

    # ---------------- 1) Models ----------------
    def _train_random_forest(self, X_tr, y_tr, X_val, y_val) -> Dict[str, Any]:
        if self.strong_reg:
            mdl = RandomForestRegressor(
                n_estimators=300,
                max_depth=None,
                min_samples_leaf=8,      # 5 -> 8
                max_features="sqrt",
                random_state=self.random_state,
                n_jobs=-1,
            )
        else:
            mdl = RandomForestRegressor(
                n_estimators=300, random_state=self.random_state, n_jobs=-1
            )
        mdl.fit(X_tr, y_tr)
        pred_val = mdl.predict(X_val)
        return {"name": "rf", "model": mdl, "pred_val": pred_val,
                "metrics": self._eval(y_val, pred_val)}

    def _train_xgboost(self, X_tr, y_tr, X_val, y_val) -> Optional[Dict[str, Any]]:
        if not _HAS_XGB:
            return None
        if self.strong_reg:
            params = dict(
                n_estimators=300,         # 500 -> 300
                max_depth=3,              # 6 -> 3
                min_child_weight=8,       # 1 -> 8
                subsample=0.7,            # 0.9 -> 0.7
                colsample_bytree=0.7,     # 0.9 -> 0.7
                reg_lambda=8.0,           # 1 -> 8
                learning_rate=0.06,       # 0.03 -> 0.06
                random_state=self.random_state,
                n_jobs=-1,
                tree_method="hist",
            )
        else:
            params = dict(
                n_estimators=500, max_depth=6, subsample=0.9, colsample_bytree=0.9,
                learning_rate=0.03, random_state=self.random_state, n_jobs=-1, tree_method="hist"
            )
        mdl = xgb.XGBRegressor(**params)
        mdl.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)
        pred_val = mdl.predict(X_val)
        return {"name": "xgb", "model": mdl, "pred_val": pred_val,
                "metrics": self._eval(y_val, pred_val)}

    # ---------------- 2) Eval / Ensemble ----------------
    def _eval(self, y_true, y_pred) -> Dict[str, float]:
        """ê¸°ë³¸ í‰ê°€ ì§€í‘œ"""
        if isinstance(y_true, pd.Series):
            y_true = y_true.to_numpy()
        if isinstance(y_pred, pd.Series):
            y_pred = y_pred.to_numpy()
        
        assert y_true.ndim == 1 and y_pred.ndim == 1 and len(y_true) == len(y_pred), "shape mismatch"
        
        return {
            "r2": float(r2_score(y_true, y_pred)),
            "mse": float(mean_squared_error(y_true, y_pred))
        }
    
    def _extra_metrics(self, y_true, y_pred) -> Dict[str, float]:
        """ì¶”ê°€ ì§„ë‹¨ ì§€í‘œ: ì •ê·œí™”ëœ RMSE ë“±"""
        rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))
        stdy = float(np.std(y_true))
        return {
            "rmse": rmse, 
            "std_y": stdy, 
            "nrmse": rmse/(stdy+1e-12)
        }
    
    def _schema_expectations(self, X: pd.DataFrame, y: pd.Series, top_k_cats: int = 5) -> dict:
        """X/yì˜ ìŠ¤í‚¤ë§ˆ ê¸°ëŒ€ì¹˜: ë¶„í¬(ìˆ˜ì¹˜/ë²”ì£¼), ê²°ì¸¡, IQR ì´ìƒì¹˜ìœ¨, íƒ€ê¹ƒ ìš”ì•½."""
        X = X.copy()
        info = {}

        # íƒ€ì… ë¶„ë¦¬
        num_cols = X.select_dtypes(include=["number"]).columns.tolist()
        cat_cols = X.select_dtypes(exclude=["number", "datetime"]).columns.tolist()

        # ê²°ì¸¡ìœ¨
        na_rate = X.isna().mean().sort_values(ascending=False).to_dict()

        # ìˆ˜ì¹˜ ìš”ì•½ + IQR ì´ìƒì¹˜ìœ¨
        num_summary = {}
        for c in num_cols:
            s = X[c].astype(float)
            q1, q2, q3 = np.nanpercentile(s, [25, 50, 75])
            iqr = q3 - q1
            lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr
            outlier_rate = float(((s < lo) | (s > hi)).mean())
            num_summary[c] = {
                "count": int(s.notna().sum()),
                "mean": float(np.nanmean(s)),
                "std": float(np.nanstd(s)),
                "min": float(np.nanmin(s)),
                "q1": float(q1), "median": float(q2), "q3": float(q3),
                "max": float(np.nanmax(s)),
                "iqr": float(iqr),
                "outlier_rate_iqr": outlier_rate,
                "na_rate": float(s.isna().mean()),
            }

        # ë²”ì£¼ ìš”ì•½(ê³ ì¹´ë””ë„ë¦¬í‹° íƒì§€ + ìƒìœ„ top_k ë¹ˆë„)
        cat_summary = {}
        for c in cat_cols:
            s = X[c].astype("object")
            vc = s.value_counts(dropna=False)
            cat_summary[c] = {
                "n_unique": int(s.nunique(dropna=False)),
                "top_values": [{ "value": str(k), "count": int(v) } for k, v in vc.head(top_k_cats).items()],
                "na_rate": float(s.isna().mean()) if s.dtype != "object" else 0.0,
            }

        # íƒ€ê¹ƒ ìš”ì•½
        y_np = pd.Series(y).astype(float).to_numpy()
        y_q = np.nanpercentile(y_np, [1, 5, 25, 50, 75, 95, 99])
        y_iqr = float(y_q[4] - y_q[2])
        y_lo, y_hi = y_q[2] - 1.5 * y_iqr, y_q[4] + 1.5 * y_iqr
        y_outlier_rate = float(((y_np < y_lo) | (y_np > y_hi)).mean())
        target_summary = {
            "count": int(np.isfinite(y_np).sum()),
            "mean": float(np.nanmean(y_np)),
            "std": float(np.nanstd(y_np)),
            "min": float(np.nanmin(y_np)),
            "q01": float(y_q[0]), "q05": float(y_q[1]),
            "q25": float(y_q[2]), "median": float(y_q[3]), "q75": float(y_q[4]),
            "q95": float(y_q[5]), "q99": float(y_q[6]),
            "iqr": y_iqr,
            "outlier_rate_iqr": y_outlier_rate,
        }

        info["shape"] = {"rows": int(X.shape[0]), "cols": int(X.shape[1])}
        info["na_rate"] = na_rate
        info["numeric_summary"] = num_summary
        info["categorical_summary"] = cat_summary
        info["target_summary"] = target_summary
        info["column_types"] = {
            "numeric": num_cols,
            "categorical": cat_cols,
        }
        return info
    
    def _quality_flags_from_schema(self, schema: dict,
                                   na_thr: float = 0.20,
                                   outlier_thr: float = 0.15) -> dict:
        """ìŠ¤í‚¤ë§ˆ ê¸°ëŒ€ì¹˜ì—ì„œ ìë™ ê²½ë³´ ì¶”ì¶œ."""
        flags = {"high_na": [], "high_outlier_iqr": [], "notes": []}
        # NA
        for c, r in (schema.get("na_rate") or {}).items():
            if r is not None and float(r) > na_thr:
                flags["high_na"].append({"col": c, "na_rate": float(r)})
        # IQR outlier
        for c, s in (schema.get("numeric_summary") or {}).items():
            if s and float(s.get("outlier_rate_iqr", 0.0)) > outlier_thr:
                flags["high_outlier_iqr"].append({
                    "col": c, "outlier_rate_iqr": float(s["outlier_rate_iqr"]),
                    "iqr": float(s.get("iqr", 0.0))
                })
        # Target sanity
        tgt = schema.get("target_summary") or {}
        if tgt:
            if abs(float(tgt.get("mean", 0.0))) > 5 * float(tgt.get("std", 1.0) or 1.0):
                flags["notes"].append("target_mean_far_from_zero: check scaling")
            if float(tgt.get("iqr", 0.0)) == 0.0:
                flags["notes"].append("target_iqr_zero: degenerate distribution")
        return flags

    def _auto_preproc_suggestions(self, flags: dict) -> dict:
        """ê²½ë³´ ê¸°ë°˜ ì „ì²˜ë¦¬ ì œì•ˆ(ì‹¤í–‰ì€ ì•ˆí•¨; ì œì•ˆë§Œ JSONì— ì²¨ë¶€)."""
        sugg = {"impute": [], "clip": [], "winsorize": [], "categorical_other": []}
        for it in flags.get("high_na", []):
            c, r = it["col"], it["na_rate"]
            if r >= 0.5:
                sugg["impute"].append({"col": c, "strategy": "constant", "fill_value": 0})
            else:
                sugg["impute"].append({"col": c, "strategy": "median"})

        for it in flags.get("high_outlier_iqr", []):
            c = it["col"]
            # ê¸°ë³¸ clip(1.5*IQR) ê¶Œì¥, ëŒ€ì•ˆìœ¼ë¡œ winsorize 1% ì œì‹œ
            sugg["clip"].append({"col": c, "method": "iqr_clip", "k": 1.5})
            sugg["winsorize"].append({"col": c, "lower_q": 0.01, "upper_q": 0.99})

        # ê³ ì¹´ë””ë„ ë²”ì£¼ (ì´ë¯¸ ê¸°ëŒ€ì¹˜ì— ë“¤ì–´ì˜¤ë©´)
        cat_sum = (getattr(self, "_last_schema_expectations", {}) or {}).get("categorical_summary", {})
        for c, s in cat_sum.items():
            if int(s.get("n_unique", 0)) > 50:
                sugg["categorical_other"].append({"col": c, "threshold": 10, "action": "group_rare_as_OTHER"})
        return sugg

    def _blend_or_ensemble(self, models: List[Dict[str, Any]], y_val) -> Dict[str, Any]:
        preds = [m["pred_val"] for m in models if m and "pred_val" in m]
        if len(preds) < 2:
            return {"name": "blend_none", "metrics": {"r2": -1e9, "mse": 1e9}}
        avg = np.mean(np.vstack(preds), axis=0)
        return {"name": "blend_mean", "pred_val": avg, "metrics": self._eval(y_val, avg)}

    def _pick_best(self, candidates: List[Optional[Dict[str, Any]]]) -> Dict[str, Any]:
        valid = [c for c in candidates if c and "metrics" in c]
        if not valid:
            return {"name": "none", "metrics": {"r2": -1e9, "mse": 1e9}}
        return max(valid, key=lambda c: c["metrics"]["r2"])

    # ---------------- 3) D1~D5 Stubs (ê³ ì • ì¸í„°í˜ì´ìŠ¤) ----------------
    def _generate_error_slicing_report(self, y_true, y_pred, X_val) -> Dict[str, Any]:
        """
        D1: ì—ëŸ¬ ìŠ¬ë¼ì´ì‹± ë¦¬í¬íŠ¸ í™•ì¥
        - ì”ì°¨ ìƒÂ·í•˜ìœ„ 10% ì¼€ì´ìŠ¤ë¶ + ë¶„í¬/ì‹œê°„ëŒ€ ìŠ¬ë¼ì´ì‹± ë¦¬í¬íŠ¸
        - ìƒìœ„ 50ê°œ ì˜¤ë¥˜ ì¼€ì´ìŠ¤ ë¶„ì„
        """
        try:
            # ì”ì°¨ ê³„ì‚°
            resid = (y_true - y_pred)
            abs_resid = np.abs(resid)
            
            # ê¸°ë³¸ í†µê³„
            rep = {
                "n": int(resid.shape[0]),
                "resid_mean": float(np.mean(resid)),
                "resid_std": float(np.std(resid)),
                "resid_q10": float(np.quantile(resid, 0.10)),
                "resid_q90": float(np.quantile(resid, 0.90)),
                "resid_q95": float(np.quantile(resid, 0.95)),
                "resid_q99": float(np.quantile(resid, 0.99)),
                "abs_resid_mean": float(np.mean(abs_resid)),
                "abs_resid_median": float(np.median(abs_resid))
            }
            
            # ìƒìœ„ ì˜¤ë¥˜ ì¼€ì´ìŠ¤ (ì ˆëŒ€ ì”ì°¨ ê¸°ì¤€)
            top_error_indices = np.argsort(abs_resid)[-50:]  # ìƒìœ„ 50ê°œ
            top_error_cases = []
            
            for idx in top_error_indices:
                case = {
                    "index": int(idx),
                    "true_value": float(y_true[idx]),
                    "predicted_value": float(y_pred[idx]),
                    "residual": float(resid[idx]),
                    "abs_residual": float(abs_resid[idx]),
                    "features": {}
                }
                
                # íŠ¹ì„± ê°’ë“¤ ì¶”ê°€ (ìˆ«ìí˜•ë§Œ)
                for col in X_val.columns:
                    try:
                        val = X_val.iloc[idx][col]
                        if isinstance(val, (int, float, np.number)):
                            case["features"][col] = float(val)
                    except:
                        continue
                
                top_error_cases.append(case)
            
            rep["top_error_cases"] = top_error_cases
            
            # ì”ì°¨ ë¶„í¬ ë¶„ì„
            resid_bins = np.linspace(resid.min(), resid.max(), 11)
            resid_hist, _ = np.histogram(resid, bins=resid_bins)
            rep["residual_distribution"] = {
                "bins": [float(b) for b in resid_bins],
                "counts": [int(c) for c in resid_hist]
            }
            
            # ì˜ˆì¸¡ê°’ vs ì”ì°¨ ê´€ê³„
            pred_bins = np.linspace(y_pred.min(), y_pred.max(), 11)
            pred_resid_analysis = []
            
            for i in range(len(pred_bins) - 1):
                mask = (y_pred >= pred_bins[i]) & (y_pred < pred_bins[i + 1])
                if np.sum(mask) > 0:
                    pred_resid_analysis.append({
                        "pred_range": [float(pred_bins[i]), float(pred_bins[i + 1])],
                        "count": int(np.sum(mask)),
                        "mean_residual": float(np.mean(resid[mask])),
                        "std_residual": float(np.std(resid[mask]))
                    })
            
            rep["prediction_residual_analysis"] = pred_resid_analysis
            
            # íŠ¹ì„±ë³„ ì”ì°¨ ë¶„ì„ (ìƒìœ„ 5ê°œ íŠ¹ì„±)
            feature_resid_analysis = {}
            for col in X_val.columns[:5]:  # ìƒìœ„ 5ê°œ íŠ¹ì„±ë§Œ
                try:
                    if X_val[col].dtype in ['int64', 'float64']:
                        # íŠ¹ì„± ê°’ êµ¬ê°„ë³„ ì”ì°¨ ë¶„ì„
                        feature_vals = X_val[col]
                        feature_bins = np.linspace(feature_vals.min(), feature_vals.max(), 6)
                        
                        bin_analysis = []
                        for i in range(len(feature_bins) - 1):
                            mask = (feature_vals >= feature_bins[i]) & (feature_vals < feature_bins[i + 1])
                            if np.sum(mask) > 0:
                                bin_analysis.append({
                                    "value_range": [float(feature_bins[i]), float(feature_bins[i + 1])],
                                    "count": int(np.sum(mask)),
                                    "mean_residual": float(np.mean(resid[mask])),
                                    "std_residual": float(np.std(resid[mask]))
                                })
                        
                        feature_resid_analysis[col] = bin_analysis
                except:
                    continue
            
            rep["feature_residual_analysis"] = feature_resid_analysis
            
            # ë¦¬í¬íŠ¸ ì €ì¥
            timestamp = int(time.time())
            path = os.path.join(self.artifacts_dir, f"error_slicing_{timestamp}.json")
            
            with open(path, "w") as f:
                json.dump(rep, f, ensure_ascii=False, indent=2)
            
            # ìš”ì•½ ì¶œë ¥
            print(f"D1 ì—ëŸ¬ ìŠ¬ë¼ì´ì‹± ë¦¬í¬íŠ¸ ì™„ë£Œ:")
            print(f"  - ì´ ìƒ˜í”Œ: {rep['n']}")
            print(f"  - í‰ê·  ì”ì°¨: {rep['resid_mean']:.4f}")
            print(f"  - ì ˆëŒ€ ì”ì°¨ ì¤‘ê°„ê°’: {rep['abs_resid_median']:.4f}")
            print(f"  - ìƒìœ„ ì˜¤ë¥˜ ì¼€ì´ìŠ¤: {len(top_error_cases)}ê°œ")
            print(f"  - ì €ì¥ ê²½ë¡œ: {path}")
            
            return {"path": path, "summary": rep}
            
        except Exception as e:
            print(f"D1 ì—ëŸ¬ ìŠ¬ë¼ì´ì‹± ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ë¦¬í¬íŠ¸ë¼ë„ ìƒì„±
            basic_rep = {
                "n": int(y_true.shape[0]),
                "resid_q10": float(np.quantile(y_true - y_pred, 0.10)),
                "resid_q90": float(np.quantile(y_true - y_pred, 0.90)),
                "error": str(e)
            }
            path = os.path.join(self.artifacts_dir, f"error_slicing_basic_{int(time.time())}.json")
            with open(path, "w") as f:
                json.dump(basic_rep, f, ensure_ascii=False, indent=2)
            return {"path": path, "summary": basic_rep}

    def _apply_calibration_methods(self, y_true, y_pred, method="auto") -> Dict[str, Any]:
        """
        D2: ëª¨ë¸ ë³´ì • (ìº˜ë¦¬ë¸Œë ˆì´ì…˜)
        - Temperature scaling ë˜ëŠ” Isotonic calibration
        - ê¸°ëŒ€ íš¨ê³¼: RÂ² +0.02~0.04 (pâ‰ˆ0.8)
        """
        try:
            from sklearn.isotonic import IsotonicRegression
            from sklearn.linear_model import LogisticRegression
            from sklearn.preprocessing import StandardScaler
            
            # ê¸°ë³¸ ë³´ì • ê²°ê³¼
            result = {
                "method": method,
                "params": {},
                "note": "calibrated",
                "original_r2": float(r2_score(y_true, y_pred)),
                "calibrated_r2": None,
                "improvement": 0.0
            }
            
            # Temperature scaling (ê°„ë‹¨í•œ ë³´ì •)
            if method in ["auto", "temperature"]:
                try:
                    # ì˜ˆì¸¡ê°’ì„ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
                    pred_min, pred_max = y_pred.min(), y_pred.max()
                    if pred_max > pred_min:
                        pred_norm = (y_pred - pred_min) / (pred_max - pred_min)
                        
                        # Temperature parameter T ì°¾ê¸° (ê°„ë‹¨í•œ ê·¸ë¦¬ë“œ ì„œì¹˜)
                        best_t = 1.0
                        best_score = r2_score(y_true, y_pred)
                        
                        for t in [0.5, 0.8, 1.0, 1.2, 1.5, 2.0]:
                            try:
                                # Temperature scaling ì ìš©
                                pred_cal = pred_norm * t
                                pred_cal = pred_cal * (pred_max - pred_min) + pred_min
                                
                                # ì„±ëŠ¥ í‰ê°€
                                score = r2_score(y_true, pred_cal)
                                if score > best_score:
                                    best_score = score
                                    best_t = t
                            except:
                                continue
                        
                        # ìµœì  temperatureë¡œ ë³´ì •
                        pred_cal = pred_norm * best_t
                        pred_cal = pred_cal * (pred_max - pred_min) + pred_min
                        
                        result["method"] = "temperature_scaling"
                        result["params"]["temperature"] = best_t
                        result["calibrated_r2"] = best_score
                        result["improvement"] = best_score - result["original_r2"]
                        
                        print(f"D2 Temperature Scaling ì™„ë£Œ: T={best_t:.2f}, RÂ² ê°œì„ : {result['improvement']:.4f}")
                        
                        return result
                        
                except Exception as e:
                    print(f"Temperature scaling ì‹¤íŒ¨: {e}")
            
            # Isotonic calibration (ê³ ê¸‰ ë³´ì •)
            if method in ["auto", "isotonic"]:
                try:
                    # Isotonic regressionìœ¼ë¡œ ë³´ì •
                    iso_reg = IsotonicRegression(out_of_bounds='clip')
                    iso_reg.fit(y_pred, y_true)
                    pred_cal = iso_reg.predict(y_pred)
                    
                    # ë³´ì • í›„ ì„±ëŠ¥ í‰ê°€
                    calibrated_score = r2_score(y_true, pred_cal)
                    improvement = calibrated_score - result["original_r2"]
                    
                    result["method"] = "isotonic_calibration"
                    result["calibrated_r2"] = calibrated_score
                    result["improvement"] = improvement
                    
                    print(f"D2 Isotonic Calibration ì™„ë£Œ: RÂ² ê°œì„ : {improvement:.4f}")
                    
                    return result
                    
                except Exception as e:
                    print(f"Isotonic calibration ì‹¤íŒ¨: {e}")
            
            # ë³´ì • ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
            result["note"] = "calibration_failed"
            result["calibrated_r2"] = result["original_r2"]
            result["improvement"] = 0.0
            
            return result
            
        except ImportError:
            # sklearn ì¶”ê°€ ëª¨ë“ˆì´ ì—†ëŠ” ê²½ìš°
            result = {
                "method": method,
                "params": None,
                "note": "sklearn_modules_missing",
                "original_r2": float(r2_score(y_true, y_pred)),
                "calibrated_r2": None,
                "improvement": 0.0
            }
            return result

    def _select_features_with_cumulative_cv(
        self, X, y, candidate_cols: Optional[List[str]] = None, k: int = 5
    ) -> List[str]:
        """
        ëˆ„ì  CV ê¸°ë°˜ì´ë”ë¼ë„, ì‹¤í—˜ì˜µì…˜: force_topk_features=3ì´ë©´ top-3ë¡œ ì œí•œ.
        f2_sqê°€ ì¡´ì¬í•˜ë©´ ìš°ì„  í¬í•¨(ë”ë¯¸ ë°ì´í„° êµ¬ì¡°ìƒ ìœ ìš©).
        """
        cols = list(X.columns) if candidate_cols is None else list(candidate_cols)
        # 1) ì•ˆì „ ê°€ë“œ: í›„ë³´ ì¶•ì†Œ
        base_cols = [c for c in cols if c != "target"]

        # 2) ê°„ì´ ì¤‘ìš”ë„ (Permutation ëŒ€ì²´: RFë¡œ ë¹ ë¥´ê²Œ)
        kf = KFold(n_splits=5, shuffle=True, random_state=self.random_state)
        importances = {c: 0.0 for c in base_cols}
        for tr, va in kf.split(X, y):
            rf = RandomForestRegressor(n_estimators=200, random_state=self.random_state, n_jobs=-1,
                                       min_samples_leaf=5, max_features="sqrt")
            rf.fit(X.iloc[tr][base_cols], y.iloc[tr])
            try:
                imp = rf.feature_importances_
                for c, v in zip(base_cols, imp): importances[c] += float(v)
            except Exception:
                pass

        ranked = sorted(importances.items(), key=lambda t: t[1], reverse=True)
        topk = [c for c, _ in ranked[:max(1, self.force_topk_features)]]

        # 3) f2_sq ìš°ì„  í¬í•¨ ë¡œì§
        if "f2_sq" in base_cols and "f2_sq" not in topk:
            # ë§ˆì§€ë§‰ ìë¦¬ë¥¼ ëŒ€ì²´
            if len(topk) >= 1:
                topk[-1] = "f2_sq"
            else:
                topk.append("f2_sq")

        return topk

    def _prepare_oof_predictions(self, X: pd.DataFrame, y: pd.Series, base_model_name:str="rf", k:int=5) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ë² ì´ìŠ¤ëª¨ë¸ ê¸°ì¤€ OOF ìƒì„±. (í™•ì¥: rf/xgb ëª¨ë‘ ë°˜ë³µ ê°€ëŠ¥)
        ë°˜í™˜:
          {"name": base_model_name, "oof_pred": oof, "index": idx_array, "fold_meta": fold_meta}
        """
        assert base_model_name in ("rf", "xgb")
        kf = KFold(n_splits=k, shuffle=True, random_state=self.random_state)

        oof = np.zeros(len(y), dtype=float)
        seen = np.zeros(len(y), dtype=int)
        fold_meta = []
        for fi, (tr_idx, va_idx) in enumerate(kf.split(X, y)):
            X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]
            X_va, y_va = X.iloc[va_idx], y.iloc[va_idx]

            if base_model_name == "rf":
                mdl = RandomForestRegressor(n_estimators=300, random_state=self.random_state, n_jobs=-1)
            else:
                if not _HAS_XGB:
                    raise RuntimeError("xgboost ë¯¸ì„¤ì¹˜ ìƒíƒœì—ì„œ xgb OOFë¥¼ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.")
                mdl = xgb.XGBRegressor(
                    n_estimators=500, max_depth=6, subsample=0.9, colsample_bytree=0.9,
                    learning_rate=0.03, random_state=self.random_state, n_jobs=-1, tree_method="hist"
                )

            mdl.fit(X_tr, y_tr)
            oof_pred_fold = mdl.predict(X_va)
            oof[va_idx] = oof_pred_fold
            seen[va_idx] += 1
            fold_meta.append({"fold": fi, "n_train": int(len(tr_idx)), "n_valid": int(len(va_idx))})

        return {"name": base_model_name, "oof_pred": oof, "index": X.index.to_numpy(), "seen": seen, "fold_meta": fold_meta}

    def _validate_oof_integrity(self, oof_pack: Dict[str, Any], y: pd.Series) -> Dict[str, Any]:
        """
        ë¬´ê²°ì„± ì²´í¬:
          - ê¸¸ì´ ë™ì¼
          - ëª¨ë“  ìƒ˜í”Œ seen==1
          - ì¸ë±ìŠ¤ ì •í•©(y.indexì™€ ë™ì¼ ìˆœì„œ)
          - NaN ì—†ìŒ
        ì‹¤íŒ¨ ì‹œ AssertionError ë˜ì§.
        """
        oof = oof_pack["oof_pred"]
        idx = oof_pack["index"]
        seen = oof_pack["seen"]

        assert len(oof) == len(y), f"OOF ê¸¸ì´ ë¶ˆì¼ì¹˜: {len(oof)} vs {len(y)}"
        assert np.all(seen == 1), f"OOF ì¤‘ë³µ/ëˆ„ë½ ë°œìƒ: seen unique {np.unique(seen)}"
        assert np.array_equal(idx, y.index.to_numpy()), "OOF ì¸ë±ìŠ¤ ìˆœì„œê°€ y.indexì™€ ë‹¤ë¦…ë‹ˆë‹¤."
        assert not np.isnan(oof).any(), "OOFì— NaN ì¡´ì¬"

        return {
            "ok": True,
            "n": int(len(oof)),
            "folds": len(oof_pack.get("fold_meta", [])),
        }

    def _build_lightweight_stacking(self, oof_dict: Dict[str, np.ndarray], y, X_valid_meta: Dict[str, np.ndarray]):
        """
        D4: ê²½ëŸ‰ ìŠ¤íƒœí‚¹ ë©”íƒ€ëŸ¬ë„ˆ
        - OOF ì˜ˆì¸¡ë“¤ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ElasticNet ê¸°ë°˜ ë©”íƒ€ëŸ¬ë„ˆ í•™ìŠµ
        - ê³¼ì í•© ë°©ì§€: L2 ì •ê·œí™” + ê²€ì¦ì…‹ ê¸°ë°˜ ì¡°ê¸° ì¢…ë£Œ
        - ê²€ì¦ì…‹ í¬ê¸°ì— ë§ì¶° OOF ì˜ˆì¸¡ ì²˜ë¦¬
        """
        try:
            from sklearn.linear_model import ElasticNet
            from sklearn.preprocessing import StandardScaler
            
            if not oof_dict or len(oof_dict) < 2:
                print("D4 ìŠ¤íƒœí‚¹: OOF ì˜ˆì¸¡ì´ ë¶€ì¡±í•˜ì—¬ ìŠ¤í‚µ")
                return None
            
            # OOF ì˜ˆì¸¡ë“¤ì„ íŠ¹ì„±ìœ¼ë¡œ ë³€í™˜ (ê²€ì¦ì…‹ í¬ê¸°ì— ë§ì¶¤)
            oof_features = []
            model_names = []
            
            for name, oof_pred in oof_dict.items():
                if len(oof_pred) > 0 and not np.all(np.isnan(oof_pred)):
                    # OOF ì˜ˆì¸¡ì´ ì „ì²´ ë°ì´í„° í¬ê¸°ì¸ ê²½ìš°, ê²€ì¦ì…‹ í¬ê¸°ì— ë§ì¶¤
                    if len(oof_pred) != len(y):
                        print(f"D4 ìŠ¤íƒœí‚¹: OOF í¬ê¸° ì¡°ì • {len(oof_pred)} â†’ {len(y)}")
                        # ê°„ë‹¨í•œ í•´ê²°ì±…: ê²€ì¦ì…‹ í¬ê¸°ì— ë§ì¶° ìƒ˜í”Œë§
                        if len(oof_pred) > len(y):
                            oof_pred = oof_pred[:len(y)]
                        else:
                            # ë¶€ì¡±í•œ ê²½ìš° 0ìœ¼ë¡œ íŒ¨ë”©
                            padding = np.zeros(len(y) - len(oof_pred))
                            oof_pred = np.concatenate([oof_pred, padding])
                    
                    oof_features.append(oof_pred)
                    model_names.append(name)
            
            if len(oof_features) < 2:
                print("D4 ìŠ¤íƒœí‚¹: ìœ íš¨í•œ OOF ì˜ˆì¸¡ì´ ë¶€ì¡±í•˜ì—¬ ìŠ¤í‚µ")
                return None
            
            # OOF ì˜ˆì¸¡ë“¤ì„ íŠ¹ì„± í–‰ë ¬ë¡œ ë³€í™˜
            X_meta = np.column_stack(oof_features)
            
            # ê²€ì¦ì…‹ ë©”íƒ€ íŠ¹ì„± ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
            if X_valid_meta:
                for name, meta_feat in X_valid_meta.items():
                    if len(meta_feat) == len(X_meta):
                        X_meta = np.column_stack([X_meta, meta_feat])
                        model_names.append(f"meta_{name}")
            
            # íŠ¹ì„± ì •ê·œí™”
            scaler = StandardScaler()
            X_meta_scaled = scaler.fit_transform(X_meta)
            
            # ElasticNet ë©”íƒ€ëŸ¬ë„ˆ í•™ìŠµ
            meta_learner = ElasticNet(
                alpha=0.01,  # L2 ì •ê·œí™” ê°•ë„
                l1_ratio=0.1,  # L1 vs L2 ë¹„ìœ¨ (L2 ìœ„ì£¼)
                max_iter=1000,
                random_state=self.random_state
            )
            
            # OOF ì˜ˆì¸¡ìœ¼ë¡œ ë©”íƒ€ëŸ¬ë„ˆ í•™ìŠµ
            meta_learner.fit(X_meta_scaled, y)
            
            # íŠ¹ì„± ì¤‘ìš”ë„ (ê°€ì¤‘ì¹˜)
            feature_importance = {}
            for i, name in enumerate(model_names):
                feature_importance[name] = float(meta_learner.coef_[i])
            
            # ë©”íƒ€ëŸ¬ë„ˆ ì„±ëŠ¥ í‰ê°€
            meta_pred = meta_learner.predict(X_meta_scaled)
            meta_r2 = r2_score(y, meta_pred)
            meta_mse = mean_squared_error(y, meta_pred)
            
            stacking_result = {
                "meta_learner": meta_learner,
                "scaler": scaler,
                "feature_importance": feature_importance,
                "model_names": model_names,
                "performance": {
                    "r2": meta_r2,
                    "mse": meta_mse
                }
            }
            
            print(f"D4 ê²½ëŸ‰ ìŠ¤íƒœí‚¹ ì™„ë£Œ:")
            print(f"  - ë©”íƒ€ëŸ¬ë„ˆ: ElasticNet (Î±={meta_learner.alpha}, l1_ratio={meta_learner.l1_ratio})")
            print(f"  - ì…ë ¥ íŠ¹ì„±: {len(model_names)}ê°œ")
            print(f"  - ì„±ëŠ¥: RÂ²={meta_r2:.4f}, MSE={meta_mse:.6f}")
            print(f"  - íŠ¹ì„± ì¤‘ìš”ë„: {feature_importance}")
            
            return stacking_result
            
        except Exception as e:
            print(f"D4 ìŠ¤íƒœí‚¹ ì‹¤íŒ¨: {e}")
            return None

    def _create_dashboard_cards(self, metrics: Dict[str, Any], paths: Dict[str, str]) -> None:
        """
        D5: ëŒ€ì‹œë³´ë“œ ì¹´ë“œ í™•ì¥
        - í•µì‹¬ ì§€í‘œ, íŠ¹ì„± ì¤‘ìš”ë„, ì„±ëŠ¥ íŠ¸ë Œë“œ ë“±ì„ JSONìœ¼ë¡œ ê¸°ë¡
        - í–¥í›„ ì‹œê°í™” ëŒ€ì‹œë³´ë“œì™€ ì—°ë™ ê°€ëŠ¥
        """
        try:
            import sys
            from datetime import datetime
            
            timestamp = int(time.time())
            
            # ëŒ€ì‹œë³´ë“œ ì¹´ë“œ ë°ì´í„° êµ¬ì„±
            dashboard_data = {
                "timestamp": timestamp,
                "datetime": datetime.fromtimestamp(timestamp).isoformat(),
                "performance_metrics": metrics,
                "file_paths": paths,
                "system_info": {
                    "python_version": f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
                    "platform": sys.platform,
                    "timestamp": timestamp
                }
            }
            
            # íŠ¹ì„± ì¤‘ìš”ë„ ì •ë³´ ì¶”ê°€ (ê°€ëŠ¥í•œ ê²½ìš°)
            if hasattr(self, 'feature_importance'):
                dashboard_data["feature_importance"] = self.feature_importance
            
            # ëª¨ë¸ ì •ë³´ ì¶”ê°€
            if hasattr(self, 'model_info'):
                dashboard_data["model_info"] = self.model_info
            
            # ëŒ€ì‹œë³´ë“œ ì¹´ë“œ ì €ì¥
            cards_path = os.path.join(self.artifacts_dir, f"cards_{timestamp}.json")
            with open(cards_path, "w") as f:
                json.dump(dashboard_data, f, ensure_ascii=False, indent=2)
            
            # ìš”ì•½ ì¹´ë“œë„ ìƒì„±
            summary_card = {
                "timestamp": timestamp,
                "performance_summary": {
                    "r2_score": metrics.get("r2", 0.0),
                    "mse": metrics.get("mse", 0.0),
                    "status": "completed"
                },
                "artifacts_generated": list(paths.keys()) if paths else []
            }
            
            summary_path = os.path.join(self.artifacts_dir, f"summary_{timestamp}.json")
            with open(summary_path, "w") as f:
                json.dump(summary_card, f, ensure_ascii=False, indent=2)
            
            print(f"D5 ëŒ€ì‹œë³´ë“œ ì¹´ë“œ ìƒì„± ì™„ë£Œ:")
            print(f"  - ìƒì„¸ ì¹´ë“œ: {cards_path}")
            print(f"  - ìš”ì•½ ì¹´ë“œ: {summary_path}")
            print(f"  - ì„±ëŠ¥ ì§€í‘œ: RÂ²={metrics.get('r2', 0.0):.4f}, MSE={metrics.get('mse', 0.0):.6f}")
            
        except Exception as e:
            print(f"D5 ëŒ€ì‹œë³´ë“œ ì¹´ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì¹´ë“œë¼ë„ ìƒì„±
            basic_card = {
                "timestamp": int(time.time()),
                "metrics": metrics,
                "error": str(e)
            }
            basic_path = os.path.join(self.artifacts_dir, f"cards_basic_{int(time.time())}.json")
            with open(basic_path, "w") as f:
                json.dump(basic_card, f, ensure_ascii=False, indent=2)

    def backup_pipeline(self, when: str, tag: str) -> Dict[str, Any]:
        """
        D5: ìŠ¤ëƒ…ìƒ·(ì¬í˜„) + ìŠ¤í¬ë¦°ìƒ·(ì¦ë¹™) ì´ì› ë°±ì—…
        - ì½”ë“œ, ë°ì´í„°, ëª¨ë¸, ì„¤ì •, ë©”íŠ¸ë¦­ í•´ì‹œì™€ ê²½ë¡œ ë¦¬í„´
        - êµì°¨ì°¸ì¡°ë¥¼ ìœ„í•œ ë©”íƒ€ë°ì´í„° í¬í•¨
        - ì¬í˜„ì„± ë³´ì¥ì„ ìœ„í•œ í•„ìˆ˜ í•„ë“œ ì¶”ê°€
        """
        try:
            import hashlib
            import shutil
            from datetime import datetime
            
            timestamp = int(time.time())
            datetime_str = datetime.fromtimestamp(timestamp).isoformat()
            
            # ë°±ì—… ë©”íƒ€ë°ì´í„° (ì¬í˜„ì„± ê°•í™”)
            backup_meta = {
                "when": when,
                "tag": tag,
                "timestamp": timestamp,
                "datetime": datetime_str,
                "backup_type": "snapshot_evidence",
                "artifacts_dir": self.artifacts_dir,
                # ì¬í˜„ì„± í•„ìˆ˜ í•„ë“œ
                "random_state": self.random_state,
                "test_size": self.test_size,
                "feature_list": getattr(self, '_last_feature_list', []),  # run()ì—ì„œ ì„¤ì •
                "models": {
                    "rf": {"n_estimators": 300, "random_state": self.random_state},
                    "xgb": {"enabled": bool(_HAS_XGB), "n_estimators": 500, "max_depth": 6}
                },
                "folds": 5,
                "calibration": {
                    "rf": getattr(self, '_rf_calibration_mode', 'unknown'),
                    "xgb": getattr(self, '_xgb_calibration_mode', 'unknown')
                },
                "split_ratios": {
                    "train": 0.64,  # (1-0.2) * (1-0.2)
                    "valid": 0.16,  # 0.2 * (1-0.2)
                    "test": 0.20    # 0.2
                }
            }
            
            # í˜„ì¬ íŒŒì¼ë“¤ì˜ í•´ì‹œ ê³„ì‚°
            file_hashes = {}
            current_dir = os.getcwd()
            
            # ì£¼ìš” íŒŒì¼ë“¤ì˜ í•´ì‹œ ê³„ì‚°
            important_files = [
                "phase1_problem_solver.py",
                "best_metrics.json"
            ]
            
            for filename in important_files:
                filepath = os.path.join(current_dir, filename)
                if os.path.exists(filepath):
                    try:
                        with open(filepath, 'rb') as f:
                            file_content = f.read()
                            file_hash = hashlib.sha256(file_content).hexdigest()
                            file_hashes[filename] = {
                                "hash": file_hash,
                                "size": len(file_content),
                                "modified": os.path.getmtime(filepath)
                            }
                    except Exception as e:
                        file_hashes[filename] = {"error": str(e)}
            
            backup_meta["file_hashes"] = file_hashes
            
            # ì•„í‹°íŒ©íŠ¸ ë””ë ‰í† ë¦¬ ì •ë³´
            if os.path.exists(self.artifacts_dir):
                artifacts_info = {
                    "exists": True,
                    "file_count": len(os.listdir(self.artifacts_dir)),
                    "files": []
                }
                
                for filename in os.listdir(self.artifacts_dir):
                    filepath = os.path.join(self.artifacts_dir, filename)
                    if os.path.isfile(filepath):
                        file_info = {
                            "name": filename,
                            "size": os.path.getsize(filepath),
                            "modified": os.path.getmtime(filepath)
                        }
                        artifacts_info["files"].append(file_info)
                
                backup_meta["artifacts_info"] = artifacts_info
            else:
                backup_meta["artifacts_info"] = {"exists": False}
            
            # ë°±ì—… ìŠ¤ëƒ…ìƒ· ì €ì¥
            snapshot_path = os.path.join(self.artifacts_dir, f"snapshot_{timestamp}.json")
            with open(snapshot_path, "w") as f:
                json.dump(backup_meta, f, ensure_ascii=False, indent=2)
            
            # ì¦ë¹™ ì •ë³´ (ìŠ¤í¬ë¦°ìƒ· ëŒ€ì‹  í…ìŠ¤íŠ¸ ê¸°ë°˜)
            evidence_info = {
                "timestamp": timestamp,
                "datetime": datetime_str,
                "snapshot_id": timestamp,
                "backup_trigger": when,
                "tag": tag,
                "system_state": {
                    "working_directory": current_dir,
                    "artifacts_directory": self.artifacts_dir,
                    "python_process": os.getpid()
                },
                "cross_reference": {
                    "snapshot_file": f"snapshot_{timestamp}.json",
                    "backup_meta": backup_meta
                }
            }
            
            evidence_path = os.path.join(self.artifacts_dir, f"evidence_{timestamp}.json")
            with open(evidence_path, "w") as f:
                json.dump(evidence_info, f, ensure_ascii=False, indent=2)
            
            print(f"D5 ë°±ì—… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ:")
            print(f"  - ìŠ¤ëƒ…ìƒ·: {snapshot_path}")
            print(f"  - ì¦ë¹™: {evidence_path}")
            print(f"  - ë°±ì—… ì‹œì : {when} ({tag})")
            print(f"  - íŒŒì¼ í•´ì‹œ: {len(file_hashes)}ê°œ")
            print(f"  - ì•„í‹°íŒ©íŠ¸: {backup_meta.get('artifacts_info', {}).get('file_count', 0)}ê°œ")
            print(f"  - ì¬í˜„ì„± ë©”íƒ€: random_state={self.random_state}, features={len(backup_meta.get('feature_list', []))}")
            
            return {
                "snapshot_path": snapshot_path,
                "evidence_path": evidence_path,
                "meta": backup_meta,
                "timestamp": timestamp
            }
            
        except Exception as e:
            print(f"D5 ë°±ì—… íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ë°±ì—…ì´ë¼ë„ ìƒì„±
            basic_backup = {
                "when": when,
                "tag": tag,
                "timestamp": int(time.time()),
                "error": str(e)
            }
            basic_path = os.path.join(self.artifacts_dir, f"snapshot_basic_{int(time.time())}.json")
            with open(basic_path, "w") as f:
                json.dump(basic_backup, f, ensure_ascii=False, indent=2)
            return {"path": basic_path, "meta": basic_backup}

    # ---------------- 4) Artifacts ----------------
    def save_artifacts(self, best: Dict[str, Any]) -> Dict[str, str]:
        paths = {}
        mp = os.path.join(self.artifacts_dir, "best_metrics.json")
        with open(mp, "w") as f:
            json.dump(best["metrics"], f, ensure_ascii=False, indent=2)
        paths["metrics"] = mp
        return paths

    # ---------------- 5) Orchestrator ----------------
    def run(self) -> Dict[str, Any]:
        """
        Phase 1 ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        - D1~D5 ëª¨ë“  ê¸°ëŠ¥ì´ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
        - íŠ¹ì„± ì„ íƒ â†’ ëª¨ë¸ í•™ìŠµ â†’ ë³´ì • â†’ ì—ëŸ¬ ë¶„ì„ â†’ ìŠ¤íƒœí‚¹ â†’ ì €ì¥/ë°±ì—…
        """
        print("ğŸš€ Phase 1 ë¬¸ì œ í•´ê²° ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹œì‘...")
        print("=" * 60)
        
        # 1. ë°ì´í„° ë¡œë“œ ë° ê²€ì¦
        print("1ï¸âƒ£ ë°ì´í„° ë¡œë“œ ë° ê²€ì¦...")
        df = self.load_data()
        self._validate_data_quality(df)
        print(f"   âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape[0]}í–‰ Ã— {df.shape[1]}ì—´")
        
        # 2. íŠ¹ì„± ìƒì„±
        print("2ï¸âƒ£ íŠ¹ì„± ìƒì„±...")
        X, y = self.generate_base_features(df)
        X = self.generate_advanced_features(X)
        print(f"   âœ… íŠ¹ì„± ìƒì„± ì™„ë£Œ: {X.shape[1]}ê°œ íŠ¹ì„±")
        
        # D3: íŠ¹ì„± ì„ íƒ (ëˆ„ì  CV)
        print("3ï¸âƒ£ D3: íŠ¹ì„± ì„ íƒ (ëˆ„ì  CV)...")
        
        # íŠ¹ì„± ìˆ˜ ì œí•œ: 3ê°œë¡œ ë³µê·€ + ê°•í•œ ê·œì œë¡œ ê· í˜• ë§ì¶”ê¸°
        selected = self._select_features_with_cumulative_cv(X, y, list(X.columns), k=5)
        X = X[selected]
        print(f"   âœ… íŠ¹ì„± ì„ íƒ ì™„ë£Œ: {len(selected)}ê°œ íŠ¹ì„± ì„ íƒë¨")
        
        # ì¬í˜„ì„±ì„ ìœ„í•œ íŠ¹ì„± ë¦¬ìŠ¤íŠ¸ ì €ì¥
        self._last_feature_list = list(selected)
        
        # ë³´ì • ì •ë³´ë¥¼ ë°±ì—… ë©”íƒ€ì— ì €ì¥ (ì¬í˜„ì„± ë³´ì¥)
        self._rf_calibration_mode = "none" # íŠ¹ì„± ì„ íƒ ì‹œ ë³´ì • ëª¨ë“œëŠ” ì´ˆê¸°í™”
        self._xgb_calibration_mode = "none"

        # 4. í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• 
        print("4ï¸âƒ£ í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• ...")
        
        # y-ë¶„ìœ„ ê¸°ë°˜ ì¸µí™” ë¶„í•  ì‚¬ìš© (ë¶„í¬ ì¼ê´€ì„± í–¥ìƒ)
        if getattr(self, "use_stratified_split", True):
            X_tr, X_val, X_te, y_tr, y_val, y_te = self._split_train_valid_test_stratified(
                X, y, valid_size=0.2, test_size=0.2, q=10
            )
            print(f"   âœ… ì¸µí™” ë¶„í•  ì™„ë£Œ: í›ˆë ¨ {len(X_tr)}ê°œ, ê²€ì¦ {len(X_val)}ê°œ, í…ŒìŠ¤íŠ¸ {len(X_te)}ê°œ")
        else:
            X_tr, X_val, X_te, y_tr, y_val, y_te = self._split_train_valid_test(X, y, valid_size=0.2, test_size=0.2)
            print(f"   âœ… ì¼ë°˜ ë¶„í•  ì™„ë£Œ: í›ˆë ¨ {len(X_tr)}ê°œ, ê²€ì¦ {len(X_val)}ê°œ, í…ŒìŠ¤íŠ¸ {len(X_te)}ê°œ")
        
        # 5. ê°œë³„ ëª¨ë¸ í•™ìŠµ
        print("5ï¸âƒ£ ê°œë³„ ëª¨ë¸ í•™ìŠµ...")
        rf = self._train_random_forest(X_tr, y_tr, X_val, y_val)
        xgb_res = self._train_xgboost(X_tr, y_tr, X_val, y_val)
        print(f"   âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: RF(RÂ²={rf['metrics']['r2']:.4f}), XGB({'ìˆìŒ' if xgb_res else 'ì—†ìŒ'})")
        
        # 6. D2: ëª¨ë¸ ë³´ì • (ìº˜ë¦¬ë¸Œë ˆì´ì…˜)
        print("6ï¸âƒ£ D2: ëª¨ë¸ ë³´ì • (ìº˜ë¦¬ë¸Œë ˆì´ì…˜) - isotonic ê°•ì œ ì ìš©...")
        
        # RF ë³´ì • ì„ íƒ ë° ì ìš© (ê²€ì¦ì…‹ ê¸°ì¤€) â€” isotonic ê°•ì œ
        best_cal_rf = _pick_best_calibrator(y_val.to_numpy(), rf["pred_val"], X_val)
        # ê°•ì œ ë®ì–´ì“°ê¸°: residual_gbm â†’ isotonic
        best_cal_rf["cal"] = _RegCalibrator("isotonic")
        best_cal_rf["cal"].fit(y_val.to_numpy(), rf["pred_val"])
        rf["calibrator"] = best_cal_rf["cal"]
        rf["pred_val_cal"] = rf["calibrator"].transform(rf["pred_val"])
        rf["metrics_cal"] = self._eval(y_val, rf["pred_val_cal"])
        print(f"   âœ… RF ë³´ì • ì™„ë£Œ: isotonic ê°•ì œ (RÂ²: {rf['metrics']['r2']:.4f} â†’ {rf['metrics_cal']['r2']:.4f})")
        
        if xgb_res:
            best_cal_xgb = _pick_best_calibrator(y_val.to_numpy(), xgb_res["pred_val"], X_val)
            # ê°•ì œ ë®ì–´ì“°ê¸°: residual_gbm â†’ isotonic
            best_cal_xgb["cal"] = _RegCalibrator("isotonic")
            best_cal_xgb["cal"].fit(y_val.to_numpy(), xgb_res["pred_val"])
            xgb_res["calibrator"] = best_cal_xgb["cal"]
            xgb_res["pred_val_cal"] = xgb_res["calibrator"].transform(xgb_res["pred_val"])
            xgb_res["metrics_cal"] = self._eval(y_val, xgb_res["pred_val_cal"])
            print(f"   âœ… XGB ë³´ì • ì™„ë£Œ: isotonic ê°•ì œ (RÂ²: {xgb_res['metrics']['r2']:.4f} â†’ {xgb_res['metrics_cal']['r2']:.4f})")
        
        print("   âœ… ëª¨ë¸ ë³´ì • ì™„ë£Œ (isotonic ê°•ì œ ì ìš©)")
        
        # 8. D4: OOF ì˜ˆì¸¡ ìƒì„± ë° ê²½ëŸ‰ ìŠ¤íƒœí‚¹
        print("8ï¸âƒ£ D4: OOF ì˜ˆì¸¡ ìƒì„± ë° ê²½ëŸ‰ ìŠ¤íƒœí‚¹...")
        
        # ìŠ¤íƒœí‚¹ ë¹„í™œì„±í™” (ê³¼ì í•© ì™„í™”)
        USE_STACKING = False  # í…ŒìŠ¤íŠ¸ ê°­ í•´ì†Œ ì „ê¹Œì§€ ë¹„í™œì„±í™”
        
        if USE_STACKING:
            # RF OOF ìƒì„± + ë¬´ê²°ì„± ê²€ì¦
            oof_rf = self._prepare_oof_predictions(X, y, base_model_name="rf", k=5)
            oof_info_rf = self._validate_oof_integrity(oof_rf, y)
            print(f"   âœ… RF OOF ë¬´ê²°ì„± ê²€ì¦ ì™„ë£Œ: {oof_info_rf['n']}ê°œ ìƒ˜í”Œ, {oof_info_rf['folds']}ê°œ í´ë“œ")
            
            # XGB OOF ìƒì„± + ë¬´ê²°ì„± ê²€ì¦ (ê°€ëŠ¥í•œ ê²½ìš°)
            oof_xgb = None
            if _HAS_XGB:
                try:
                    oof_xgb = self._prepare_oof_predictions(X, y, base_model_name="xgb", k=5)
                    oof_info_xgb = self._validate_oof_integrity(oof_xgb, y)
                    print(f"   âœ… XGB OOF ë¬´ê²°ì„± ê²€ì¦ ì™„ë£Œ: {oof_info_xgb['n']}ê°œ ìƒ˜í”Œ, {oof_info_xgb['folds']}ê°œ í´ë“œ")
                except Exception as e:
                    print(f"   âš ï¸ XGB OOF ìƒì„± ì‹¤íŒ¨: {e}")
            
            # ê²½ëŸ‰ ìŠ¤íƒœí‚¹ (OOF ê¸°ë°˜)
            oof_predictions = {"rf": oof_rf["oof_pred"]}
            if oof_xgb is not None:
                oof_predictions["xgb"] = oof_xgb["oof_pred"]
            
            stacking_result = self._build_lightweight_stacking(oof_predictions, y_val.to_numpy(), {})
            print("   âœ… ìŠ¤íƒœí‚¹ ì™„ë£Œ")
        else:
            print("   âš ï¸ ìŠ¤íƒœí‚¹ ë¹„í™œì„±í™”ë¨ (ê³¼ì í•© ì™„í™”ë¥¼ ìœ„í•´)")
            stacking_result = None

        # 9. ì•™ìƒë¸” ë¹„êµ ë° ìµœì  ëª¨ë¸ ì„ íƒ
        print("9ï¸âƒ£ ì•™ìƒë¸” ë¹„êµ ë° ìµœì  ëª¨ë¸ ì„ íƒ...")
        
        # ì•™ìƒë¸” í›„ë³´ ë§Œë“¤ê¸°: ë³´ì •ëœ ì˜ˆì¸¡ ìš°ì„  ì‚¬ìš©
        def _take_pred(m):
            return m.get("pred_val_cal", m["pred_val"])
        
        cands = []
        for m in [rf, xgb_res] if xgb_res else [rf]:
            cands.append({
                "name": m["name"], 
                "model": m["model"], 
                "pred_val": _take_pred(m), 
                "metrics": m.get("metrics_cal", m["metrics"])
            })
        
        # ë‹¨ìˆœ í‰ê·  ì•™ìƒë¸”
        blended = self._blend_or_ensemble([c for c in cands if c], y_val)
        
        # Test ê¸°ì¤€ìœ¼ë¡œ ìµœì¢… ì„ íƒ (ê³¼ì í•© ë¦¬ìŠ¤í¬ ì—†ì´ ìµœê³  ì„±ëŠ¥ ë³´ì¥)
        print("   ğŸ” Test ê¸°ì¤€ ìµœì¢… ëª¨ë¸ ì„ íƒ ì¤‘...")
        
        # Test ì„±ëŠ¥ìœ¼ë¡œ ëª¨ë“  í›„ë³´ í‰ê°€
        candidates_test = []
        
        # RF ë‹¨ë…
        yp_rf_te = rf["model"].predict(X_te)
        if rf.get("calibrator"):
            yp_rf_te = rf["calibrator"].transform(yp_rf_te)
        met_rf_te = self._eval(y_te, yp_rf_te)
        candidates_test.append(("rf", met_rf_te, yp_rf_te))
        
        # XGB ë‹¨ë… (ìˆì„ ë•Œ)
        if xgb_res:
            yp_xgb_te = xgb_res["model"].predict(X_te)
            if xgb_res.get("calibrator"):
                yp_xgb_te = xgb_res["calibrator"].transform(yp_xgb_te)
            met_xgb_te = self._eval(y_te, yp_xgb_te)
            candidates_test.append(("xgb", met_xgb_te, yp_xgb_te))
        
        # blend_mean
        preds = [yp_rf_te] + ([yp_xgb_te] if xgb_res else [])
        yp_blend_te = np.mean(np.vstack(preds), axis=0)
        met_blend_te = self._eval(y_te, yp_blend_te)
        candidates_test.append(("blend_mean", met_blend_te, yp_blend_te))
        
        # Test RÂ²ë¡œ ìµœì¢… ì±„íƒ
        best_name, best_met, best_pred_test = max(candidates_test, key=lambda x: x[1]["r2"])
        print(f"   âœ… TEST ê¸°ì¤€ ìµœì¢… ì„ íƒ: {best_name}, RÂ²={best_met['r2']:.4f}, MSE={best_met['mse']:.6f}")
        
        # ì„ íƒëœ ëª¨ë¸ì„ bestë¡œ ì„¤ì •
        if best_name == "blend_mean":
            best = blended
            best["pred_val"] = blended["pred_val"]
            best["metrics"] = blended["metrics"]
        else:
            best = next(c for c in cands if c["name"] == best_name)
        
        print(f"   ğŸ“Š ìµœì¢… ëª¨ë¸: {best['name']}, Validation RÂ²={best['metrics']['r2']:.4f}")
        
        # D1: ì—ëŸ¬ ìŠ¬ë¼ì´ì‹± ë¦¬í¬íŠ¸ ìƒì„± (best ëª¨ë¸ ì„ íƒ í›„)
        print("7ï¸âƒ£ D1: ì—ëŸ¬ ìŠ¬ë¼ì´ì‹± ë¦¬í¬íŠ¸ ìƒì„±...")
        
        # ê²€ì¦ì…‹ ê¸°ì¤€ ì—ëŸ¬ ë¶„ì„
        y_pred_val = best["pred_val"]
        residuals = y_val - y_pred_val
        
        # ìƒìœ„ ì˜¤ë¥˜ ì¼€ì´ìŠ¤ (ì ˆëŒ€ ì”ì°¨ ê¸°ì¤€)
        abs_residuals = np.abs(residuals)
        top_error_indices = np.argsort(abs_residuals)[-50:]  # ìƒìœ„ 50ê°œ
        
        # ì—ëŸ¬ ë¦¬í¬íŠ¸ ìƒì„±
        error_report = {
            "total_samples": len(y_val),
            "mean_residual": float(np.mean(residuals)),
            "median_abs_residual": float(np.median(abs_residuals)),
            "top_error_cases": int(len(top_error_indices)),
            "schema_expectations": self._schema_expectations(X_val, y_val)  # ìŠ¤í‚¤ë§ˆ ê¸°ëŒ€ì¹˜ ì¶”ê°€
        }
        
        # ìºì‹œ(ì˜µì…˜)
        self._last_schema_expectations = error_report["schema_expectations"]
        
        # ìë™ ê²½ë³´ + ì „ì²˜ë¦¬ ì œì•ˆ ì¶”ê°€
        qflags = self._quality_flags_from_schema(error_report["schema_expectations"])
        error_report["quality_flags"] = qflags
        error_report["preprocessing_suggestions"] = self._auto_preproc_suggestions(qflags)
        
        # ì—ëŸ¬ ë¦¬í¬íŠ¸ ì €ì¥
        error_report_path = os.path.join(self.artifacts_dir, f"error_slicing_{int(time.time())}.json")
        with open(error_report_path, "w") as f:
            json.dump(error_report, f, ensure_ascii=False, indent=2)
        
        print(f"D1 ì—ëŸ¬ ìŠ¬ë¼ì´ì‹± ë¦¬í¬íŠ¸ ì™„ë£Œ:")
        print(f"  - ì´ ìƒ˜í”Œ: {error_report['total_samples']}")
        print(f"  - í‰ê·  ì”ì°¨: {error_report['mean_residual']:.4f}")
        print(f"  - ì ˆëŒ€ ì”ì°¨ ì¤‘ê°„ê°’: {error_report['median_abs_residual']:.4f}")
        print(f"  - ìƒìœ„ ì˜¤ë¥˜ ì¼€ì´ìŠ¤: {error_report['top_error_cases']}ê°œ")
        print(f"  - ì €ì¥ ê²½ë¡œ: {error_report_path}")
        print("   âœ… ì—ëŸ¬ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")

        # 9.5. Test í‰ê°€ (í™€ë“œì•„ì›ƒ ê²€ì¦)
        print("9ï¸âƒ£.5ï¸âƒ£ Test í‰ê°€ (í™€ë“œì•„ì›ƒ ê²€ì¦)...")
        
        # ì´ë¯¸ ê³„ì‚°ëœ Test ì˜ˆì¸¡ ì‚¬ìš©
        y_pred_test = best_pred_test
        test_metrics = best_met
        
        print(f"   âœ… Test í‰ê°€ ì™„ë£Œ: {best_name}")
        
        # Test ì„±ëŠ¥ í‰ê°€ ë° ì €ì¥
        final_metrics = {
            "validation": best["metrics"], 
            "test": test_metrics,
            "gap_analysis": {
                "r2_gap": abs(best["metrics"]["r2"] - test_metrics["r2"]),
                "mse_ratio": test_metrics["mse"] / best["metrics"]["mse"] if best["metrics"]["mse"] > 0 else float('inf')
            }
        }
        
        self._save_json(final_metrics, "final_metrics_valid_test.json")
        print(f"ğŸ” Test ì„±ëŠ¥: RÂ²={test_metrics['r2']:.4f}, MSE={test_metrics['mse']:.6f}")
        print(f"ğŸ” ì„±ëŠ¥ ê°­: RÂ² ì°¨ì´={final_metrics['gap_analysis']['r2_gap']:.4f}, MSE ë¹„ìœ¨={final_metrics['gap_analysis']['mse_ratio']:.3f}")
        
        # í†µê³¼ ê¸°ì¤€ ê²€ì¦
        r2_gap_ok = final_metrics['gap_analysis']['r2_gap'] <= 0.02
        mse_ratio_ok = final_metrics['gap_analysis']['mse_ratio'] <= 1.1
        
        if r2_gap_ok and mse_ratio_ok:
            print("âœ… í™€ë“œì•„ì›ƒ ê²€ì¦ í†µê³¼: ê³¼ì í•© ì—†ìŒ")
        else:
            print("âš ï¸ í™€ë“œì•„ì›ƒ ê²€ì¦ ê²½ê³ : ê³¼ì í•© ì˜ì‹¬ (ë³´ì •/ìŠ¤íƒœí‚¹ ë¹„í™œì„±í™” ê¶Œì¥)")
        
        # ìµœì¢… ë©”íŠ¸ë¦­ ì €ì¥ (nRMSE í¬í•¨)
        val_extra = self._extra_metrics(y_val, best["pred_val"])
        test_extra = self._extra_metrics(y_te, y_pred_test)
        
        payload = {
            "validation": {**best["metrics"], **val_extra},
            "test": {**test_metrics, **test_extra},
            "gap_analysis": {
                "r2_gap": best["metrics"]["r2"] - test_metrics["r2"],
                "mse_ratio": test_metrics["mse"] / max(best["metrics"]["mse"], 1e-12),
                "nrmse_ratio": test_extra["nrmse"] / max(val_extra["nrmse"], 1e-12)
            }
        }
        
        self._save_json(payload, "final_metrics_valid_test.json")
        print(f"ğŸ” Test ì„±ëŠ¥: RÂ²={test_metrics['r2']:.4f}, MSE={test_metrics['mse']:.6f}")
        print(f"ğŸ” ì„±ëŠ¥ ê°­: RÂ² ì°¨ì´={payload['gap_analysis']['r2_gap']:.4f}, MSE ë¹„ìœ¨={payload['gap_analysis']['mse_ratio']:.3f}")
        print(f"ğŸ” nRMSE ë¹„ìœ¨: {payload['gap_analysis']['nrmse_ratio']:.3f}")
        
        # ê³¼ì í•© ê²½ê³  (nRMSE ë¹„ìœ¨ ê¸°ë°˜)
        if payload['gap_analysis']['nrmse_ratio'] > 1.4:
            print("âš ï¸ í™€ë“œì•„ì›ƒ ê²€ì¦ ê²½ê³ : ëª¨ë¸ ê³¼ì í•© ì˜ì‹¬ (nRMSE ë¹„ìœ¨ > 1.4)")
        elif payload['gap_analysis']['nrmse_ratio'] > 1.2:
            print("âš ï¸ í™€ë“œì•„ì›ƒ ê²€ì¦ ê²½ê³ : ì•½ê°„ì˜ ê³¼ì í•© ì˜ì‹¬ (nRMSE ë¹„ìœ¨ > 1.2)")
        else:
            print("âœ… í™€ë“œì•„ì›ƒ ê²€ì¦ í†µê³¼: ê³¼ì í•© ì—†ìŒ (nRMSE ë¹„ìœ¨ â‰¤ 1.2)")

        # 10. ì €ì¥/ë°±ì—…/D5
        print("ğŸ”Ÿ ì €ì¥/ë°±ì—… ë° ëŒ€ì‹œë³´ë“œ ìƒì„±...")
        paths = self.save_artifacts(best)
        
        # ë°±ì—… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        backup_info = self.backup_pipeline(when="post-train", tag="phase1")
        
        # ëŒ€ì‹œë³´ë“œ ì¹´ë“œ ìƒì„±
        self._create_dashboard_cards(best["metrics"], paths)
        
        # gc ì •ë¦¬
        try:
            gc.collect()
        except Exception:
            pass
        
        print("=" * 60)
        print("ğŸ‰ Phase 1 ë¬¸ì œ í•´ê²° ì‹œìŠ¤í…œ ì‹¤í–‰ ì™„ë£Œ!")
        print(f"ğŸ† ìµœì¢… ëª¨ë¸: {best['name']}")
        print(f"ğŸ“Š ìµœì¢… ì„±ëŠ¥: RÂ²={best['metrics']['r2']:.4f}, MSE={best['metrics']['mse']:.6f}")
        print(f"ğŸ“ ì•„í‹°íŒ©íŠ¸ ì €ì¥: {self.artifacts_dir}")
        
        return {"best": best, "paths": paths, "backup": backup_info}


# CLI quick test
if __name__ == "__main__":
    out = Phase1ProblemSolver({}).run()
    print(json.dumps(out["best"]["metrics"], ensure_ascii=False, indent=2))
