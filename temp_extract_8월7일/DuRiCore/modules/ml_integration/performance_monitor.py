"""
ğŸš€ ìë™ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëª¨ë“ˆ (Advanced Performance Monitor)
í†µí•© ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ê³  ìë™ ìµœì í™” ì œì•ˆì„ ì œê³µí•˜ëŠ” ê³ ê¸‰ ëª¨ë“ˆ

ì£¼ìš” ê¸°ëŠ¥:
â€¢ ì‹¤ì‹œê°„ ì„±ëŠ¥ ì§€í‘œ ìë™ ìˆ˜ì§‘
â€¢ ì„±ëŠ¥ ê²½ê³  ì‹œìŠ¤í…œ
â€¢ ìë™ ìµœì í™” ì œì•ˆ
â€¢ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ ìƒì„±
â€¢ ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ ìë™ ì‹¤í–‰
"""

import logging
import threading
import time
import psutil
import json
from typing import Dict, Any, List, Optional, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from enum import Enum
import queue

logger = logging.getLogger(__name__)

class PerformanceLevel(Enum):
    """ì„±ëŠ¥ ìˆ˜ì¤€ ì •ì˜"""
    EXCELLENT = "excellent"
    GOOD = "good"
    FAIR = "fair"
    POOR = "poor"
    CRITICAL = "critical"

class AlertType(Enum):
    """ê²½ê³  ìœ í˜• ì •ì˜"""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

@dataclass
class PerformanceMetric:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    timestamp: datetime
    cpu_usage: float
    memory_usage: float
    execution_time: float
    success_rate: float
    error_count: int
    throughput: float
    latency: float
    score: float
    level: PerformanceLevel
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        data = asdict(self)
        data['timestamp'] = self.timestamp.isoformat()
        data['level'] = self.level.value
        return data

@dataclass
class PerformanceAlert:
    """ì„±ëŠ¥ ê²½ê³  ë°ì´í„° í´ë˜ìŠ¤"""
    timestamp: datetime
    alert_type: AlertType
    message: str
    metric: str
    current_value: float
    threshold: float
    recommendation: str
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        data = asdict(self)
        data['timestamp'] = self.timestamp.isoformat()
        data['alert_type'] = self.alert_type.value
        return data

@dataclass
class OptimizationSuggestion:
    """ìµœì í™” ì œì•ˆ ë°ì´í„° í´ë˜ìŠ¤"""
    timestamp: datetime
    priority: str
    category: str
    title: str
    description: str
    expected_improvement: str
    implementation_steps: List[str]
    estimated_effort: str
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        data = asdict(self)
        data['timestamp'] = self.timestamp.isoformat()
        return data

class AdvancedPerformanceMonitor:
    """ê³ ê¸‰ ìë™ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        ê³ ê¸‰ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ì •ë³´
        """
        self.config = config or {}
        
        # ëª¨ë‹ˆí„°ë§ ìƒíƒœ
        self.monitoring_active = False
        self.monitoring_thread = None
        self.monitoring_interval = self.config.get('monitoring_interval', 30)  # 30ì´ˆë§ˆë‹¤
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.performance_history: List[PerformanceMetric] = []
        self.alerts: List[PerformanceAlert] = []
        self.optimization_suggestions: List[OptimizationSuggestion] = []
        
        # ì„±ëŠ¥ ì„ê³„ê°’ (ë™ì  ì¡°ì • ê°€ëŠ¥)
        self.performance_thresholds = {
            'cpu_usage': {'warning': 70.0, 'critical': 90.0},
            'memory_usage': {'warning': 80.0, 'critical': 95.0},
            'execution_time': {'warning': 30.0, 'critical': 60.0},
            'success_rate': {'warning': 0.8, 'critical': 0.6},
            'error_count': {'warning': 5, 'critical': 10},
            'throughput': {'warning': 100.0, 'critical': 50.0},
            'latency': {'warning': 1000.0, 'critical': 5000.0}
        }
        
        # ì„±ëŠ¥ ì ìˆ˜ ê°€ì¤‘ì¹˜
        self.score_weights = {
            'cpu_usage': 0.15,
            'memory_usage': 0.15,
            'execution_time': 0.20,
            'success_rate': 0.25,
            'error_count': 0.15,
            'throughput': 0.05,
            'latency': 0.05
        }
        
        # ì½œë°± í•¨ìˆ˜ë“¤
        self.alert_callbacks: List[Callable] = []
        self.optimization_callbacks: List[Callable] = []
        
        # í†µê³„ ì •ë³´
        self.stats = {
            'total_metrics_collected': 0,
            'total_alerts_generated': 0,
            'total_suggestions_generated': 0,
            'monitoring_start_time': None,
            'last_optimization_time': None
        }
        
        logger.info("ğŸš€ ê³ ê¸‰ ìë™ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def start_monitoring(self, background: bool = True) -> bool:
        """
        ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        
        Args:
            background: ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í• ì§€ ì—¬ë¶€
        
        Returns:
            bool: ì‹œì‘ ì„±ê³µ ì—¬ë¶€
        """
        try:
            if self.monitoring_active:
                logger.warning("ëª¨ë‹ˆí„°ë§ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
                return True
            
            self.monitoring_active = True
            self.stats['monitoring_start_time'] = datetime.now()
            
            if background:
                # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰
                self.monitoring_thread = threading.Thread(
                    target=self._monitoring_loop,
                    daemon=True,
                    name="PerformanceMonitor"
                )
                self.monitoring_thread.start()
                logger.info("ğŸš€ ë°±ê·¸ë¼ìš´ë“œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
            else:
                # ë™ê¸°ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰
                self._collect_performance_metrics()
                logger.info("ğŸš€ ë™ê¸° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰")
            
            return True
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘ ì‹¤íŒ¨: {e}")
            self.monitoring_active = False
            return False
    
    def stop_monitoring(self) -> bool:
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        try:
            self.monitoring_active = False
            
            if self.monitoring_thread and self.monitoring_thread.is_alive():
                self.monitoring_thread.join(timeout=5)
            
            logger.info("ğŸ›‘ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
            return True
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ ì‹¤íŒ¨: {e}")
            return False
    
    def _monitoring_loop(self):
        """ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        logger.info("ğŸ”„ ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì‹œì‘")
        
        while self.monitoring_active:
            try:
                self._collect_performance_metrics()
                time.sleep(self.monitoring_interval)
            except Exception as e:
                logger.error(f"ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì˜¤ë¥˜: {e}")
                time.sleep(5)  # ì˜¤ë¥˜ ë°œìƒì‹œ ì ì‹œ ëŒ€ê¸°
        
        logger.info("ğŸ”„ ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì¢…ë£Œ")
    
    def _collect_performance_metrics(self):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë³´ ìˆ˜ì§‘
            cpu_usage = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            memory_usage = memory.percent
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìƒì„±
            metric = PerformanceMetric(
                timestamp=datetime.now(),
                cpu_usage=cpu_usage,
                memory_usage=memory_usage,
                execution_time=self._measure_execution_time(),
                success_rate=self._calculate_success_rate(),
                error_count=self._count_errors(),
                throughput=self._calculate_throughput(),
                latency=self._measure_latency(),
                score=0.0,  # ë‚˜ì¤‘ì— ê³„ì‚°
                level=PerformanceLevel.GOOD  # ë‚˜ì¤‘ì— ê²°ì •
            )
            
            # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
            metric.score = self._calculate_performance_score(metric)
            metric.level = self._determine_performance_level(metric.score)
            
            # ë©”íŠ¸ë¦­ ì €ì¥
            self.performance_history.append(metric)
            self.stats['total_metrics_collected'] += 1
            
            # ì´ë ¥ í¬ê¸° ì œí•œ (ìµœê·¼ 1000ê°œë§Œ ìœ ì§€)
            if len(self.performance_history) > 1000:
                self.performance_history = self.performance_history[-1000:]
            
            # ê²½ê³  ìƒì„± ë° ìµœì í™” ì œì•ˆ
            self._check_performance_alerts(metric)
            self._generate_optimization_suggestions(metric)
            
            logger.debug(f"ğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì™„ë£Œ: ì ìˆ˜={metric.score:.2f}, ìˆ˜ì¤€={metric.level.value}")
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def _measure_execution_time(self) -> float:
        """ì‹¤í–‰ ì‹œê°„ ì¸¡ì • (ì‹œë®¬ë ˆì´ì…˜)"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹¤ì œ ì‘ì—…ì˜ ì‹¤í–‰ ì‹œê°„ì„ ì¸¡ì •
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ê°’ ë°˜í™˜
            import random
            return random.uniform(0.1, 2.0)
        except:
            return 1.0
    
    def _calculate_success_rate(self) -> float:
        """ì„±ê³µë¥  ê³„ì‚° (ì‹œë®¬ë ˆì´ì…˜)"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹¤ì œ ì‘ì—…ì˜ ì„±ê³µë¥ ì„ ê³„ì‚°
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ê°’ ë°˜í™˜
            import random
            return random.uniform(0.85, 0.99)
        except:
            return 0.95
    
    def _count_errors(self) -> int:
        """ì˜¤ë¥˜ ìˆ˜ ê³„ì‚° (ì‹œë®¬ë ˆì´ì…˜)"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹¤ì œ ì˜¤ë¥˜ ìˆ˜ë¥¼ ê³„ì‚°
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ê°’ ë°˜í™˜
            import random
            return random.randint(0, 3)
        except:
            return 1
    
    def _calculate_throughput(self) -> float:
        """ì²˜ë¦¬ëŸ‰ ê³„ì‚° (ì‹œë®¬ë ˆì´ì…˜)"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹¤ì œ ì²˜ë¦¬ëŸ‰ì„ ê³„ì‚°
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ê°’ ë°˜í™˜
            import random
            return random.uniform(80.0, 120.0)
        except:
            return 100.0
    
    def _measure_latency(self) -> float:
        """ì§€ì—°ì‹œê°„ ì¸¡ì • (ì‹œë®¬ë ˆì´ì…˜)"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹¤ì œ ì§€ì—°ì‹œê°„ì„ ì¸¡ì •
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ê°’ ë°˜í™˜
            import random
            return random.uniform(50.0, 200.0)
        except:
            return 100.0
    
    def _calculate_performance_score(self, metric: PerformanceMetric) -> float:
        """ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°"""
        try:
            score = 0.0
            
            # CPU ì‚¬ìš©ë¥  (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            cpu_score = max(0, 100 - metric.cpu_usage) / 100
            score += cpu_score * self.score_weights['cpu_usage']
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            memory_score = max(0, 100 - metric.memory_usage) / 100
            score += memory_score * self.score_weights['memory_usage']
            
            # ì‹¤í–‰ ì‹œê°„ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            exec_score = max(0, 1 - (metric.execution_time / 60))  # 60ì´ˆ ê¸°ì¤€
            score += exec_score * self.score_weights['execution_time']
            
            # ì„±ê³µë¥  (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
            success_score = metric.success_rate
            score += success_score * self.score_weights['success_rate']
            
            # ì˜¤ë¥˜ ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            error_score = max(0, 1 - (metric.error_count / 10))  # 10ê°œ ê¸°ì¤€
            score += error_score * self.score_weights['error_count']
            
            # ì²˜ë¦¬ëŸ‰ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
            throughput_score = min(1.0, metric.throughput / 100)  # 100 ê¸°ì¤€
            score += throughput_score * self.score_weights['throughput']
            
            # ì§€ì—°ì‹œê°„ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            latency_score = max(0, 1 - (metric.latency / 1000))  # 1000ms ê¸°ì¤€
            score += latency_score * self.score_weights['latency']
            
            return min(1.0, max(0.0, score))
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _determine_performance_level(self, score: float) -> PerformanceLevel:
        """ì„±ëŠ¥ ìˆ˜ì¤€ ê²°ì •"""
        if score >= 0.9:
            return PerformanceLevel.EXCELLENT
        elif score >= 0.8:
            return PerformanceLevel.GOOD
        elif score >= 0.6:
            return PerformanceLevel.FAIR
        elif score >= 0.4:
            return PerformanceLevel.POOR
        else:
            return PerformanceLevel.CRITICAL
    
    def _check_performance_alerts(self, metric: PerformanceMetric):
        """ì„±ëŠ¥ ê²½ê³  í™•ì¸ ë° ìƒì„±"""
        try:
            alerts_created = []
            
            # CPU ì‚¬ìš©ë¥  ê²½ê³ 
            if metric.cpu_usage > self.performance_thresholds['cpu_usage']['critical']:
                alert = PerformanceAlert(
                    timestamp=datetime.now(),
                    alert_type=AlertType.CRITICAL,
                    message=f"CPU ì‚¬ìš©ë¥ ì´ ì„ê³„ê°’ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤: {metric.cpu_usage:.1f}%",
                    metric="cpu_usage",
                    current_value=metric.cpu_usage,
                    threshold=self.performance_thresholds['cpu_usage']['critical'],
                    recommendation="CPU ì§‘ì•½ì ì¸ ì‘ì—…ì„ ì¤„ì´ê±°ë‚˜ ë¦¬ì†ŒìŠ¤ë¥¼ ì¶”ê°€í•˜ì„¸ìš”"
                )
                alerts_created.append(alert)
                
            elif metric.cpu_usage > self.performance_thresholds['cpu_usage']['warning']:
                alert = PerformanceAlert(
                    timestamp=datetime.now(),
                    alert_type=AlertType.WARNING,
                    message=f"CPU ì‚¬ìš©ë¥ ì´ ê²½ê³  ìˆ˜ì¤€ì…ë‹ˆë‹¤: {metric.cpu_usage:.1f}%",
                    metric="cpu_usage",
                    current_value=metric.cpu_usage,
                    threshold=self.performance_thresholds['cpu_usage']['warning'],
                    recommendation="CPU ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  í•„ìš”ì‹œ ìµœì í™”í•˜ì„¸ìš”"
                )
                alerts_created.append(alert)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê²½ê³ 
            if metric.memory_usage > self.performance_thresholds['memory_usage']['critical']:
                alert = PerformanceAlert(
                    timestamp=datetime.now(),
                    alert_type=AlertType.CRITICAL,
                    message=f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ì„ê³„ê°’ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤: {metric.memory_usage:.1f}%",
                    metric="memory_usage",
                    current_value=metric.memory_usage,
                    threshold=self.performance_thresholds['memory_usage']['critical'],
                    recommendation="ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í™•ì¸ ë° ë¶ˆí•„ìš”í•œ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬"
                )
                alerts_created.append(alert)
                
            elif metric.memory_usage > self.performance_thresholds['memory_usage']['warning']:
                alert = PerformanceAlert(
                    timestamp=datetime.now(),
                    alert_type=AlertType.WARNING,
                    message=f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ê²½ê³  ìˆ˜ì¤€ì…ë‹ˆë‹¤: {metric.memory_usage:.1f}%",
                    metric="memory_usage",
                    current_value=metric.memory_usage,
                    threshold=self.performance_thresholds['memory_usage']['warning'],
                    recommendation="ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  í•„ìš”ì‹œ ì •ë¦¬í•˜ì„¸ìš”"
                )
                alerts_created.append(alert)
            
            # ì„±ê³µë¥  ê²½ê³ 
            if metric.success_rate < self.performance_thresholds['success_rate']['critical']:
                alert = PerformanceAlert(
                    timestamp=datetime.now(),
                    alert_type=AlertType.CRITICAL,
                    message=f"ì„±ê³µë¥ ì´ ì„ê³„ê°’ ì´í•˜ì…ë‹ˆë‹¤: {metric.success_rate:.2f}",
                    metric="success_rate",
                    current_value=metric.success_rate,
                    threshold=self.performance_thresholds['success_rate']['critical'],
                    recommendation="ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¥¼ ì¦‰ì‹œ í™•ì¸í•˜ê³  ìˆ˜ì •í•˜ì„¸ìš”"
                )
                alerts_created.append(alert)
                
            elif metric.success_rate < self.performance_thresholds['success_rate']['warning']:
                alert = PerformanceAlert(
                    timestamp=datetime.now(),
                    alert_type=AlertType.WARNING,
                    message=f"ì„±ê³µë¥ ì´ ê²½ê³  ìˆ˜ì¤€ì…ë‹ˆë‹¤: {metric.success_rate:.2f}",
                    metric="success_rate",
                    current_value=metric.success_rate,
                    threshold=self.performance_thresholds['success_rate']['warning'],
                    recommendation="ì„±ê³µë¥  ì €í•˜ ì›ì¸ì„ ë¶„ì„í•˜ê³  ê°œì„ í•˜ì„¸ìš”"
                )
                alerts_created.append(alert)
            
            # ê²½ê³  ì €ì¥ ë° ì½œë°± ì‹¤í–‰
            for alert in alerts_created:
                self.alerts.append(alert)
                self.stats['total_alerts_generated'] += 1
                
                # ì½œë°± í•¨ìˆ˜ ì‹¤í–‰
                for callback in self.alert_callbacks:
                    try:
                        callback(alert)
                    except Exception as e:
                        logger.error(f"ê²½ê³  ì½œë°± ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                
                logger.warning(f"ğŸš¨ ì„±ëŠ¥ ê²½ê³  ìƒì„±: {alert.alert_type.value} - {alert.message}")
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ê²½ê³  í™•ì¸ ì‹¤íŒ¨: {e}")
    
    def _generate_optimization_suggestions(self, metric: PerformanceMetric):
        """ìµœì í™” ì œì•ˆ ìƒì„±"""
        try:
            suggestions = []
            
            # CPU ìµœì í™” ì œì•ˆ
            if metric.cpu_usage > 80:
                suggestion = OptimizationSuggestion(
                    timestamp=datetime.now(),
                    priority="high",
                    category="resource_optimization",
                    title="CPU ì‚¬ìš©ë¥  ìµœì í™”",
                    description=f"í˜„ì¬ CPU ì‚¬ìš©ë¥ ì´ {metric.cpu_usage:.1f}%ë¡œ ë†’ìŠµë‹ˆë‹¤",
                    expected_improvement="CPU ì‚¬ìš©ë¥  20-30% ê°ì†Œ",
                    implementation_steps=[
                        "ë¶ˆí•„ìš”í•œ ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬",
                        "ì‘ì—… ìš°ì„ ìˆœìœ„ ì¡°ì •",
                        "ì½”ë“œ ìµœì í™” ë° ìºì‹± ì ìš©"
                    ],
                    estimated_effort="2-4ì‹œê°„"
                )
                suggestions.append(suggestion)
            
            # ë©”ëª¨ë¦¬ ìµœì í™” ì œì•ˆ
            if metric.memory_usage > 85:
                suggestion = OptimizationSuggestion(
                    timestamp=datetime.now(),
                    priority="high",
                    category="memory_optimization",
                    title="ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ìµœì í™”",
                    description=f"í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ {metric.memory_usage:.1f}%ë¡œ ë†’ìŠµë‹ˆë‹¤",
                    expected_improvement="ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  15-25% ê°ì†Œ",
                    implementation_steps=[
                        "ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í™•ì¸ ë° ìˆ˜ì •",
                        "ë¶ˆí•„ìš”í•œ ë°ì´í„° êµ¬ì¡° ì •ë¦¬",
                        "ë©”ëª¨ë¦¬ í’€ë§ ë° ì¬ì‚¬ìš© êµ¬í˜„"
                    ],
                    estimated_effort="3-6ì‹œê°„"
                )
                suggestions.append(suggestion)
            
            # ì„±ê³µë¥  ê°œì„  ì œì•ˆ
            if metric.success_rate < 0.9:
                suggestion = OptimizationSuggestion(
                    timestamp=datetime.now(),
                    priority="medium",
                    category="reliability_improvement",
                    title="ì„±ê³µë¥  ê°œì„ ",
                    description=f"í˜„ì¬ ì„±ê³µë¥ ì´ {metric.success_rate:.2f}ë¡œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤",
                    expected_improvement="ì„±ê³µë¥  5-10% í–¥ìƒ",
                    implementation_steps=[
                        "ì˜¤ë¥˜ ë¡œê·¸ ë¶„ì„ ë° íŒ¨í„´ íŒŒì•…",
                        "ì˜ˆì™¸ ì²˜ë¦¬ ë¡œì§ ê°œì„ ",
                        "ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„"
                    ],
                    estimated_effort="4-8ì‹œê°„"
                )
                suggestions.append(suggestion)
            
            # ì„±ëŠ¥ ì ìˆ˜ ê¸°ë°˜ ì œì•ˆ
            if metric.score < 0.7:
                suggestion = OptimizationSuggestion(
                    timestamp=datetime.now(),
                    priority="medium",
                    category="general_optimization",
                    title="ì „ì²´ ì„±ëŠ¥ ìµœì í™”",
                    description=f"í˜„ì¬ ì„±ëŠ¥ ì ìˆ˜ê°€ {metric.score:.2f}ë¡œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤",
                    expected_improvement="ì„±ëŠ¥ ì ìˆ˜ 15-25% í–¥ìƒ",
                    implementation_steps=[
                        "ë³‘ëª© ì§€ì  ë¶„ì„ ë° ìµœì í™”",
                        "ì•Œê³ ë¦¬ì¦˜ íš¨ìœ¨ì„± ê°œì„ ",
                        "ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ìµœì í™”"
                    ],
                    estimated_effort="8-16ì‹œê°„"
                )
                suggestions.append(suggestion)
            
            # ì œì•ˆ ì €ì¥ ë° ì½œë°± ì‹¤í–‰
            for suggestion in suggestions:
                self.optimization_suggestions.append(suggestion)
                self.stats['total_suggestions_generated'] += 1
                
                # ì½œë°± í•¨ìˆ˜ ì‹¤í–‰
                for callback in self.optimization_callbacks:
                    try:
                        callback(suggestion)
                    except Exception as e:
                        logger.error(f"ìµœì í™” ì œì•ˆ ì½œë°± ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                
                logger.info(f"ğŸ’¡ ìµœì í™” ì œì•ˆ ìƒì„±: {suggestion.title}")
            
            # ë§ˆì§€ë§‰ ìµœì í™” ì œì•ˆ ì‹œê°„ ì—…ë°ì´íŠ¸
            if suggestions:
                self.stats['last_optimization_time'] = datetime.now()
            
        except Exception as e:
            logger.error(f"ìµœì í™” ì œì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def add_alert_callback(self, callback: Callable[[PerformanceAlert], None]):
        """ê²½ê³  ì½œë°± í•¨ìˆ˜ ì¶”ê°€"""
        self.alert_callbacks.append(callback)
        logger.info("ğŸ”” ê²½ê³  ì½œë°± í•¨ìˆ˜ ì¶”ê°€ë¨")
    
    def add_optimization_callback(self, callback: Callable[[OptimizationSuggestion], None]):
        """ìµœì í™” ì œì•ˆ ì½œë°± í•¨ìˆ˜ ì¶”ê°€"""
        self.optimization_callbacks.append(callback)
        logger.info("ğŸ’¡ ìµœì í™” ì œì•ˆ ì½œë°± í•¨ìˆ˜ ì¶”ê°€ë¨")
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        try:
            if not self.performance_history:
                return {'message': 'ìˆ˜ì§‘ëœ ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'}
            
            # ìµœê·¼ ì„±ëŠ¥ ë°ì´í„°
            recent_metrics = self.performance_history[-10:] if len(self.performance_history) >= 10 else self.performance_history
            
            # í†µê³„ ê³„ì‚°
            scores = [m.score for m in recent_metrics]
            cpu_usage = [m.cpu_usage for m in recent_metrics]
            memory_usage = [m.memory_usage for m in recent_metrics]
            
            summary = {
                'monitoring_active': self.monitoring_active,
                'monitoring_interval': self.monitoring_interval,
                'total_metrics_collected': self.stats['total_metrics_collected'],
                'total_alerts_generated': self.stats['total_alerts_generated'],
                'total_suggestions_generated': self.stats['total_suggestions_generated'],
                'monitoring_start_time': self.stats['monitoring_start_time'].isoformat() if self.stats['monitoring_start_time'] else None,
                'last_optimization_time': self.stats['last_optimization_time'].isoformat() if self.stats['last_optimization_time'] else None,
                
                # ìµœê·¼ ì„±ëŠ¥ í†µê³„
                'recent_performance': {
                    'average_score': sum(scores) / len(scores) if scores else 0,
                    'min_score': min(scores) if scores else 0,
                    'max_score': max(scores) if scores else 0,
                    'average_cpu_usage': sum(cpu_usage) / len(cpu_usage) if cpu_usage else 0,
                    'average_memory_usage': sum(memory_usage) / len(memory_usage) if memory_usage else 0
                },
                
                # ìµœê·¼ ê²½ê³  ë° ì œì•ˆ
                'recent_alerts': [alert.to_dict() for alert in self.alerts[-5:]],
                'recent_suggestions': [suggestion.to_dict() for suggestion in self.optimization_suggestions[-5:]],
                
                # ì„±ëŠ¥ ì„ê³„ê°’
                'performance_thresholds': self.performance_thresholds.copy(),
                
                # ì„±ëŠ¥ ìˆ˜ì¤€ ë¶„í¬
                'performance_levels': {
                    level.value: len([m for m in self.performance_history if m.level == level])
                    for level in PerformanceLevel
                }
            }
            
            return summary
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def get_performance_dashboard(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ë°˜í™˜"""
        try:
            if not self.performance_history:
                return {'message': 'ëŒ€ì‹œë³´ë“œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤'}
            
            # ì‹œê°„ë³„ ì„±ëŠ¥ ì¶”ì´ (ìµœê·¼ 24ì‹œê°„)
            now = datetime.now()
            day_ago = now - timedelta(hours=24)
            
            hourly_data = {}
            for metric in self.performance_history:
                if metric.timestamp >= day_ago:
                    hour = metric.timestamp.replace(minute=0, second=0, microsecond=0)
                    if hour not in hourly_data:
                        hourly_data[hour] = []
                    hourly_data[hour].append(metric)
            
            # ì‹œê°„ë³„ í‰ê·  ê³„ì‚°
            hourly_averages = {}
            for hour, metrics in hourly_data.items():
                hourly_averages[hour.isoformat()] = {
                    'score': sum(m.score for m in metrics) / len(metrics),
                    'cpu_usage': sum(m.cpu_usage for m in metrics) / len(metrics),
                    'memory_usage': sum(m.memory_usage for m in metrics) / len(metrics),
                    'count': len(metrics)
                }
            
            dashboard = {
                'current_status': {
                    'monitoring_active': self.monitoring_active,
                    'last_metric_time': self.performance_history[-1].timestamp.isoformat() if self.performance_history else None,
                    'current_score': self.performance_history[-1].score if self.performance_history else 0,
                    'current_level': self.performance_history[-1].level.value if self.performance_history else 'unknown'
                },
                
                'hourly_trends': hourly_averages,
                
                'alerts_summary': {
                    'total_alerts': len(self.alerts),
                    'critical_alerts': len([a for a in self.alerts if a.alert_type == AlertType.CRITICAL]),
                    'warning_alerts': len([a for a in self.alerts if a.alert_type == AlertType.WARNING]),
                    'recent_alerts': [alert.to_dict() for alert in self.alerts[-10:]]
                },
                
                'optimization_summary': {
                    'total_suggestions': len(self.optimization_suggestions),
                    'high_priority': len([s for s in self.optimization_suggestions if s.priority == 'high']),
                    'medium_priority': len([s for s in self.optimization_suggestions if s.priority == 'medium']),
                    'recent_suggestions': [suggestion.to_dict() for suggestion in self.optimization_suggestions[-10:]]
                },
                
                'performance_metrics': {
                    'total_metrics': len(self.performance_history),
                    'average_score': sum(m.score for m in self.performance_history) / len(self.performance_history) if self.performance_history else 0,
                    'best_score': max(m.score for m in self.performance_history) if self.performance_history else 0,
                    'worst_score': min(m.score for m in self.performance_history) if self.performance_history else 0
                }
            }
            
            return dashboard
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def set_performance_thresholds(self, thresholds: Dict[str, Any]) -> bool:
        """ì„±ëŠ¥ ì„ê³„ê°’ ì„¤ì •"""
        try:
            for category, values in thresholds.items():
                if category in self.performance_thresholds:
                    self.performance_thresholds[category].update(values)
            
            logger.info("âœ… ì„±ëŠ¥ ì„ê³„ê°’ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ì„ê³„ê°’ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def clear_data(self) -> bool:
        """ëª¨ë“  ë°ì´í„° ì´ˆê¸°í™”"""
        try:
            self.performance_history.clear()
            self.alerts.clear()
            self.optimization_suggestions.clear()
            
            # í†µê³„ ì´ˆê¸°í™”
            self.stats = {
                'total_metrics_collected': 0,
                'total_alerts_generated': 0,
                'total_suggestions_generated': 0,
                'monitoring_start_time': None,
                'last_optimization_time': None
            }
            
            logger.info("ğŸ—‘ï¸ ëª¨ë“  ì„±ëŠ¥ ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def export_data(self, format_type: str = 'json') -> str:
        """ì„±ëŠ¥ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        try:
            if format_type == 'json':
                # statsì—ì„œ datetime ê°ì²´ ì²˜ë¦¬
                export_stats = self.stats.copy()
                if export_stats.get('monitoring_start_time'):
                    export_stats['monitoring_start_time'] = export_stats['monitoring_start_time'].isoformat()
                if export_stats.get('last_optimization_time'):
                    export_stats['last_optimization_time'] = export_stats['last_optimization_time'].isoformat()
                
                export_data = {
                    'export_time': datetime.now().isoformat(),
                    'performance_history': [metric.to_dict() for metric in self.performance_history],
                    'alerts': [alert.to_dict() for alert in self.alerts],
                    'optimization_suggestions': [suggestion.to_dict() for suggestion in self.optimization_suggestions],
                    'stats': export_stats,
                    'thresholds': self.performance_thresholds
                }
                
                filename = f"performance_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, ensure_ascii=False, indent=2)
                
                logger.info(f"ğŸ“¤ ì„±ëŠ¥ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filename}")
                return filename
            
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format_type}")
                
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return ""

    def monitor_integration_performance(self) -> Dict[str, Any]:
        """í†µí•© ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±)"""
        try:
            # í˜„ì¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            current_metrics = self._collect_performance_metrics()
            
            # ë©”íŠ¸ë¦­ì´ Noneì¸ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
            if current_metrics is None:
                current_metrics = PerformanceMetric(
                    timestamp=datetime.now(),
                    cpu_usage=0.0,
                    memory_usage=0.0,
                    execution_time=0.0,
                    success_rate=1.0,
                    error_count=0,
                    throughput=0.0,
                    latency=0.0,
                    score=1.0,
                    level=PerformanceLevel.EXCELLENT
                )
            
            # í†µí•© ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
            integration_report = {
                'timestamp': datetime.now().isoformat(),
                'performance_level': current_metrics.level.value,
                'performance_score': current_metrics.score,
                'cpu_usage': current_metrics.cpu_usage,
                'memory_usage': current_metrics.memory_usage,
                'execution_time': current_metrics.execution_time,
                'success_rate': current_metrics.success_rate,
                'alerts_count': len(self.alerts),
                'suggestions_count': len(self.optimization_suggestions),
                'status': 'active' if self.monitoring_active else 'inactive'
            }
            
            logger.info("ğŸ” í†µí•© ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì™„ë£Œ")
            return integration_report
            
        except Exception as e:
            logger.error(f"í†µí•© ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨: {e}")
            return {
                'timestamp': datetime.now().isoformat(),
                'status': 'error',
                'error': str(e)
            }

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
PerformanceMonitor = AdvancedPerformanceMonitor
