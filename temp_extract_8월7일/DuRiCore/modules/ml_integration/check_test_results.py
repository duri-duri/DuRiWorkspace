#!/usr/bin/env python3
"""
Phase 1 í…ŒìŠ¤íŠ¸ ê²°ê³¼ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
ì €ì¥ëœ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë¡œë“œí•˜ê³  ë¶„ì„
"""

import pickle
import sys
from pathlib import Path

def check_test_results():
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ í™•ì¸"""
    try:
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
        results_file = "phase1_test_results.pkl"
        
        if not Path(results_file).exists():
            print(f"âŒ í…ŒìŠ¤íŠ¸ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {results_file}")
            return
        
        print("ğŸ” Phase 1 í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ì¤‘...")
        
        # ê²°ê³¼ íŒŒì¼ ë¡œë“œ
        with open(results_file, 'rb') as f:
            results_data = pickle.load(f)
        
        print("\n" + "="*60)
        print("ğŸ¯ PHASE 1 í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        # ê¸°ë³¸ ì •ë³´
        if 'test_config' in results_data:
            config = results_data['test_config']
            print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ì„¤ì •:")
            print(f"   - ì„±ëŠ¥ ì„ê³„ê°’: {config.get('performance_threshold', 'N/A')}")
            print(f"   - êµì°¨ ê²€ì¦ í´ë“œ: {config.get('cross_validation_folds', 'N/A')}")
            print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨: {config.get('test_data_ratio', 'N/A')}")
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼
        if 'test_results' in results_data:
            test_results = results_data['test_results']
            print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
            print(f"   - ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {len(test_results)}")
            
            for test_name, test_result in test_results.items():
                print(f"   - {test_name}: {'âœ… ì™„ë£Œ' if 'error' not in test_result else 'âŒ ì‹¤íŒ¨'}")
        
        # ì„±ëŠ¥ ë¹„êµ
        if 'performance_comparison' in results_data:
            perf_comp = results_data['performance_comparison']
            print(f"\nğŸ“ˆ ì„±ëŠ¥ ë¹„êµ:")
            print(f"   - ì›ë³¸ vs ìµœì í™”: {'âœ… ì™„ë£Œ' if perf_comp else 'âŒ ì—†ìŒ'}")
        
        # ê²€ì¦ ë©”íŠ¸ë¦­
        if 'validation_metrics' in results_data:
            val_metrics = results_data['validation_metrics']
            print(f"\nğŸ¯ ê²€ì¦ ë©”íŠ¸ë¦­:")
            print(f"   - ì´ ë©”íŠ¸ë¦­ ìˆ˜: {len(val_metrics)}")
        
        # íŒŒì¼ í¬ê¸° ì •ë³´
        file_size = Path(results_file).stat().st_size
        print(f"\nğŸ’¾ íŒŒì¼ ì •ë³´:")
        print(f"   - íŒŒì¼ í¬ê¸°: {file_size} bytes")
        print(f"   - íŒŒì¼ ê²½ë¡œ: {Path(results_file).absolute()}")
        
        print("\n" + "="*60)
        print("âœ… í…ŒìŠ¤íŠ¸ ê²°ê³¼ í™•ì¸ ì™„ë£Œ!")
        print("="*60)
        
        # ìƒì„¸ ê²°ê³¼ ìš”ì•½
        print("\nğŸ” ìƒì„¸ ê²°ê³¼ ë¶„ì„:")
        analyze_detailed_results(results_data)
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ê²°ê³¼ í™•ì¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def analyze_detailed_results(results_data):
    """ìƒì„¸ ê²°ê³¼ ë¶„ì„"""
    try:
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„¸ ë¶„ì„
        if 'test_results' in results_data:
            test_results = results_data['test_results']
            
            for test_name, test_result in test_results.items():
                print(f"\nğŸ“‹ {test_name}:")
                
                if isinstance(test_result, dict):
                    if 'error' in test_result:
                        print(f"   âŒ ì˜¤ë¥˜: {test_result['error']}")
                    else:
                        # ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ì˜ ì£¼ìš” ì •ë³´ ì¶”ì¶œ
                        if 'optimization_results' in test_result:
                            opt_results = test_result['optimization_results']
                            if isinstance(opt_results, dict):
                                print(f"   âœ… ìµœì í™” ì™„ë£Œ: {len(opt_results)}ê°œ ê²°ê³¼")
                        
                        if 'performance_improvement' in test_result:
                            improvement = test_result['performance_improvement']
                            if isinstance(improvement, dict):
                                avg_improvement = improvement.get('average_improvement', 0)
                                print(f"   ğŸ“ˆ í‰ê·  ì„±ëŠ¥ í–¥ìƒ: {avg_improvement:.2f}%")
                        
                        if 'feature_optimization_analysis' in test_result:
                            feature_analysis = test_result['feature_optimization_analysis']
                            if isinstance(feature_analysis, dict):
                                reduction = feature_analysis.get('feature_reduction_percentage', 0)
                                new_features = feature_analysis.get('new_features_created', 0)
                                print(f"   ğŸ”§ íŠ¹ì„± ìµœì í™”: {reduction:.1f}% ê°ì†Œ, {new_features}ê°œ ìƒˆ íŠ¹ì„±")
                        
                        if 'ensemble_performance_analysis' in test_result:
                            ensemble_analysis = test_result['ensemble_performance_analysis']
                            if isinstance(ensemble_analysis, dict):
                                overall_perf = ensemble_analysis.get('overall_ensemble_performance', 0)
                                print(f"   ğŸ¯ ì•™ìƒë¸” ì„±ëŠ¥: {overall_perf:.3f}")
                else:
                    print(f"   ğŸ“ ê²°ê³¼ íƒ€ì…: {type(test_result)}")
        
        # ì„±ëŠ¥ ë¹„êµ ìƒì„¸ ë¶„ì„
        if 'performance_comparison' in results_data:
            perf_comp = results_data['performance_comparison']
            print(f"\nğŸ“Š ì„±ëŠ¥ ë¹„êµ ìƒì„¸:")
            
            if 'original_vs_optimized' in perf_comp:
                orig_vs_opt = perf_comp['original_vs_optimized']
                print(f"   - ì›ë³¸ vs ìµœì í™” ë¹„êµ: {len(orig_vs_opt)}ê°œ ëª¨ë¸")
            
            if 'optimization_effectiveness' in perf_comp:
                opt_effectiveness = perf_comp['optimization_effectiveness']
                print(f"   - ìµœì í™” íš¨ê³¼ì„±: {opt_effectiveness}")
        
        # ê²€ì¦ ë©”íŠ¸ë¦­ ìƒì„¸ ë¶„ì„
        if 'validation_metrics' in results_data:
            val_metrics = results_data['validation_metrics']
            print(f"\nğŸ¯ ê²€ì¦ ë©”íŠ¸ë¦­ ìƒì„¸:")
            print(f"   - ì´ ë©”íŠ¸ë¦­ ìˆ˜: {len(val_metrics)}")
            
            for metric_name, metric_value in val_metrics.items():
                if isinstance(metric_value, (int, float)):
                    print(f"   - {metric_name}: {metric_value}")
                elif isinstance(metric_value, str):
                    print(f"   - {metric_name}: {metric_value}")
                else:
                    print(f"   - {metric_name}: {type(metric_value)}")
                    
    except Exception as e:
        print(f"âŒ ìƒì„¸ ê²°ê³¼ ë¶„ì„ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    check_test_results()
