name: insight-metrics-report

on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
    paths:
      - 'insight/**'
      - 'tests/test_metrics.py'
      - '.github/workflows/insight-metrics-report.yml'
  workflow_dispatch: {}

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  pull-requests: write

jobs:
  metrics-report:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install -U pip pytest

      - name: Run metrics tests
        run: python -m pytest tests/test_metrics.py -v

      - name: Generate metrics report
        id: generate
        run: |
          python - <<'PY'
          import json
          from insight.metrics import evaluate_text
          sample_texts = [
              'The quick brown fox jumps over the lazy dog.',
              'A fast brown fox leaps over a sleepy canine.',
              'The dog is lazy and sleeps all day.',
              'Innovative solutions require creative thinking and comprehensive analysis.',
              'This is a very short text.',
          ]
          results = []
          for i, text in enumerate(sample_texts):
              r = evaluate_text(text, detailed=True)
              r['text'] = text
              r['index'] = i
              results.append(r)
          report = {
              'total_samples': len(sample_texts),
              'average_composite_score': sum(r['composite_score'] for r in results)/len(results),
              'results': results,
              'summary': {
                  'highest_score': max(results, key=lambda x: x['composite_score']),
                  'lowest_score': min(results, key=lambda x: x['composite_score']),
                  'score_range': max(r['composite_score'] for r in results) - min(r['composite_score'] for r in results),
              },
          }
          with open('metrics_report.json','w', encoding='utf-8') as f:
              json.dump(report, f, ensure_ascii=False, indent=2)
          print('Metrics report generated successfully!')
          print(f'AVG={report["average_composite_score"]:.3f}')
          print(f'RANGE={report["summary"]["score_range"]:.3f}')
          PY

      - name: Upload metrics report
        uses: actions/upload-artifact@v4
        with:
          name: insight-metrics-report
          path: metrics_report.json
          retention-days: 30

      - name: Write job summary
        run: |
          echo "## 📊 Insight Metrics Summary" >> $GITHUB_STEP_SUMMARY
          echo "- Average: ${{ steps.generate.outputs.AVG }}" >> $GITHUB_STEP_SUMMARY
          echo "- Range: ${{ steps.generate.outputs.RANGE }}" >> $GITHUB_STEP_SUMMARY
          echo "- Samples: 5" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "See PR comment for detailed results." >> $GITHUB_STEP_SUMMARY

      - name: Comment PR with metrics summary (upsert)
        if: ${{ github.event_name != 'workflow_dispatch' }}
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const report = JSON.parse(fs.readFileSync('metrics_report.json','utf8'));
            const esc = s => String(s).replace(/[`$\\]/g, m => ({'`':'\\`','$':'\\$','\\':'\\\\'}[m]));
            const runUrl = `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            
            const lines = [];
            lines.push('## 📊 Insight Metrics Report','',
              '**Summary:**',
              `- 📈 Average Composite Score: ${report.average_composite_score.toFixed(3)}`,
              `- 📊 Score Range: ${report.summary.score_range.toFixed(3)}`,
              `- 📝 Total Samples: ${report.total_samples}`,'',
              '**Top Performer:**',
              `> ${esc(report.summary.highest_score.text)}`,
              `> Score: ${report.summary.highest_score.composite_score.toFixed(3)}`,'',
              '**Detailed Results:**');
            for (const [i,r] of report.results.entries()) {
              const t = r.text.length>50 ? r.text.slice(0,50)+'…' : r.text;
              lines.push(`${i+1}. Score: ${r.composite_score.toFixed(3)} - ${esc(t)}`);
            }
            lines.push('',`📁 Full report: [Artifacts on this run](${runUrl})`);
            const body = lines.join('\n');

            const {data: comments} = await github.rest.issues.listComments({
              owner: context.repo.owner, repo: context.repo.repo, issue_number: context.issue.number, per_page: 100
            });
            const prev = comments.find(c => c.user.type==='Bot' && /Insight Metrics Report/.test(c.body||''));
            if (prev) {
              await github.rest.issues.updateComment({ owner: context.repo.owner, repo: context.repo.repo, comment_id: prev.id, body });
            } else {
              await github.rest.issues.createComment({ owner: context.repo.owner, repo: context.repo.repo, issue_number: context.issue.number, body });
            }
